name: Run Spark tests (internal)

on:
  workflow_call:
    inputs:
      ref:
        description: The Git ref to run the tests on
        type: string
        required: true
      name:
        description: The name of the test run
        type: string
        required: true
      spark_version:
        description: The Spark version to test
        type: string
        default: "3.5.1"
      java_version:
        description: Java version
        type: string
        default: "17"
      python_version:
        description: Python version
        type: string
        default: "3.11"

# All the scripts used in this workflow come from the commit specified by `inputs.ref`,
# which can be different from the commit that defines the workflow.
# We assume that the scripts are compatible with the workflow definition between the two commits.
# Otherwise, we may encounter errors such as "file not found" when the workflow invokes
# a script that does not exist in the commit being tested.
# Therefore, when you have both script changes and workflow changes, you should consider
# separating them into two pull requests. For example, you can add a script in one pull request
# and update the workflow to invoke the script in a follow-up pull request.

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.ref }}

      - name: Get job information
        id: info
        run: |
          commit_hash="$(git rev-parse HEAD)"
          echo "env_key=spark-${{ inputs.spark_version }}-java-${{ inputs.java_version }}-python-${{ inputs.python_version }}" >> "$GITHUB_OUTPUT"
          echo "commit_hash=${commit_hash}" >> "$GITHUB_OUTPUT"

      - uses: actions/cache@v4
        name: Cache test logs
        id: cache-logs
        with:
          path: |
            tmp/spark-tests
          key: spark-tests-logs-${{ runner.os }}-${{ steps.info.outputs.env_key }}-${{ steps.info.outputs.commit_hash }}

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        uses: actions/setup-java@v4
        with:
          distribution: "corretto"
          java-version: ${{ inputs.java_version }}

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version }}

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Install Hatch
        uses: pypa/hatch@install

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Install grpc_health_probe
        run: |
          wget -q -O /usr/local/bin/grpc_health_probe "https://github.com/grpc-ecosystem/grpc-health-probe/releases/download/v0.4.26/grpc_health_probe-linux-amd64"
          chmod +x /usr/local/bin/grpc_health_probe
          grpc_health_probe -version

      # Building PySpark package is slow (~40 minutes on GitHub Linux runners),
      # so we create a dedicated cache for them to increase the cache hit rate.
      - if: steps.cache-logs.outputs.cache-hit != 'true'
        uses: actions/cache@v4
        name: Cache PySpark package
        id: cache-pyspark
        with:
          path: |
            opt/spark/python/dist/pyspark-*.tar.gz
          # We must use `format()` to define the key here since nested variable substitution is not supported.
          key: ${{ format('spark-tests-package-{0}-pyspark-{1}-{2}', runner.os, inputs.spark_version, hashFiles(format('scripts/spark-tests/spark-{0}.patch', inputs.spark_version), 'scripts/spark-tests/build-pyspark.sh')) }}

      - if: steps.cache-logs.outputs.cache-hit != 'true' && steps.cache-pyspark.outputs.cache-hit != 'true'
        uses: actions/checkout@v4
        name: Checkout Spark
        with:
          repository: apache/spark
          path: opt/spark
          ref: v${{ inputs.spark_version }}
          fetch-depth: 1

      - if: steps.cache-logs.outputs.cache-hit != 'true' && steps.cache-pyspark.outputs.cache-hit != 'true'
        name: Build PySpark package
        run: |
          scripts/spark-tests/build-pyspark.sh

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        uses: actions/cache/restore@v4
        name: Restore python package
        id: restore-python-package
        with:
          path: target/wheels
          key: python-package-${{ runner.os }}-${{ steps.info.outputs.commit_hash }}

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-rust
        name: Set up Rust

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Build Python package
        uses: PyO3/maturin-action@v1

      # We do not cache the environments since package installation via `uv` is fast, while downloading
      # the environments from the cache still takes time.
      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Set up Hatch environments
        run: |
          hatch run test:bash -c '{env:HATCH_UV} pip install framework --no-index -f target/wheels --force-reinstall'

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Run Spark Connect server
        run: |
          nohup hatch run test:python -m framework.spark.server > /dev/null 2>&1 < /dev/null &

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Wait for Spark Connect server to start
        run: |
          scripts/spark-tests/wait-for-server.sh

      - if: steps.cache-logs.outputs.cache-hit != 'true'
        name: Run Spark tests
        # We set a timeout to prevent the tests from hanging indefinitely,
        # which can happen occasionally for unknown reasons.
        # The timeout value should be adjusted as we add more tests.
        timeout-minutes: 5
        run: |
          export TEST_RUN_GIT_COMMIT="${{ steps.info.outputs.commit_hash }}"
          export TEST_RUN_GIT_REF="${{ inputs.ref }}"
          scripts/spark-tests/run-tests.sh

      - name: Upload test logs
        uses: actions/upload-artifact@v4
        with:
          name: test-logs-${{ inputs.name }}
          path: tmp/spark-tests/latest
          retention-days: 7
