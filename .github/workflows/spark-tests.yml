name: Spark Tests

on:
  workflow_call:
    inputs:
      spark_version:
        description: The Spark version to test
        type: string
        default: "4.0.0"
    secrets:
      CODECOV_TOKEN:
        description: Token for uploading coverage to Codecov
        required: true

jobs:
  setup:
    name: Setup
    runs-on: ubuntu-latest
    outputs:
      run_tests: ${{ ((github.event_name == 'pull_request' && (github.event.action == 'opened' || steps.match.outputs.result == 'true' || steps.label.outputs.has_label == 'true')) || github.event_name == 'push') && 'true' || 'false' }}
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/commit-message-match
        id: match
        with:
          pattern: "\\[(spark )?tests?\\]"
          ignore_case: true

      - uses: actions/github-script@v7
        id: label
        with:
          script: |
            const labels = context.payload.pull_request?.labels?.map(label => label.name) || [];
            core.setOutput('has_label', labels.includes('run spark tests') ? 'true' : 'false');

  test:
    name: Test
    if: needs.setup.outputs.run_tests == 'true'
    runs-on: ubuntu-latest
    needs:
      - setup
    env:
      # Set the Hatch environment for `hatch run` commands.
      HATCH_ENV: "test-spark.spark-${{ inputs.spark_version }}"
      # Coverage environment variables
      RUSTC_WORKSPACE_WRAPPER: ${{ github.workspace }}/.github/scripts/rustc-workspace-wrapper.sh
      LLVM_PROFILE_FILE: ${{ github.workspace }}/target/sail-spark-${{ inputs.spark_version }}-%p-%m.profraw
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-java@v4
        with:
          distribution: "corretto"
          java-version: "17"

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Hatch
        uses: pypa/hatch@install

      - name: Install Rust toolchain and coverage tools
        run: |
          {
            rustup toolchain install stable
            rustup component add --toolchain stable llvm-tools-preview
          } &

        # Install grcov for code coverage
      - name: Install grcov
        run: |
          wget -q -O grcov.tar.bz2 "https://github.com/mozilla/grcov/releases/download/v0.10.5/grcov-x86_64-unknown-linux-gnu.tar.bz2"
          tar -xjf grcov.tar.bz2
          sudo mv grcov /usr/local/bin/
          chmod +x /usr/local/bin/grcov
          rm grcov.tar.bz2
        shell: bash

      - name: Install grpc_health_probe
        run: |
          wget -q -O /usr/local/bin/grpc_health_probe "https://github.com/grpc-ecosystem/grpc-health-probe/releases/download/v0.4.26/grpc_health_probe-linux-amd64"
          chmod +x /usr/local/bin/grpc_health_probe
          grpc_health_probe -version

      # Building the patched PySpark package is slow (~40 minutes on GitHub Linux runners),
      # so we create a dedicated cache entry for it.
      - uses: actions/cache@v4
        name: Cache PySpark
        id: cache-pyspark
        with:
          path: opt/spark/python/dist/pyspark-*.tar.gz
          # We must use `format()` to define the key here since nested variable substitution is not supported.
          key: ${{ format('spark-tests-pyspark-{0}-{1}-{2}-{3}', inputs.spark_version, runner.os, runner.arch, hashFiles(format('scripts/spark-tests/spark-{0}.patch', inputs.spark_version), 'scripts/spark-tests/build-pyspark.sh')) }}

      - if: steps.cache-pyspark.outputs.cache-hit != 'true'
        uses: actions/checkout@v4
        name: Checkout Spark
        with:
          repository: apache/spark
          path: opt/spark
          ref: v${{ inputs.spark_version }}
          fetch-depth: 1

      - if: steps.cache-pyspark.outputs.cache-hit != 'true'
        name: Build PySpark
        env:
          SPARK_VERSION: ${{ inputs.spark_version }}
        run: |
          scripts/spark-tests/build-pyspark.sh

      - name: Download Python Package
        uses: actions/download-artifact@v4
        with:
          name: python-package
          path: target/wheels

      # We do not cache the environments since package installation via `uv` is fast, while downloading
      # the environments from the cache still takes time.
      - name: Set Up Hatch Environment
        run: |
          hatch run install-pyspark
          hatch run install-pysail

      - name: Run Spark Connect Server
        run: |
          nohup hatch run scripts/spark-tests/run-server.sh > /dev/null 2>&1 < /dev/null &

      - name: Wait for Spark Connect Server to Start
        run: |
          scripts/spark-tests/wait-for-server.sh

      - name: Run Spark Tests
        # We set a timeout to prevent the tests from hanging indefinitely,
        # which can happen occasionally for unknown reasons.
        # The timeout value should be adjusted as we add more tests.
        timeout-minutes: 10
        env:
          TEST_RUN_GIT_COMMIT: ${{ github.sha }}
          TEST_RUN_GIT_REF: ${{ github.ref }}
        run: |
          hatch run scripts/spark-tests/run-tests.sh

      - name: Shutdown Spark Connect Server
        if: always()
        run: |
          # Find and kill sail server processes
          SAIL_PIDS=$(pgrep -f "sail spark server" || echo "")
          if [ -n "$SAIL_PIDS" ]; then
            echo "Found sail server processes: $SAIL_PIDS"
            # Send SIGINT for graceful shutdown
            kill -INT $SAIL_PIDS 2>/dev/null || true
            # Wait up to 30 seconds for graceful shutdown
            for i in {1..30}; do
              if ! pgrep -f "sail spark server" > /dev/null; then
                echo "Sail server stopped gracefully"
                break
              fi
              sleep 1
            done
            # Force kill if still running
            REMAINING_PIDS=$(pgrep -f "sail spark server" || echo "")
            if [ -n "$REMAINING_PIDS" ]; then
              echo "Force killing remaining processes: $REMAINING_PIDS"
              kill -KILL $REMAINING_PIDS 2>/dev/null || true
            fi
          else
            echo "No sail server processes found"
          fi
          sleep 10

      # Generate coverage report after Spark tests
      - name: Generate coverage report
        run: |
          # Wait for background Rust toolchain installation to complete
          wait
          BINARY_PATH=$(hatch run python -c 'import pysail._native; import os; print(os.path.dirname(pysail._native.__file__))')
          echo "Binary path: $BINARY_PATH"
          grcov target --binary-path "$BINARY_PATH" -s . -t lcov --branch --ignore-not-existing -o coverage-spark-${{ inputs.spark_version }}.info

      - name: Record Rust version
        run: echo "RUST=$(rustc --version)" >> "$GITHUB_ENV"

      - name: Upload spark coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: spark
          fail_ci_if_error: true
          env_vars: OS,RUST
          files: "coverage-spark-${{ inputs.spark_version }}.info"

      - name: Upload Test Logs
        uses: actions/upload-artifact@v4
        with:
          name: spark-${{ inputs.spark_version }}-test-logs
          path: tmp/spark-tests/latest
          retention-days: 30

  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    needs:
      - test
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/commit-baseline
        id: baseline

      - uses: ./.github/actions/commit-workflow
        id: baseline-workflow
        with:
          sha: ${{ steps.baseline.outputs.sha }}
          workflow_name: Build
          artifact_name: spark-${{ inputs.spark_version }}-test-logs

      - name: Download Test Logs
        uses: actions/download-artifact@v4
        with:
          name: spark-${{ inputs.spark_version }}-test-logs
          path: /tmp/test-after

      - name: Download Test Logs (Baseline)
        if: ${{ steps.baseline-workflow.outputs.workflow_run_id != '' }}
        uses: actions/download-artifact@v4
        with:
          name: spark-${{ inputs.spark_version }}-test-logs
          path: /tmp/test-before
          run-id: ${{ steps.baseline-workflow.outputs.workflow_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download Test Logs (No Baseline)
        if: ${{ steps.baseline-workflow.outputs.workflow_run_id == '' }}
        run: |
          cp -r /tmp/test-after /tmp/test-before

      - name: Generate Test Report
        env:
          SPARK_VERSION: ${{ inputs.spark_version }}
          BASELINE_WORKFLOW_RUN_ID: ${{ steps.baseline-workflow.outputs.workflow_run_id }}
        run: |
          mkdir -p /tmp/report
          report="/tmp/report/report.md"
          if [[ -z "$BASELINE_WORKFLOW_RUN_ID" ]]; then
            printf "> [!WARNING]\n> The baseline was not found.\n\n" >> "$report"
          fi
          scripts/spark-tests/generate-test-report.sh /tmp/test-after /tmp/test-before >> "$report"
          cat "$report" >> "$GITHUB_STEP_SUMMARY"

      - name: Save Pull Request Number
        if: github.event_name == 'pull_request'
        env:
          NUMBER: ${{ github.event.number }}
        run: |
          echo "$NUMBER" > /tmp/report/pull-request.txt

      - name: Upload Test Report
        uses: actions/upload-artifact@v4
        with:
          name: spark-${{ inputs.spark_version }}-test-report
          path: /tmp/report
          retention-days: 1
