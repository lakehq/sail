name: Spark Tests

on:
  workflow_call:
    inputs:
      spark_version:
        description: The Spark version to test
        type: string
        default: "3.5.5"

jobs:
  setup:
    name: Setup
    runs-on: ubuntu-latest
    outputs:
      run_tests: ${{ ((github.event_name == 'pull_request' && (github.event.action == 'opened' || steps.match.outputs.result == 'true' || steps.label.outputs.has_label == 'true')) || github.event_name == 'push') && 'true' || 'false' }}
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/commit-message-match
        id: match
        with:
          pattern: "\\[(spark )?tests?\\]"
          ignore_case: true

      - uses: actions/github-script@v7
        id: label
        with:
          script: |
            const labels = context.payload.pull_request?.labels?.map(label => label.name) || [];
            core.setOutput('has_label', labels.includes('run spark tests') ? 'true' : 'false');

  test:
    name: Test
    if: needs.setup.outputs.run_tests == 'true'
    runs-on: ubuntu-latest
    needs:
      - setup
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-java@v4
        with:
          distribution: "corretto"
          java-version: "17"

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Hatch
        uses: pypa/hatch@install

      - name: Install grpc_health_probe
        run: |
          wget -q -O /usr/local/bin/grpc_health_probe "https://github.com/grpc-ecosystem/grpc-health-probe/releases/download/v0.4.26/grpc_health_probe-linux-amd64"
          chmod +x /usr/local/bin/grpc_health_probe
          grpc_health_probe -version

      # Building the patched PySpark package is slow (~40 minutes on GitHub Linux runners),
      # so we create a dedicated cache entry for it.
      - uses: actions/cache@v4
        name: Cache PySpark
        id: cache-pyspark
        with:
          path: |
            opt/spark/python/dist/pyspark-*.tar.gz
            opt/spark/python/test_support/
          # We must use `format()` to define the key here since nested variable substitution is not supported.
          key: ${{ format('spark-tests-pyspark-{0}-{1}-{2}-{3}', inputs.spark_version, runner.os, runner.arch, hashFiles(format('scripts/spark-tests/spark-{0}.patch', inputs.spark_version), 'scripts/spark-tests/build-pyspark.sh')) }}

      - if: steps.cache-pyspark.outputs.cache-hit != 'true'
        uses: actions/checkout@v4
        name: Checkout Spark
        with:
          repository: apache/spark
          path: opt/spark
          ref: v${{ inputs.spark_version }}
          fetch-depth: 1

      - if: steps.cache-pyspark.outputs.cache-hit != 'true'
        name: Build PySpark
        run: |
          scripts/spark-tests/build-pyspark.sh

      - name: Download Python Package
        uses: actions/download-artifact@v4
        with:
          name: python-package
          path: target/wheels

      # We do not cache the environments since package installation via `uv` is fast, while downloading
      # the environments from the cache still takes time.
      - name: Set Up Hatch Environment
        run: |
          hatch env create test
          hatch run test:install-pyspark
          hatch run test:install-pysail

      - name: Run Spark Connect Server
        run: |
          nohup hatch run test:sail spark server -C opt/spark > /dev/null 2>&1 < /dev/null &

      - name: Wait for Spark Connect Server to Start
        run: |
          scripts/spark-tests/wait-for-server.sh

      - name: Run Spark Tests
        # We set a timeout to prevent the tests from hanging indefinitely,
        # which can happen occasionally for unknown reasons.
        # The timeout value should be adjusted as we add more tests.
        timeout-minutes: 5
        env:
          TEST_RUN_GIT_COMMIT: ${{ github.sha }}
          TEST_RUN_GIT_REF: ${{ github.ref }}
        run: |
          scripts/spark-tests/run-tests.sh

      - name: Upload Test Logs
        uses: actions/upload-artifact@v4
        with:
          name: spark-tests-logs
          path: tmp/spark-tests/latest
          retention-days: 30

  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    needs:
      - test
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/commit-baseline
        id: baseline

      - uses: ./.github/actions/commit-workflow
        id: baseline-workflow
        with:
          sha: ${{ steps.baseline.outputs.sha }}
          workflow_name: Build
          artifact_name: spark-tests-logs

      - name: Download Test Logs
        uses: actions/download-artifact@v4
        with:
          name: spark-tests-logs
          path: /tmp/test-after

      - name: Download Test Logs (Baseline)
        if: ${{ steps.baseline-workflow.outputs.workflow_run_id != '' }}
        uses: actions/download-artifact@v4
        with:
          name: spark-tests-logs
          path: /tmp/test-before
          run-id: ${{ steps.baseline-workflow.outputs.workflow_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download Test Logs (No Baseline)
        if: ${{ steps.baseline-workflow.outputs.workflow_run_id == '' }}
        run: |
          cp -r /tmp/test-after /tmp/test-before

      - name: Generate Test Report
        env:
          BASELINE_WORKFLOW_RUN_ID: ${{ steps.baseline-workflow.outputs.workflow_run_id }}
        run: |
          mkdir -p /tmp/report
          report="/tmp/report/report.md"
          if [[ -z "$BASELINE_WORKFLOW_RUN_ID" ]]; then
            printf "> [!WARNING]\n> The baseline was not found.\n\n" >> "$report"
          fi
          scripts/spark-tests/generate-test-report.sh /tmp/test-after /tmp/test-before >> "$report"
          cat "$report" >> "$GITHUB_STEP_SUMMARY"

      - name: Save Pull Request Number
        if: github.event_name == 'pull_request'
        env:
          NUMBER: ${{ github.event.number }}
        run: |
          echo "$NUMBER" > /tmp/report/pull-request.txt

      - name: Upload Test Report
        uses: actions/upload-artifact@v4
        with:
          name: spark-tests-report
          path: /tmp/report
          retention-days: 1
