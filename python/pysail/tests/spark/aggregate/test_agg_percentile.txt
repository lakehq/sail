>>> from pyspark.sql import functions as sf, types as T

# Test 1: Basic percentile with simple numeric values
>>> df = spark.createDataFrame([(0,), (10,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.3)).alias("p30")).show()
+---+
|p30|
+---+
|3.0|
+---+

>>> df.select(sf.percentile("x", sf.lit(0.3)).alias("p30")).printSchema()
root
 |-- p30: double (nullable = true)

# Test 2: Percentile 0.5 (median) with even number of values
>>> df = spark.createDataFrame([(0,), (1,), (2,), (3,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("p_05"), sf.median("x").alias("median")).show()
+----+------+
|p_05|median|
+----+------+
| 1.5|   1.5|
+----+------+

# Test 3: Percentile with odd number of values
>>> df = spark.createDataFrame([(0,), (1,), (2,), (3,), (4,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 4: Percentile with NULL values (should ignore NULLs)
>>> df = spark.createDataFrame([(None,), (1,), (2,), (3,), (None,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 5: Percentile 0.25 (first quartile)
>>> df = spark.createDataFrame([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.25)).alias("q1")).show()
+----+
|  q1|
+----+
|1.75|
+----+

# Test 6: Percentile 0.75 (third quartile)
>>> df.select(sf.percentile("x", sf.lit(0.75)).alias("q3")).show()
+----+
|  q3|
+----+
|5.25|
+----+

# Test 7: Percentile with float values
>>> df = spark.createDataFrame([(0.0,), (1.0,), (2.5,), (3.5,), (5.0,), (6.0,), (7.5,), (8.5,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  4.25|
+------+

# Test 8: Percentile 0.0 (minimum)
>>> df = spark.createDataFrame([(5,), (10,), (15,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.0)).alias("min_val")).show()
+-------+
|min_val|
+-------+
|    5.0|
+-------+

# Test 9: Percentile 1.0 (maximum)
>>> df.select(sf.percentile("x", sf.lit(1.0)).alias("max_val")).show()
+-------+
|max_val|
+-------+
|   15.0|
+-------+

# Test 10: Percentile with negative values
>>> df = spark.createDataFrame([(-10,), (-5,), (0,), (5,), (10,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   0.0|
+------+

# Test 11: Percentile with single value
>>> df = spark.createDataFrame([(42,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  42.0|
+------+

# Test 12: Percentile with all NULL values
>>> from pyspark.sql.types import StructType, StructField, IntegerType
>>> schema = StructType([StructField("x", IntegerType(), True)])
>>> df = spark.createDataFrame([(None,), (None,), (None,)], schema)
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  NULL|
+------+

# Test 13: Percentile with group by
>>> data = [("A", 1), ("A", 2), ("A", 3), ("B", 10), ("B", 20), ("B", 30),]
>>> df = spark.createDataFrame(data, ["group", "value"])
>>> out = df.groupBy("group").agg(sf.percentile("value", sf.lit(0.5)).alias("median")).orderBy("group")
>>> out.show()
+-----+------+
|group|median|
+-----+------+
|    A|   2.0|
|    B|  20.0|
+-----+------+

>>> out.printSchema()
root
 |-- group: string (nullable = true)
 |-- median: double (nullable = true)

# Test 14: Percentile with duplicates
>>> df = spark.createDataFrame([(1,), (2,), (2,), (2,), (3,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 15: Percentile with large integers
>>> from pyspark.sql.types import LongType
>>> LONG_MAX = 2**63 - 1
>>> df = spark.createDataFrame([(0,), (LONG_MAX // 2,), (LONG_MAX,)], ["x"])
>>> df = df.withColumn("x", sf.col("x").cast(LongType()))
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show(truncate=False)
+--------------------+
|median              |
+--------------------+
|4.611686018427388e18|
+--------------------+

# Test 16: Percentile with very small values
>>> df = spark.createDataFrame([(0.001,), (0.002,), (0.003,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
| 0.002|
+------+

# Test 17: Percentile comparison with median function
>>> df = spark.createDataFrame([(i,) for i in range(10)], ["x"])
>>> result = df.select(sf.percentile("x", sf.lit(0.5)).alias("percentile_50"), sf.median("x").alias("median") )
>>> result.show()
+-------------+------+
|percentile_50|median|
+-------------+------+
|          4.5|   4.5|
+-------------+------+

# Test 18: Percentile with different percentile values
>>> df = spark.createDataFrame([(i,) for i in range(100)], ["x"])
>>> result = df.select(
...     sf.percentile("x", sf.lit(0.25)).alias("q1"),
...     sf.percentile("x", sf.lit(0.50)).alias("q2"),
...     sf.percentile("x", sf.lit(0.75)).alias("q3")
... )
>>> result.show()
+-----+----+-----+
|   q1|  q2|   q3|
+-----+----+-----+
|24.75|49.5|74.25|
+-----+----+-----+

# Test 19: Percentile with string column (if supported)
>>> df = spark.createDataFrame([("a",), ("b",), ("c",), ("d",), ("e",)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|     c|
+------+

# Test 20: Percentile with repeated string values
>>> df = spark.createDataFrame([("a",), ("a",), ("a",), ("a",), ("a",)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|     a|
+------+

# Test 21: Percentile with sorted strings
>>> df = spark.createDataFrame([
...     ("apple",),
...     ("banana",),
...     ("cherry",),
...     ("date",),
...     ("elderberry",)
...     ], ["fruit"])
>>> df.select(sf.percentile("fruit", sf.lit(0.5)).alias("median_fruit")).show(truncate=False)
+------------+
|median_fruit|
+------------+
|cherry      |
+------------+

# Test 22: Percentile with NULLs in string column
>>> df = spark.createDataFrame([
...     ("apple",),
...     (None,),
...     ("banana",),
...     ("cherry",),
...     (None,)
... ], ["fruit"])
>>> df.select(sf.percentile("fruit", sf.lit(0.5)).alias("median_fruit")).show()
+------------+
|median_fruit|
+------------+
|      banana|
+------------+

>>> df.select(sf.percentile("fruit", sf.lit(0.0)).alias("median_fruit")).show()
+------------+
|median_fruit|
+------------+
|       apple|
+------------+

>>> df.select(sf.percentile("fruit", sf.lit(1.0)).alias("median_fruit")).show()
+------------+
|median_fruit|
+------------+
|      cherry|
+------------+

# Test 23: Percentile with interval types (YearMonth)
>>> spark.sql("SELECT percentile(col, 0.5) as median FROM VALUES (interval '0 months'), (interval '10 months') AS tab(col)").show(truncate=False)
+----------------------------+
|median                      |
+----------------------------+
|INTERVAL '0-5' YEAR TO MONTH|
+----------------------------+

# Test 24: Percentile with interval types (DayTime)
>>> spark.sql("SELECT percentile(col, 0.5) as median FROM VALUES (interval '0 seconds'), (interval '10 seconds') AS tab(col)").show(truncate=False)
+-----------------------------------+
|median                             |
+-----------------------------------+
|INTERVAL '0 00:00:05' DAY TO SECOND|
+-----------------------------------+

# Test 25: Percentile with interval types and NULLs
>>> spark.sql("SELECT percentile(col, 0.5) as median FROM VALUES (interval '0 months'), null, (interval '10 months') AS tab(col)").show(truncate=False)
+----------------------------+
|median                      |
+----------------------------+
|INTERVAL '0-5' YEAR TO MONTH|
+----------------------------+

# Test 26: Percentile with decimal values
>>> from decimal import Decimal
>>> df = spark.createDataFrame(
...     [(Decimal("1.00"),), (Decimal("2.00"),), (Decimal("3.00"),)],
...     "x DECIMAL(10,2)"
... )
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 27: Percentile 0.9 (90th percentile)
>>> df = spark.createDataFrame([(i,) for i in range(11)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.9)).alias("p90")).show()
+---+
|p90|
+---+
|9.0|
+---+

# Test 28: Percentile 0.1 (10th percentile)
>>> df.select(sf.percentile("x", sf.lit(0.1)).alias("p10")).show()
+---+
|p10|
+---+
|1.0|
+---+

# Test 29: Empty dataset
>>> from pyspark.sql.types import IntegerType, StructType, StructField
>>> schema = StructType([StructField("x", IntegerType(), True)])
>>> df = spark.createDataFrame([], schema)
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  NULL|
+------+

# Test 30: Percentile with mixed positive and negative floats
>>> df = spark.createDataFrame([(-2.5,), (-1.0,), (0.0,), (1.0,), (2.5,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   0.0|
+------+

>>> spark.sql("SELECT percentile(x, 0.5) AS p FROM (VALUES (0), (1), (2), (3)) AS t(x)").show()
+---+
|  p|
+---+
|1.5|
+---+

# Test 31: Percentile with nulls
>>> spark.sql("""
...     SELECT
...         min(x) AS min_value,
...         percentile(x, 0.0) AS p_0,
...         percentile(x, 0.5) AS p_50,
...         percentile(x, 1.0) AS p_100,
...         max(x) AS max_value
...     FROM (VALUES (0), (1), (2), (3)) AS t(x)
... """).show()
+---------+---+----+-----+---------+
|min_value|p_0|p_50|p_100|max_value|
+---------+---+----+-----+---------+
|        0|0.0| 1.5|  3.0|        3|
+---------+---+----+-----+---------+

>>> spark.sql("""
...     SELECT
...         min(x) AS min_value,
...         percentile(x, 0.0) AS p_0,
...         percentile(x, 0.5) AS p_50,
...         percentile(x, 1.0) AS p_100,
...         max(x) AS max_value
...     FROM (VALUES (0), (1), (2), (3), (NULL), (NULL), (NULL), (NULL)) AS t(x)
... """).show()
+---------+---+----+-----+---------+
|min_value|p_0|p_50|p_100|max_value|
+---------+---+----+-----+---------+
|        0|0.0| 1.5|  3.0|        3|
+---------+---+----+-----+---------+

# Array of percentiles
>>> spark.sql("""SELECT percentile(col, 0.5) AS p50 FROM (VALUES (0), (1), (2), (3), (4)) AS t(col)""").show()
+---+
|p50|
+---+
|2.0|
+---+

>>> spark.sql("""SELECT percentile(col, array(0.25, 0.5, 0.75)) AS percentiles FROM (VALUES (0), (1), (2), (3), (4)) AS t(col)""").show()
+---------------+
|    percentiles|
+---------------+
|[1.0, 2.0, 3.0]|
+---------------+

>>> spark.sql("""SELECT percentile(x, array(0.0, 0.25, 0.5, 0.75, 1.0)) AS percentiles FROM (VALUES (0), (10)) AS tab(x)""").show(truncate=False)
+--------------------------+
|percentiles               |
+--------------------------+
|[0.0, 2.5, 5.0, 7.5, 10.0]|
+--------------------------+

-- Test 4: Multiple percentiles with strings
spark.sql("""SELECT percentile(str, array(0.0, 0.5, 1.0)) AS percentiles FROM (VALUES ('a'), ('b'), ('c'), ('d'), ('e')) AS t(str)""").show()
-- fallo
