>>> from pyspark.sql import functions as sf, types as T

# Test 1: Basic percentile with simple numeric values
>>> df = spark.createDataFrame([(0,), (10,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.3)).alias("p30")).show()
+---+
|p30|
+---+
|3.0|
+---+

>>> df.select(sf.percentile("x", sf.lit(0.3)).alias("p30")).printSchema()
root
 |-- p30: double (nullable = true)

# Test 2: Percentile 0.5 (median) with even number of values
>>> df = spark.createDataFrame([(0,), (1,), (2,), (3,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   1.5|
+------+

# Test 3: Percentile with odd number of values
>>> df = spark.createDataFrame([(0,), (1,), (2,), (3,), (4,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 4: Percentile with NULL values (should ignore NULLs)
>>> df = spark.createDataFrame([(None,), (1,), (2,), (3,), (None,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 5: Percentile 0.25 (first quartile)
>>> df = spark.createDataFrame([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.25)).alias("q1")).show()
+----+
|  q1|
+----+
|1.75|
+----+

# Test 6: Percentile 0.75 (third quartile)
>>> df.select(sf.percentile("x", sf.lit(0.75)).alias("q3")).show()
+----+
|  q3|
+----+
|5.25|
+----+

# Test 7: Percentile with float values
>>> df = spark.createDataFrame([(0.0,), (1.0,), (2.5,), (3.5,), (5.0,), (6.0,), (7.5,), (8.5,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  4.25|
+------+

# Test 8: Percentile 0.0 (minimum)
>>> df = spark.createDataFrame([(5,), (10,), (15,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.0)).alias("min_val")).show()
+-------+
|min_val|
+-------+
|    5.0|
+-------+

# Test 9: Percentile 1.0 (maximum)
>>> df.select(sf.percentile("x", sf.lit(1.0)).alias("max_val")).show()
+-------+
|max_val|
+-------+
|   15.0|
+-------+

# Test 10: Percentile with negative values
>>> df = spark.createDataFrame([(-10,), (-5,), (0,), (5,), (10,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   0.0|
+------+

# Test 11: Percentile with single value
>>> df = spark.createDataFrame([(42,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  42.0|
+------+

# Test 12: Percentile with all NULL values
>>> from pyspark.sql.types import StructType, StructField, IntegerType
>>> schema = StructType([StructField("x", IntegerType(), True)])
>>> df = spark.createDataFrame([(None,), (None,), (None,)], schema)
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  NULL|
+------+

# Test 13: Percentile with group by
>>> data = [("A", 1), ("A", 2), ("A", 3), ("B", 10), ("B", 20), ("B", 30),]
>>> df = spark.createDataFrame(data, ["group", "value"])
>>> out = df.groupBy("group").agg(sf.percentile("value", sf.lit(0.5)).alias("median")).orderBy("group")
>>> out.show()
+-----+------+
|group|median|
+-----+------+
|    A|   2.0|
|    B|  20.0|
+-----+------+

>>> out.printSchema()
root
 |-- group: string (nullable = true)
 |-- median: double (nullable = true)

# Test 14: Percentile with duplicates
>>> df = spark.createDataFrame([(1,), (2,), (2,), (2,), (3,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 15: Percentile with large integers
>>> from pyspark.sql.types import LongType
>>> LONG_MAX = 2**63 - 1
>>> df = spark.createDataFrame([(0,), (LONG_MAX // 2,), (LONG_MAX,)], ["x"])
>>> df = df.withColumn("x", sf.col("x").cast(LongType()))
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show(truncate=False)
+--------------------+
|median              |
+--------------------+
|4.611686018427388e18|
+--------------------+

# Test 16: Percentile with very small values
>>> df = spark.createDataFrame([(0.001,), (0.002,), (0.003,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
| 0.002|
+------+

# Test 17: Percentile comparison with median function
>>> df = spark.createDataFrame([(i,) for i in range(10)], ["x"])
>>> result = df.select(sf.percentile("x", sf.lit(0.5)).alias("percentile_50"), sf.median("x").alias("median") )
>>> result.show()
+-------------+------+
|percentile_50|median|
+-------------+------+
|          4.5|   4.5|
+-------------+------+

# Test 18: Percentile with different percentile values
>>> df = spark.createDataFrame([(i,) for i in range(100)], ["x"])
>>> result = df.select(
...     sf.percentile("x", sf.lit(0.25)).alias("q1"),
...     sf.percentile("x", sf.lit(0.50)).alias("q2"),
...     sf.percentile("x", sf.lit(0.75)).alias("q3")
... )
>>> result.show()
+-----+----+-----+
|   q1|  q2|   q3|
+-----+----+-----+
|24.75|49.5|74.25|
+-----+----+-----+

# Test 19: Percentile with string column (if supported)
>>> df = spark.createDataFrame([("a",), ("b",), ("c",), ("d",), ("e",)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|     c|
+------+

# Test 20: Percentile with repeated string values
>>> df = spark.createDataFrame([("a",), ("a",), ("a",), ("a",), ("a",)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|     a|
+------+

# Test 21: Percentile with sorted strings
>>> df = spark.createDataFrame([
...     ("apple",),
...     ("banana",),
...     ("cherry",),
...     ("date",),
...     ("elderberry",)
...     ], ["fruit"])
>>> df.select(sf.percentile("fruit", sf.lit(0.5)).alias("median_fruit")).show(truncate=False)
+------------+
|median_fruit|
+------------+
|cherry      |
+------------+

# Test 22: Percentile with NULLs in string column
>>> df = spark.createDataFrame([
...     ("apple",),
...     (None,),
...     ("banana",),
...     ("cherry",),
...     (None,)
... ], ["fruit"])
>>> df.select(sf.percentile("fruit", sf.lit(0.5)).alias("median_fruit")).show()
+------------+
|median_fruit|
+------------+
|      banana|
+------------+

>>> df.select(sf.percentile("fruit", sf.lit(0.0)).alias("median_fruit")).show()
+------------+
|median_fruit|
+------------+
|       apple|
+------------+

>>> df.select(sf.percentile("fruit", sf.lit(1.0)).alias("median_fruit")).show()
+------------+
|median_fruit|
+------------+
|      cherry|
+------------+

# Test 23: Percentile with interval types (YearMonth)
>>> spark.sql("SELECT percentile(col, 0.5) as median FROM VALUES (interval '0 months'), (interval '10 months') AS tab(col)").show(truncate=False)
+----------------------------+
|median                      |
+----------------------------+
|INTERVAL '0-5' YEAR TO MONTH|
+----------------------------+

# Test 24: Percentile with interval types (DayTime)
>>> spark.sql("SELECT percentile(col, 0.5) as median FROM VALUES (interval '0 seconds'), (interval '10 seconds') AS tab(col)").show(truncate=False)
+-----------------------------------+
|median                             |
+-----------------------------------+
|INTERVAL '0 00:00:05' DAY TO SECOND|
+-----------------------------------+

# Test 25: Percentile with interval types and NULLs
>>> spark.sql("SELECT percentile(col, 0.5) as median FROM VALUES (interval '0 months'), null, (interval '10 months') AS tab(col)").show(truncate=False)
+----------------------------+
|median                      |
+----------------------------+
|INTERVAL '0-5' YEAR TO MONTH|
+----------------------------+

# Test 26: Percentile with decimal values
>>> from decimal import Decimal
>>> df = spark.createDataFrame(
...     [(Decimal("1.00"),), (Decimal("2.00"),), (Decimal("3.00"),)],
...     "x DECIMAL(10,2)"
... )
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   2.0|
+------+

# Test 27: Percentile 0.9 (90th percentile)
>>> df = spark.createDataFrame([(i,) for i in range(11)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.9)).alias("p90")).show()
+---+
|p90|
+---+
|9.0|
+---+

# Test 28: Percentile 0.1 (10th percentile)
>>> df.select(sf.percentile("x", sf.lit(0.1)).alias("p10")).show()
+---+
|p10|
+---+
|1.0|
+---+

# Test 29: Empty dataset
>>> from pyspark.sql.types import IntegerType, StructType, StructField
>>> schema = StructType([StructField("x", IntegerType(), True)])
>>> df = spark.createDataFrame([], schema)
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|  NULL|
+------+

# Test 30: Percentile with mixed positive and negative floats
>>> df = spark.createDataFrame([(-2.5,), (-1.0,), (0.0,), (1.0,), (2.5,)], ["x"])
>>> df.select(sf.percentile("x", sf.lit(0.5)).alias("median")).show()
+------+
|median|
+------+
|   0.0|
+------+

>>> spark.sql("SELECT percentile(x, 0.5) AS p FROM (VALUES (0), (1), (2), (3)) AS t(x)").show()
+---+
|  p|
+---+
|1.5|
+---+

# Test 31: Percentile with nulls
>>> spark.sql("""
...     SELECT
...         min(x) AS min_value,
...         percentile(x, 0.0) AS p_0,
...         percentile(x, 0.5) AS p_50,
...         percentile(x, 1.0) AS p_100,
...         max(x) AS max_value
...     FROM (VALUES (0), (1), (2), (3)) AS t(x)
... """).show()
+---------+---+----+-----+---------+
|min_value|p_0|p_50|p_100|max_value|
+---------+---+----+-----+---------+
|        0|0.0| 1.5|  3.0|        3|
+---------+---+----+-----+---------+

>>> spark.sql("""
...     SELECT
...         min(x) AS min_value,
...         percentile(x, 0.0) AS p_0,
...         percentile(x, 0.5) AS p_50,
...         percentile(x, 1.0) AS p_100,
...         max(x) AS max_value
...     FROM (VALUES (0), (1), (2), (3), (NULL), (NULL), (NULL), (NULL)) AS t(x)
... """).show()
+---------+---+----+-----+---------+
|min_value|p_0|p_50|p_100|max_value|
+---------+---+----+-----+---------+
|        0|0.0| 1.5|  3.0|        3|
+---------+---+----+-----+---------+

# OVER tests
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...         ORDER BY x
...         ) AS running_p50
...     FROM (VALUES (0), (1), (2), (3), (4)) AS t(x);
... """).show()
+---+-----------+
|  x|running_p50|
+---+-----------+
|  0|        0.0|
|  1|        0.5|
|  2|        1.0|
|  3|        1.5|
|  4|        2.0|
+---+-----------+

>>> spark.sql("""
... SELECT
...     x,
...     percentile(x, 1.0) OVER (
...         ORDER BY x
...         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
...     ) AS running_p50
... FROM VALUES (0), (1), (2), (3), (4), (5) AS t(x);
... """).show()
+---+-----------+
|  x|running_p50|
+---+-----------+
|  0|        0.0|
|  1|        1.0|
|  2|        2.0|
|  3|        3.0|
|  4|        4.0|
|  5|        5.0|
+---+-----------+
>>> spark.sql("""
... SELECT
...     x,
...     percentile(x, 0.5) OVER (
...         ORDER BY x
...         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
...     ) AS running_p50
... FROM VALUES (0), (1), (2), (3), (4), (5) AS t(x);
... """).show()
+---+-----------+
|  x|running_p50|
+---+-----------+
|  0|        0.0|
|  1|        0.5|
|  2|        1.0|
|  3|        1.5|
|  4|        2.0|
|  5|        2.5|
+---+-----------+

>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
...         ) AS sliding_p50
...     FROM VALUES (0), (1), (2), (3), (4), (5) AS t(x)
... """).show()
+---+-----------+
|  x|sliding_p50|
+---+-----------+
|  0|        0.0|
|  1|        0.5|
|  2|        1.0|
|  3|        2.0|
|  4|        3.0|
|  5|        4.0|
+---+-----------+

# Test with PARTITION BY
>>> spark.sql("""
...     SELECT
...         group,
...         x,
...         percentile(x, 0.5) OVER (
...             PARTITION BY group
...             ORDER BY x
...         ) AS running_p50
...     FROM VALUES ('A', 1), ('A', 2), ('A', 3), ('B', 10), ('B', 20), ('B', 30) AS t(group, x)
...     ORDER BY group, x
... """).show()
+-----+---+-----------+
|group|  x|running_p50|
+-----+---+-----------+
|    A|  1|        1.0|
|    A|  2|        1.5|
|    A|  3|        2.0|
|    B| 10|       10.0|
|    B| 20|       15.0|
|    B| 30|       20.0|
+-----+---+-----------+

# Test with PARTITION BY and sliding window
>>> spark.sql("""
...     SELECT
...         group,
...         x,
...         percentile(x, 0.5) OVER (
...             PARTITION BY group
...             ORDER BY x
...             ROWS BETWEEN 1 PRECEDING AND CURRENT ROW
...         ) AS sliding_p50
...     FROM VALUES ('A', 1), ('A', 2), ('A', 3), ('A', 4), ('B', 10), ('B', 20), ('B', 30) AS t(group, x)
...     ORDER BY group, x
... """).show()
+-----+---+-----------+
|group|  x|sliding_p50|
+-----+---+-----------+
|    A|  1|        1.0|
|    A|  2|        1.5|
|    A|  3|        2.5|
|    A|  4|        3.5|
|    B| 10|       10.0|
|    B| 20|       15.0|
|    B| 30|       25.0|
+-----+---+-----------+

# Test with ROWS BETWEEN ... AND ... FOLLOWING
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
...         ) AS centered_p50
...     FROM VALUES (0), (10), (20), (30), (40) AS t(x)
... """).show()
+---+------------+
|  x|centered_p50|
+---+------------+
|  0|         5.0|
| 10|        10.0|
| 20|        20.0|
| 30|        30.0|
| 40|        35.0|
+---+------------+

# Test with ROWS BETWEEN CURRENT ROW AND ... FOLLOWING
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING
...         ) AS forward_p50
...     FROM VALUES (0), (1), (2), (3), (4) AS t(x)
... """).show()
+---+-----------+
|  x|forward_p50|
+---+-----------+
|  0|        1.0|
|  1|        2.0|
|  2|        3.0|
|  3|        3.5|
|  4|        4.0|
+---+-----------+

# Test window with intervals (YearMonth)
>>> spark.sql("""
...     SELECT
...         col,
...         percentile(col, 0.5) OVER (
...             ORDER BY col
...         ) AS running_median
...     FROM VALUES (interval '0 months'), (interval '10 months'), (interval '20 months') AS t(col)
... """).show(truncate=False)
+-------------------+-----------------------------+
|col                |running_median               |
+-------------------+-----------------------------+
|INTERVAL '0' MONTH |INTERVAL '0-0' YEAR TO MONTH |
|INTERVAL '10' MONTH|INTERVAL '0-5' YEAR TO MONTH |
|INTERVAL '20' MONTH|INTERVAL '0-10' YEAR TO MONTH|
+-------------------+-----------------------------+

# Test window with intervals (DayTime)

>>> spark.sql("""
...     SELECT
...         col,
...         percentile(col, 0.5) OVER (
...             ORDER BY col
...         ) AS running_median
...     FROM VALUES (interval '0 seconds'), (interval '10 seconds'), (interval '20 seconds') AS t(col)
... """).show(truncate=False)
+--------------------+-----------------------------------+
|col                 |running_median                     |
+--------------------+-----------------------------------+
|INTERVAL '00' SECOND|INTERVAL '0 00:00:00' DAY TO SECOND|
|INTERVAL '10' SECOND|INTERVAL '0 00:00:05' DAY TO SECOND|
|INTERVAL '20' SECOND|INTERVAL '0 00:00:10' DAY TO SECOND|
+--------------------+-----------------------------------+

# Test window with different percentiles
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.25) OVER (ORDER BY x) AS p25,
...         percentile(x, 0.50) OVER (ORDER BY x) AS p50,
...         percentile(x, 0.75) OVER (ORDER BY x) AS p75
...     FROM VALUES (0), (10), (20), (30), (40) AS t(x)
... """).show()
+---+----+----+----+
|  x| p25| p50| p75|
+---+----+----+----+
|  0| 0.0| 0.0| 0.0|
| 10| 2.5| 5.0| 7.5|
| 20| 5.0|10.0|15.0|
| 30| 7.5|15.0|22.5|
| 40|10.0|20.0|30.0|
+---+----+----+----+

# Test window with NULLs
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
...         ) AS running_median
...     FROM VALUES (0), (NULL), (10), (NULL), (20), (30) AS t(x)
... """).show()
+----+--------------+
|   x|running_median|
+----+--------------+
|NULL|          NULL|
|NULL|          NULL|
|   0|           0.0|
|  10|           5.0|
|  20|          10.0|
|  30|          15.0|
+----+--------------+

# Test PARTITION BY with multiple partitions and different percentiles
>>> spark.sql("""
...     SELECT
...         category,
...         value,
...         percentile(value, 0.5) OVER (PARTITION BY category ORDER BY value) AS p50,
...         percentile(value, 0.9) OVER (PARTITION BY category ORDER BY value) AS p90
...     FROM VALUES
...         ('A', 1), ('A', 2), ('A', 3), ('A', 4), ('A', 5),
...         ('B', 10), ('B', 20), ('B', 30)
...     AS t(category, value)
...     ORDER BY category, value
... """).show()
+--------+-----+----+------------------+
|category|value| p50|               p90|
+--------+-----+----+------------------+
|       A|    1| 1.0|               1.0|
|       A|    2| 1.5|               1.9|
|       A|    3| 2.0|2.8000000000000003|
|       A|    4| 2.5|               3.7|
|       A|    5| 3.0|               4.6|
|       B|   10|10.0|              10.0|
|       B|   20|15.0|              19.0|
|       B|   30|20.0|              28.0|
+--------+-----+----+------------------+
