== Codegen ==
Whole-stage codegen is not supported; showing physical plan instead.

== Plan Steps ==
initial_logical_plan:
Limit: skip=0, fetch=Int32(100)
  Projection: <exprs>
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>, Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>, Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Filter: item.#<col> >= Decimal128(Some(99),2,2) AND item.#<col> <= Decimal128(Some(149),3,2) AND item.#<col> = catalog_sales.#<col> AND catalog_sales.#<col> = warehouse.#<col> AND catalog_sales.#<col> = date_dim.#<col> AND date_dim.#<col> >= spark_date(Utf8("2002-05-18")) - CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano)) AND date_dim.#<col> <= spark_date(Utf8("2002-05-18")) + CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano))
            Cross Join: 
              Cross Join: 
                Cross Join: 
                  Left Join:  Filter: catalog_sales.#<col> = catalog_returns.#<col> AND catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        TableScan: ?table?

logical_plan after resolve_grouping_function:
SAME TEXT AS ABOVE

logical_plan after type_coercion:
Limit: skip=0, fetch=CAST(Int32(100) AS Int64)
  Projection: <exprs>
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), CAST(Int32(0) AS Decimal128(12, 2))) WHEN Boolean(true) THEN CAST(Int32(0) AS Decimal128(13, 2)) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), CAST(Int32(0) AS Decimal128(12, 2))) WHEN Boolean(true) THEN CAST(Int32(0) AS Decimal128(13, 2)) END)]]
          Filter: item.#<col> >= CAST(Decimal128(Some(99),2,2) AS Decimal128(4, 2)) AND item.#<col> <= CAST(Decimal128(Some(149),3,2) AS Decimal128(4, 2)) AND item.#<col> = catalog_sales.#<col> AND catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64) AND catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64) AND date_dim.#<col> >= spark_date(Utf8("2002-05-18")) - CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano)) AND date_dim.#<col> <= spark_date(Utf8("2002-05-18")) + CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano))
            Cross Join: 
              Cross Join: 
                Cross Join: 
                  Left Join:  Filter: catalog_sales.#<col> = catalog_returns.#<col> AND catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        TableScan: ?table?

analyzed_logical_plan:
SAME TEXT AS ABOVE

logical_plan after eliminate_nested_union:
SAME TEXT AS ABOVE

logical_plan after simplify_expressions:
Limit: skip=0, fetch=100
  Projection: <exprs>
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND item.#<col> = catalog_sales.#<col> AND catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64) AND catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64) AND date_dim.#<col> >= Date32("2002-04-18") AND date_dim.#<col> <= Date32("2002-06-17")
            Cross Join: 
              Cross Join: 
                Cross Join: 
                  Left Join:  Filter: catalog_sales.#<col> = catalog_returns.#<col> AND catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        TableScan: ?table?

logical_plan after replace_distinct_aggregate:
SAME TEXT AS ABOVE

logical_plan after eliminate_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_predicate_subquery:
SAME TEXT AS ABOVE

logical_plan after scalar_subquery_to_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_lateral_join:
SAME TEXT AS ABOVE

logical_plan after extract_equijoin_predicate:
Limit: skip=0, fetch=100
  Projection: <exprs>
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND item.#<col> = catalog_sales.#<col> AND catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64) AND catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64) AND date_dim.#<col> >= Date32("2002-04-18") AND date_dim.#<col> <= Date32("2002-06-17")
            Cross Join: 
              Cross Join: 
                Cross Join: 
                  Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        TableScan: ?table?

logical_plan after eliminate_duplicated_expr:
SAME TEXT AS ABOVE

logical_plan after eliminate_filter:
SAME TEXT AS ABOVE

logical_plan after eliminate_cross_join:
Limit: skip=0, fetch=100
  Projection: <exprs>
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND date_dim.#<col> >= Date32("2002-04-18") AND date_dim.#<col> <= Date32("2002-06-17")
            Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
              Inner Join: catalog_sales.#<col> = item.#<col>
                Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                  Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        TableScan: ?table?

logical_plan after eliminate_limit:
SAME TEXT AS ABOVE

logical_plan after propagate_empty_relation:
SAME TEXT AS ABOVE

logical_plan after eliminate_one_union:
SAME TEXT AS ABOVE

logical_plan after filter_null_join_keys:
SAME TEXT AS ABOVE

logical_plan after eliminate_outer_join:
SAME TEXT AS ABOVE

logical_plan after push_down_limit:
Projection: <exprs>
  Limit: skip=0, fetch=100
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND date_dim.#<col> >= Date32("2002-04-18") AND date_dim.#<col> <= Date32("2002-06-17")
            Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
              Inner Join: catalog_sales.#<col> = item.#<col>
                Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                  Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        TableScan: ?table?

logical_plan after push_down_filter:
Projection: <exprs>
  Limit: skip=0, fetch=100
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
            Inner Join: catalog_sales.#<col> = item.#<col>
              Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: catalog_sales
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: catalog_returns
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: warehouse
                      Projection: <exprs>
                        Projection: <exprs>
                          TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: item
                    Projection: <exprs>
                      Projection: <exprs>
                        Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                          TableScan: ?table?
            Projection: <exprs>
              Projection: <exprs>
                SubqueryAlias: date_dim
                  Projection: <exprs>
                    Projection: <exprs>
                      Filter: ?table?._2 >= Date32("2002-04-18") AND ?table?._2 <= Date32("2002-06-17")
                        TableScan: ?table?

logical_plan after single_distinct_aggregation_to_group_by:
SAME TEXT AS ABOVE

logical_plan after eliminate_group_by_constant:
SAME TEXT AS ABOVE

logical_plan after common_sub_expression_eliminate:
Projection: <exprs>
  Limit: skip=0, fetch=100
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_1 < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_1 >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Projection: <exprs>
            Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
              Inner Join: catalog_sales.#<col> = item.#<col>
                Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                  Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_sales
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: catalog_returns
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Projection: <exprs>
                          Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                            TableScan: ?table?
              Projection: <exprs>
                Projection: <exprs>
                  SubqueryAlias: date_dim
                    Projection: <exprs>
                      Projection: <exprs>
                        Filter: ?table?._2 >= Date32("2002-04-18") AND ?table?._2 <= Date32("2002-06-17")
                          TableScan: ?table?

logical_plan after optimize_projections:
Projection: <exprs>
  Limit: skip=0, fetch=100
    Sort: <exprs>
      Projection: <exprs>
        Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_1 < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_1 >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
          Projection: <exprs>
            Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
              Projection: <exprs>
                Inner Join: catalog_sales.#<col> = item.#<col>
                  Projection: <exprs>
                    Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                      Projection: <exprs>
                        Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                          Projection: <exprs>
                            SubqueryAlias: catalog_sales
                              Projection: <exprs>
                                TableScan: ?table? projection=[_0, _14, _15, _17, _21]
                          Projection: <exprs>
                            SubqueryAlias: catalog_returns
                              Projection: <exprs>
                                TableScan: ?table? projection=[_2, _16, _23]
                      Projection: <exprs>
                        SubqueryAlias: warehouse
                          Projection: <exprs>
                            TableScan: ?table? projection=[_0, _10]
                  Projection: <exprs>
                    SubqueryAlias: item
                      Projection: <exprs>
                        Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                          TableScan: ?table? projection=[_0, _1, _5]
              Projection: <exprs>
                SubqueryAlias: date_dim
                  Projection: <exprs>
                    Filter: ?table?._2 >= Date32("2002-04-18") AND ?table?._2 <= Date32("2002-06-17")
                      TableScan: ?table? projection=[_0, _2]

logical_plan after eliminate_nested_union:
SAME TEXT AS ABOVE

logical_plan after simplify_expressions:
SAME TEXT AS ABOVE

logical_plan after replace_distinct_aggregate:
SAME TEXT AS ABOVE

logical_plan after eliminate_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_predicate_subquery:
SAME TEXT AS ABOVE

logical_plan after scalar_subquery_to_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_lateral_join:
SAME TEXT AS ABOVE

logical_plan after extract_equijoin_predicate:
SAME TEXT AS ABOVE

logical_plan after eliminate_duplicated_expr:
SAME TEXT AS ABOVE

logical_plan after eliminate_filter:
SAME TEXT AS ABOVE

logical_plan after eliminate_cross_join:
SAME TEXT AS ABOVE

logical_plan after eliminate_limit:
SAME TEXT AS ABOVE

logical_plan after propagate_empty_relation:
SAME TEXT AS ABOVE

logical_plan after eliminate_one_union:
SAME TEXT AS ABOVE

logical_plan after filter_null_join_keys:
SAME TEXT AS ABOVE

logical_plan after eliminate_outer_join:
SAME TEXT AS ABOVE

logical_plan after push_down_limit:
Projection: <exprs>
  Sort: <exprs>
    Projection: <exprs>
      Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_1 < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_1 >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
        Projection: <exprs>
          Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
            Projection: <exprs>
              Inner Join: catalog_sales.#<col> = item.#<col>
                Projection: <exprs>
                  Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                    Projection: <exprs>
                      Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                        Projection: <exprs>
                          SubqueryAlias: catalog_sales
                            Projection: <exprs>
                              TableScan: ?table? projection=[_0, _14, _15, _17, _21]
                        Projection: <exprs>
                          SubqueryAlias: catalog_returns
                            Projection: <exprs>
                              TableScan: ?table? projection=[_2, _16, _23]
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          TableScan: ?table? projection=[_0, _10]
                Projection: <exprs>
                  SubqueryAlias: item
                    Projection: <exprs>
                      Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                        TableScan: ?table? projection=[_0, _1, _5]
            Projection: <exprs>
              SubqueryAlias: date_dim
                Projection: <exprs>
                  Filter: ?table?._2 >= Date32("2002-04-18") AND ?table?._2 <= Date32("2002-06-17")
                    TableScan: ?table? projection=[_0, _2]

logical_plan after push_down_filter:
SAME TEXT AS ABOVE

logical_plan after single_distinct_aggregation_to_group_by:
SAME TEXT AS ABOVE

logical_plan after eliminate_group_by_constant:
SAME TEXT AS ABOVE

logical_plan after common_sub_expression_eliminate:
SAME TEXT AS ABOVE

logical_plan after optimize_projections:
SAME TEXT AS ABOVE

logical_plan after eliminate_nested_union:
SAME TEXT AS ABOVE

logical_plan after simplify_expressions:
SAME TEXT AS ABOVE

logical_plan after replace_distinct_aggregate:
SAME TEXT AS ABOVE

logical_plan after eliminate_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_predicate_subquery:
SAME TEXT AS ABOVE

logical_plan after scalar_subquery_to_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_lateral_join:
SAME TEXT AS ABOVE

logical_plan after extract_equijoin_predicate:
SAME TEXT AS ABOVE

logical_plan after eliminate_duplicated_expr:
SAME TEXT AS ABOVE

logical_plan after eliminate_filter:
SAME TEXT AS ABOVE

logical_plan after eliminate_cross_join:
SAME TEXT AS ABOVE

logical_plan after eliminate_limit:
SAME TEXT AS ABOVE

logical_plan after propagate_empty_relation:
SAME TEXT AS ABOVE

logical_plan after eliminate_one_union:
SAME TEXT AS ABOVE

logical_plan after filter_null_join_keys:
SAME TEXT AS ABOVE

logical_plan after eliminate_outer_join:
SAME TEXT AS ABOVE

logical_plan after push_down_limit:
SAME TEXT AS ABOVE

logical_plan after push_down_filter:
SAME TEXT AS ABOVE

logical_plan after single_distinct_aggregation_to_group_by:
SAME TEXT AS ABOVE

logical_plan after eliminate_group_by_constant:
SAME TEXT AS ABOVE

logical_plan after common_sub_expression_eliminate:
SAME TEXT AS ABOVE

logical_plan after optimize_projections:
SAME TEXT AS ABOVE

logical_plan:
Projection: <exprs>
  Sort: <exprs>
    Projection: <exprs>
      Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_1 < Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_1 >= Date32("2002-05-18") THEN catalog_sales.#<col> - coalesce(CAST(catalog_returns.#<col> AS Decimal128(12, 2)), Decimal128(Some(0),12,2)) ELSE Decimal128(Some(0),13,2) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]]
        Projection: <exprs>
          Inner Join: catalog_sales.#<col> = CAST(date_dim.#<col> AS Float64)
            Projection: <exprs>
              Inner Join: catalog_sales.#<col> = item.#<col>
                Projection: <exprs>
                  Inner Join: catalog_sales.#<col> = CAST(warehouse.#<col> AS Float64)
                    Projection: <exprs>
                      Left Join: catalog_sales.#<col> = catalog_returns.#<col>, catalog_sales.#<col> = catalog_returns.#<col>
                        Projection: <exprs>
                          SubqueryAlias: catalog_sales
                            Projection: <exprs>
                              TableScan: ?table? projection=[_0, _14, _15, _17, _21]
                        Projection: <exprs>
                          SubqueryAlias: catalog_returns
                            Projection: <exprs>
                              TableScan: ?table? projection=[_2, _16, _23]
                    Projection: <exprs>
                      SubqueryAlias: warehouse
                        Projection: <exprs>
                          TableScan: ?table? projection=[_0, _10]
                Projection: <exprs>
                  SubqueryAlias: item
                    Projection: <exprs>
                      Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                        TableScan: ?table? projection=[_0, _1, _5]
            Projection: <exprs>
              SubqueryAlias: date_dim
                Projection: <exprs>
                  Filter: ?table?._2 >= Date32("2002-04-18") AND ?table?._2 <= Date32("2002-06-17")
                    TableScan: ?table? projection=[_0, _2]

initial_physical_plan:
ProjectionExec: expr=[<exprs>]
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
          CoalesceBatchesExec: target_batch_size=8192
            RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
              AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                ProjectionExec: expr=[<exprs>]
                  CoalesceBatchesExec: target_batch_size=8192
                    HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                      CoalescePartitionsExec
                        CoalesceBatchesExec: target_batch_size=8192
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                            CoalescePartitionsExec
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                    ProjectionExec: expr=[<exprs>]
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192
                                  FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                      ProjectionExec: expr=[<exprs>]
                        CoalesceBatchesExec: target_batch_size=8192
                          FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]


initial_physical_plan_with_stats:
ProjectionExec: expr=[<exprs>]
  SortExec: [<sort_exprs>]
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
          CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
            RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
              AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
                ProjectionExec: expr=[<exprs>]
                  CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(84)),(Col[1]: Null=Exact(33)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Inexact(0)),(Col[4]: Null=Inexact(0))]]
                    HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(84)),(Col[1]: Null=Exact(33)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Inexact(0)),(Col[4]: Null=Inexact(0))]]
                      CoalescePartitionsExec, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(81)),(Col[1]: Null=Exact(84)),(Col[2]: Null=Exact(33)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Inexact(0))]]
                        CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(81)),(Col[1]: Null=Exact(84)),(Col[2]: Null=Exact(33)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Inexact(0))]]
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(81)),(Col[1]: Null=Exact(84)),(Col[2]: Null=Exact(33)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Inexact(0))]]
                            CoalescePartitionsExec, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(81)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(84)),(Col[3]: Null=Exact(33)),(Col[4]: Null=Exact(0))]]
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(81)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(84)),(Col[4]: Null=Exact(33))]]
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(81)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(84)),(Col[4]: Null=Exact(33))]]
                                    ProjectionExec: expr=[<exprs>]
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(1), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0))]]
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(14313), Bytes=Absent, [(Col[0]: Null=Exact(33)),(Col[1]: Null=Exact(81)),(Col[2]: Null=Exact(80)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Exact(84))]]
                                        HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(14313), Bytes=Absent, [(Col[0]: Null=Exact(33)),(Col[1]: Null=Exact(81)),(Col[2]: Null=Exact(80)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Exact(84))]]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(1358), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(33))]]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, statistics=[Rows=Exact(14313), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(81)),(Col[1]: Null=Exact(80)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Exact(84))]]
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(14313), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(81)),(Col[1]: Null=Exact(80)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(0)),(Col[4]: Null=Exact(84))]]
                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, statistics=[Rows=Inexact(36), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(36), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                                  FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>], statistics=[Rows=Inexact(36), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(180), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(0))]]
                      ProjectionExec: expr=[<exprs>]
                        CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(14610), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                          FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17, statistics=[Rows=Inexact(14610), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, statistics=[Rows=Exact(73049), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0))]]
                              DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(73049), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0))]]


initial_physical_plan_with_schema:
ProjectionExec: expr=[<exprs>]
  SortExec: [<sort_exprs>]
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)], schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END):Decimal128(23, 2);N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END):Decimal128(23, 2);N]
          CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)[sum]:Decimal128(23, 2);N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)[sum]:Decimal128(23, 2);N]
            RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>, schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)[sum]:Decimal128(23, 2);N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)[sum]:Decimal128(23, 2);N]
              AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)], schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)[sum]:Decimal128(23, 2);N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)[sum]:Decimal128(23, 2);N]
                ProjectionExec: expr=[<exprs>]
                  CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N, #<col>:Utf8;N, #<col>:Utf8;N, #<col>:Date32;N]
                    HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], schema=[#<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N, #<col>:Utf8;N, #<col>:Utf8;N, #<col>:Date32;N]
                      CoalescePartitionsExec, schema=[#<col>:Float64;N, #<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N, #<col>:Utf8;N, #<col>:Utf8;N]
                        CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Float64;N, #<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N, #<col>:Utf8;N, #<col>:Utf8;N]
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], schema=[#<col>:Float64;N, #<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N, #<col>:Utf8;N, #<col>:Utf8;N]
                            CoalescePartitionsExec, schema=[#<col>:Float64;N, #<col>:Int64;N, #<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N, #<col>:Utf8;N]
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Utf8;N, #<col>:Float64;N, #<col>:Int64;N, #<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N]
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], schema=[#<col>:Utf8;N, #<col>:Float64;N, #<col>:Int64;N, #<col>:Decimal128(5, 2);N, #<col>:Decimal128(7, 2);N]
                                    ProjectionExec: expr=[<exprs>]
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _10:Utf8;N]
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Decimal128(7, 2);N, #<col>:Float64;N, #<col>:Float64;N, #<col>:Int64;N, #<col>:Decimal128(5, 2);N]
                                        HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>], schema=[#<col>:Decimal128(7, 2);N, #<col>:Float64;N, #<col>:Float64;N, #<col>:Int64;N, #<col>:Decimal128(5, 2);N]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_2:Int64;N, _16:Int64;N, _23:Decimal128(7, 2);N]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, schema=[#<col>:Float64;N, #<col>:Float64;N, #<col>:Int64;N, #<col>:Int64;N, #<col>:Decimal128(5, 2);N]
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Float64;N, _14:Float64;N, _15:Int64;N, _17:Int64;N, _21:Decimal128(5, 2);N]
                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, schema=[#<col>:Int64;N, #<col>:Utf8;N]
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192, schema=[_0:Int64;N, _1:Utf8;N]
                                  FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>], schema=[_0:Int64;N, _1:Utf8;N]
                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _1:Utf8;N, _5:Decimal128(4, 2);N]
                      ProjectionExec: expr=[<exprs>]
                        CoalesceBatchesExec: target_batch_size=8192, schema=[_0:Int64;N, _2:Date32;N]
                          FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17, schema=[_0:Int64;N, _2:Date32;N]
                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, schema=[_0:Int64;N, _2:Date32;N]
                              DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _2:Date32;N]


physical_plan after OutputRequirements:
ProjectionExec: expr=[<exprs>]
  OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
    SortExec: [<sort_exprs>], fetch=100
      SortExec: expr=[<exprs>]
        ProjectionExec: expr=[<exprs>]
          AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
            CoalesceBatchesExec: target_batch_size=8192
              RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                  ProjectionExec: expr=[<exprs>]
                    CoalesceBatchesExec: target_batch_size=8192
                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                        CoalescePartitionsExec
                          CoalesceBatchesExec: target_batch_size=8192
                            HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                              CoalescePartitionsExec
                                ProjectionExec: expr=[<exprs>]
                                  CoalesceBatchesExec: target_batch_size=8192
                                    HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                      ProjectionExec: expr=[<exprs>]
                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                              RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                ProjectionExec: expr=[<exprs>]
                                  CoalesceBatchesExec: target_batch_size=8192
                                    FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                        ProjectionExec: expr=[<exprs>]
                          CoalesceBatchesExec: target_batch_size=8192
                            FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                              RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after aggregate_statistics:
SAME TEXT AS ABOVE

physical_plan after join_selection:
SAME TEXT AS ABOVE

physical_plan after LimitedDistinctAggregation:
SAME TEXT AS ABOVE

physical_plan after FilterPushdown:
SAME TEXT AS ABOVE

physical_plan after EnforceDistribution:
ProjectionExec: expr=[<exprs>]
  OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
    SortExec: expr=[<exprs>]
      CoalescePartitionsExec
        SortExec: expr=[<exprs>]
          ProjectionExec: expr=[<exprs>]
            AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
              RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                CoalesceBatchesExec: target_batch_size=8192
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192
                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                          CoalescePartitionsExec
                            CoalesceBatchesExec: target_batch_size=8192
                              HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                CoalescePartitionsExec
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                        ProjectionExec: expr=[<exprs>]
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                        ProjectionExec: expr=[<exprs>]
                                          CoalesceBatchesExec: target_batch_size=8192
                                            HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                              RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                ProjectionExec: expr=[<exprs>]
                                                  DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192
                              FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                  DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after CombinePartialFinalAggregate:
SAME TEXT AS ABOVE

physical_plan after EnforceSorting:
ProjectionExec: expr=[<exprs>]
  OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
    SortExec: [<sort_exprs>], fetch=100
      SortExec: expr=[<exprs>]
        ProjectionExec: expr=[<exprs>]
          AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
            RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
              CoalesceBatchesExec: target_batch_size=8192
                AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                  ProjectionExec: expr=[<exprs>]
                    CoalesceBatchesExec: target_batch_size=8192
                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                        CoalescePartitionsExec
                          CoalesceBatchesExec: target_batch_size=8192
                            HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                              CoalescePartitionsExec
                                ProjectionExec: expr=[<exprs>]
                                  CoalesceBatchesExec: target_batch_size=8192
                                    HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                      ProjectionExec: expr=[<exprs>]
                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                              RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                ProjectionExec: expr=[<exprs>]
                                  CoalesceBatchesExec: target_batch_size=8192
                                    FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                        ProjectionExec: expr=[<exprs>]
                          CoalesceBatchesExec: target_batch_size=8192
                            FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                              RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after OptimizeAggregateOrder:
SAME TEXT AS ABOVE

physical_plan after ProjectionPushdown:
SAME TEXT AS ABOVE

physical_plan after coalesce_batches:
ProjectionExec: expr=[<exprs>]
  OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
    SortExec: [<sort_exprs>], fetch=100
      SortExec: expr=[<exprs>]
        ProjectionExec: expr=[<exprs>]
          AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
            CoalesceBatchesExec: target_batch_size=8192
              RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                CoalesceBatchesExec: target_batch_size=8192
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192
                        CoalesceBatchesExec: target_batch_size=8192
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                            CoalescePartitionsExec
                              CoalesceBatchesExec: target_batch_size=8192
                                CoalesceBatchesExec: target_batch_size=8192
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                    CoalescePartitionsExec
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          CoalesceBatchesExec: target_batch_size=8192
                                            HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                              ProjectionExec: expr=[<exprs>]
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  CoalesceBatchesExec: target_batch_size=8192
                                                    HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                      RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                        ProjectionExec: expr=[<exprs>]
                                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          CoalesceBatchesExec: target_batch_size=8192
                                            FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                            ProjectionExec: expr=[<exprs>]
                              CoalesceBatchesExec: target_batch_size=8192
                                CoalesceBatchesExec: target_batch_size=8192
                                  FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after coalesce_async_exec_input:
SAME TEXT AS ABOVE

physical_plan after OutputRequirements:
ProjectionExec: expr=[<exprs>]
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
          CoalesceBatchesExec: target_batch_size=8192
            RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
              CoalesceBatchesExec: target_batch_size=8192
                AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                  ProjectionExec: expr=[<exprs>]
                    CoalesceBatchesExec: target_batch_size=8192
                      CoalesceBatchesExec: target_batch_size=8192
                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                          CoalescePartitionsExec
                            CoalesceBatchesExec: target_batch_size=8192
                              CoalesceBatchesExec: target_batch_size=8192
                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                  CoalescePartitionsExec
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        CoalesceBatchesExec: target_batch_size=8192
                                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                                    ProjectionExec: expr=[<exprs>]
                                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        CoalesceBatchesExec: target_batch_size=8192
                                          FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192
                              CoalesceBatchesExec: target_batch_size=8192
                                FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after LimitAggregation:
SAME TEXT AS ABOVE

physical_plan after LimitPushPastWindows:
SAME TEXT AS ABOVE

physical_plan after LimitPushdown:
SAME TEXT AS ABOVE

physical_plan after ProjectionPushdown:
SAME TEXT AS ABOVE

physical_plan after EnsureCooperative:
SAME TEXT AS ABOVE

physical_plan after FilterPushdown(Post):
SAME TEXT AS ABOVE

physical_plan after RewriteExplicitRepartition:
SAME TEXT AS ABOVE

physical_plan after SanityCheckPlan:
SAME TEXT AS ABOVE

physical_plan:
ProjectionExec: expr=[<exprs>]
  ProjectionExec: expr=[<exprs>]
    SortExec: [<sort_exprs>], fetch=100
      SortExec: expr=[<exprs>]
        ProjectionExec: expr=[<exprs>]
          AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
            CoalesceBatchesExec: target_batch_size=8192
              RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                CoalesceBatchesExec: target_batch_size=8192
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192
                        CoalesceBatchesExec: target_batch_size=8192
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                            CoalescePartitionsExec
                              CoalesceBatchesExec: target_batch_size=8192
                                CoalesceBatchesExec: target_batch_size=8192
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                    CoalescePartitionsExec
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          CoalesceBatchesExec: target_batch_size=8192
                                            HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                              ProjectionExec: expr=[<exprs>]
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  CoalesceBatchesExec: target_batch_size=8192
                                                    HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                      RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                        ProjectionExec: expr=[<exprs>]
                                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          CoalesceBatchesExec: target_batch_size=8192
                                            FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                            ProjectionExec: expr=[<exprs>]
                              CoalesceBatchesExec: target_batch_size=8192
                                CoalesceBatchesExec: target_batch_size=8192
                                  FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]


== Physical Plan ==
ProjectionExec: expr=[<exprs>]
  ProjectionExec: expr=[<exprs>]
    SortExec: [<sort_exprs>], fetch=100
      SortExec: expr=[<exprs>]
        ProjectionExec: expr=[<exprs>]
          AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
            CoalesceBatchesExec: target_batch_size=8192
              RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                CoalesceBatchesExec: target_batch_size=8192
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2002-05-18")) THEN catalog_sales.#<col> - coalesce(catalog_returns.#<col>,Int32(0)) WHEN Boolean(true) THEN Int32(0) END)]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192
                        CoalesceBatchesExec: target_batch_size=8192
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, CAST(date_dim.#<col> AS Float64)@<id>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                            CoalescePartitionsExec
                              CoalesceBatchesExec: target_batch_size=8192
                                CoalesceBatchesExec: target_batch_size=8192
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                    CoalescePartitionsExec
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          CoalesceBatchesExec: target_batch_size=8192
                                            HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(CAST(warehouse.#<col> AS Float64)@<id>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                              ProjectionExec: expr=[<exprs>]
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  CoalesceBatchesExec: target_batch_size=8192
                                                    HashJoinExec: mode=CollectLeft, join_type=Right, on=[(#<col>, #<col>), (#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>, #<col>]
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                      RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                        ProjectionExec: expr=[<exprs>]
                                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          CoalesceBatchesExec: target_batch_size=8192
                                            FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                            ProjectionExec: expr=[<exprs>]
                              CoalesceBatchesExec: target_batch_size=8192
                                CoalesceBatchesExec: target_batch_size=8192
                                  FilterExec: _2@<id> >= 2002-04-18 AND _2@<id> <= 2002-06-17
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]