== Codegen ==
Whole-stage codegen is not supported; showing physical plan instead.

== Plan Steps ==
initial_logical_plan:
Limit: skip=0, fetch=Int32(100)
  Sort: <exprs>
    Projection: <exprs>
      Filter: CASE WHEN x.#<col> > Int32(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) WHEN Boolean(true) THEN NULL END >= Decimal128(Some(20),2,1) / Decimal128(Some(30),2,1) AND CASE WHEN x.#<col> > Int32(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) WHEN Boolean(true) THEN NULL END <= Decimal128(Some(30),2,1) / Decimal128(Some(20),2,1)
        SubqueryAlias: x
          Projection: <exprs>
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Filter: item.#<col> >= Decimal128(Some(99),2,2) AND item.#<col> <= Decimal128(Some(149),3,2) AND item.#<col> = inventory.#<col> AND inventory.#<col> = warehouse.#<col> AND inventory.#<col> = date_dim.#<col> AND date_dim.#<col> >= spark_date(Utf8("2000-05-19")) - CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano)) AND date_dim.#<col> <= spark_date(Utf8("2000-05-19")) + CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano))
                Cross Join: 
                  Cross Join: 
                    Cross Join: 
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: inventory
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: warehouse
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?

logical_plan after resolve_grouping_function:
SAME TEXT AS ABOVE

logical_plan after type_coercion:
Limit: skip=0, fetch=CAST(Int32(100) AS Int64)
  Sort: <exprs>
    Projection: <exprs>
      Filter: CAST(CASE WHEN x.#<col> > CAST(Int32(0) AS Float64) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) WHEN Boolean(true) THEN CAST(NULL AS Float64) END AS Decimal128(30, 15)) >= CAST(Decimal128(Some(20),2,1) / Decimal128(Some(30),2,1) AS Decimal128(30, 15)) AND CAST(CASE WHEN x.#<col> > CAST(Int32(0) AS Float64) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) WHEN Boolean(true) THEN CAST(NULL AS Float64) END AS Decimal128(30, 15)) <= CAST(Decimal128(Some(30),2,1) / Decimal128(Some(20),2,1) AS Decimal128(30, 15))
        SubqueryAlias: x
          Projection: <exprs>
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN CAST(Int32(0) AS Float64) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN CAST(Int32(0) AS Float64) END)]]
              Filter: item.#<col> >= CAST(Decimal128(Some(99),2,2) AS Decimal128(4, 2)) AND item.#<col> <= CAST(Decimal128(Some(149),3,2) AS Decimal128(4, 2)) AND item.#<col> = inventory.#<col> AND inventory.#<col> = warehouse.#<col> AND inventory.#<col> = date_dim.#<col> AND date_dim.#<col> >= spark_date(Utf8("2000-05-19")) - CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano)) AND date_dim.#<col> <= spark_date(Utf8("2000-05-19")) + CAST(DurationMicrosecond("2592000000000") AS Interval(MonthDayNano))
                Cross Join: 
                  Cross Join: 
                    Cross Join: 
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: inventory
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: warehouse
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?

analyzed_logical_plan:
SAME TEXT AS ABOVE

logical_plan after eliminate_nested_union:
SAME TEXT AS ABOVE

logical_plan after simplify_expressions:
Limit: skip=0, fetch=100
  Sort: <exprs>
    Projection: <exprs>
      Filter: CAST(CASE WHEN x.#<col> > Float64(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) >= Decimal128(Some(666660000000000),30,15) AND CAST(CASE WHEN x.#<col> > Float64(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) <= Decimal128(Some(1500000000000000),30,15)
        SubqueryAlias: x
          Projection: <exprs>
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND item.#<col> = inventory.#<col> AND warehouse.#<col> = inventory.#<col> AND inventory.#<col> = date_dim.#<col> AND date_dim.#<col> >= Date32("2000-04-19") AND date_dim.#<col> <= Date32("2000-06-18")
                Cross Join: 
                  Cross Join: 
                    Cross Join: 
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: inventory
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: warehouse
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?

logical_plan after replace_distinct_aggregate:
SAME TEXT AS ABOVE

logical_plan after eliminate_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_predicate_subquery:
SAME TEXT AS ABOVE

logical_plan after scalar_subquery_to_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_lateral_join:
SAME TEXT AS ABOVE

logical_plan after extract_equijoin_predicate:
SAME TEXT AS ABOVE

logical_plan after eliminate_duplicated_expr:
SAME TEXT AS ABOVE

logical_plan after eliminate_filter:
SAME TEXT AS ABOVE

logical_plan after eliminate_cross_join:
Limit: skip=0, fetch=100
  Sort: <exprs>
    Projection: <exprs>
      Filter: CAST(CASE WHEN x.#<col> > Float64(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) >= Decimal128(Some(666660000000000),30,15) AND CAST(CASE WHEN x.#<col> > Float64(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) <= Decimal128(Some(1500000000000000),30,15)
        SubqueryAlias: x
          Projection: <exprs>
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND date_dim.#<col> >= Date32("2000-04-19") AND date_dim.#<col> <= Date32("2000-06-18")
                Inner Join: inventory.#<col> = date_dim.#<col>
                  Inner Join: inventory.#<col> = item.#<col>
                    Inner Join: inventory.#<col> = warehouse.#<col>
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: inventory
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: warehouse
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?

logical_plan after eliminate_limit:
SAME TEXT AS ABOVE

logical_plan after propagate_empty_relation:
SAME TEXT AS ABOVE

logical_plan after eliminate_one_union:
SAME TEXT AS ABOVE

logical_plan after filter_null_join_keys:
SAME TEXT AS ABOVE

logical_plan after eliminate_outer_join:
SAME TEXT AS ABOVE

logical_plan after push_down_limit:
Limit: skip=0, fetch=100
  Sort: <exprs>
    Projection: <exprs>
      Filter: CAST(CASE WHEN x.#<col> > Float64(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) >= Decimal128(Some(666660000000000),30,15) AND CAST(CASE WHEN x.#<col> > Float64(0) THEN CAST(x.#<col> AS Float64) / CAST(x.#<col> AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) <= Decimal128(Some(1500000000000000),30,15)
        SubqueryAlias: x
          Projection: <exprs>
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Filter: item.#<col> >= Decimal128(Some(99),4,2) AND item.#<col> <= Decimal128(Some(149),4,2) AND date_dim.#<col> >= Date32("2000-04-19") AND date_dim.#<col> <= Date32("2000-06-18")
                Inner Join: inventory.#<col> = date_dim.#<col>
                  Inner Join: inventory.#<col> = item.#<col>
                    Inner Join: inventory.#<col> = warehouse.#<col>
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: inventory
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: warehouse
                            Projection: <exprs>
                              Projection: <exprs>
                                TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Projection: <exprs>
                            TableScan: ?table?

logical_plan after push_down_filter:
Limit: skip=0, fetch=100
  Sort: <exprs>
    Projection: <exprs>
      SubqueryAlias: x
        Projection: <exprs>
          Filter: CAST(CASE WHEN sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) > Float64(0) THEN CAST(sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) / CAST(sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) >= Decimal128(Some(666660000000000),30,15) AND CAST(CASE WHEN sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) > Float64(0) THEN CAST(sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) / CAST(sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) <= Decimal128(Some(1500000000000000),30,15)
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN CAST(date_dim.#<col> AS Date32) < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN CAST(date_dim.#<col> AS Date32) >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Inner Join: inventory.#<col> = date_dim.#<col>
                Inner Join: inventory.#<col> = item.#<col>
                  Inner Join: inventory.#<col> = warehouse.#<col>
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: inventory
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                    Projection: <exprs>
                      Projection: <exprs>
                        SubqueryAlias: warehouse
                          Projection: <exprs>
                            Projection: <exprs>
                              TableScan: ?table?
                  Projection: <exprs>
                    Projection: <exprs>
                      SubqueryAlias: item
                        Projection: <exprs>
                          Projection: <exprs>
                            Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                              TableScan: ?table?
                Projection: <exprs>
                  Projection: <exprs>
                    SubqueryAlias: date_dim
                      Projection: <exprs>
                        Projection: <exprs>
                          Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                            TableScan: ?table?

logical_plan after single_distinct_aggregation_to_group_by:
SAME TEXT AS ABOVE

logical_plan after eliminate_group_by_constant:
SAME TEXT AS ABOVE

logical_plan after common_sub_expression_eliminate:
Limit: skip=0, fetch=100
  Sort: <exprs>
    Projection: <exprs>
      SubqueryAlias: x
        Projection: <exprs>
          Projection: <exprs>
            Filter: __common_expr_1 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_1 <= Decimal128(Some(1500000000000000),30,15)
              Projection: <exprs>
                Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                  Projection: <exprs>
                    Inner Join: inventory.#<col> = date_dim.#<col>
                      Inner Join: inventory.#<col> = item.#<col>
                        Inner Join: inventory.#<col> = warehouse.#<col>
                          Projection: <exprs>
                            Projection: <exprs>
                              SubqueryAlias: inventory
                                Projection: <exprs>
                                  Projection: <exprs>
                                    TableScan: ?table?
                          Projection: <exprs>
                            Projection: <exprs>
                              SubqueryAlias: warehouse
                                Projection: <exprs>
                                  Projection: <exprs>
                                    TableScan: ?table?
                        Projection: <exprs>
                          Projection: <exprs>
                            SubqueryAlias: item
                              Projection: <exprs>
                                Projection: <exprs>
                                  Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                    TableScan: ?table?
                      Projection: <exprs>
                        Projection: <exprs>
                          SubqueryAlias: date_dim
                            Projection: <exprs>
                              Projection: <exprs>
                                Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                                  TableScan: ?table?

logical_plan after optimize_projections:
Limit: skip=0, fetch=100
  Sort: <exprs>
    Projection: <exprs>
      SubqueryAlias: x
        Projection: <exprs>
          Filter: __common_expr_1 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_1 <= Decimal128(Some(1500000000000000),30,15)
            Projection: <exprs>
              Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                Projection: <exprs>
                  Inner Join: inventory.#<col> = date_dim.#<col>
                    Projection: <exprs>
                      Inner Join: inventory.#<col> = item.#<col>
                        Projection: <exprs>
                          Inner Join: inventory.#<col> = warehouse.#<col>
                            Projection: <exprs>
                              SubqueryAlias: inventory
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _1, _2, _3]
                            Projection: <exprs>
                              SubqueryAlias: warehouse
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _2]
                        Projection: <exprs>
                          SubqueryAlias: item
                            Projection: <exprs>
                              Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                TableScan: ?table? projection=[_0, _1, _5]
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                            TableScan: ?table? projection=[_0, _2]

logical_plan after eliminate_nested_union:
SAME TEXT AS ABOVE

logical_plan after simplify_expressions:
SAME TEXT AS ABOVE

logical_plan after replace_distinct_aggregate:
SAME TEXT AS ABOVE

logical_plan after eliminate_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_predicate_subquery:
SAME TEXT AS ABOVE

logical_plan after scalar_subquery_to_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_lateral_join:
SAME TEXT AS ABOVE

logical_plan after extract_equijoin_predicate:
SAME TEXT AS ABOVE

logical_plan after eliminate_duplicated_expr:
SAME TEXT AS ABOVE

logical_plan after eliminate_filter:
SAME TEXT AS ABOVE

logical_plan after eliminate_cross_join:
SAME TEXT AS ABOVE

logical_plan after eliminate_limit:
SAME TEXT AS ABOVE

logical_plan after propagate_empty_relation:
SAME TEXT AS ABOVE

logical_plan after eliminate_one_union:
SAME TEXT AS ABOVE

logical_plan after filter_null_join_keys:
SAME TEXT AS ABOVE

logical_plan after eliminate_outer_join:
SAME TEXT AS ABOVE

logical_plan after push_down_limit:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Filter: __common_expr_1 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_1 <= Decimal128(Some(1500000000000000),30,15)
          Projection: <exprs>
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Projection: <exprs>
                Inner Join: inventory.#<col> = date_dim.#<col>
                  Projection: <exprs>
                    Inner Join: inventory.#<col> = item.#<col>
                      Projection: <exprs>
                        Inner Join: inventory.#<col> = warehouse.#<col>
                          Projection: <exprs>
                            SubqueryAlias: inventory
                              Projection: <exprs>
                                TableScan: ?table? projection=[_0, _1, _2, _3]
                          Projection: <exprs>
                            SubqueryAlias: warehouse
                              Projection: <exprs>
                                TableScan: ?table? projection=[_0, _2]
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                              TableScan: ?table? projection=[_0, _1, _5]
                  Projection: <exprs>
                    SubqueryAlias: date_dim
                      Projection: <exprs>
                        Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                          TableScan: ?table? projection=[_0, _2]

logical_plan after push_down_filter:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Filter: CAST(CASE WHEN sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) > Float64(0) THEN CAST(sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) / CAST(sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) >= Decimal128(Some(666660000000000),30,15) AND CAST(CASE WHEN sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) > Float64(0) THEN CAST(sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) / CAST(sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) <= Decimal128(Some(1500000000000000),30,15)
            Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
              Projection: <exprs>
                Inner Join: inventory.#<col> = date_dim.#<col>
                  Projection: <exprs>
                    Inner Join: inventory.#<col> = item.#<col>
                      Projection: <exprs>
                        Inner Join: inventory.#<col> = warehouse.#<col>
                          Projection: <exprs>
                            SubqueryAlias: inventory
                              Projection: <exprs>
                                TableScan: ?table? projection=[_0, _1, _2, _3]
                          Projection: <exprs>
                            SubqueryAlias: warehouse
                              Projection: <exprs>
                                TableScan: ?table? projection=[_0, _2]
                      Projection: <exprs>
                        SubqueryAlias: item
                          Projection: <exprs>
                            Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                              TableScan: ?table? projection=[_0, _1, _5]
                  Projection: <exprs>
                    SubqueryAlias: date_dim
                      Projection: <exprs>
                        Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                          TableScan: ?table? projection=[_0, _2]

logical_plan after single_distinct_aggregation_to_group_by:
SAME TEXT AS ABOVE

logical_plan after eliminate_group_by_constant:
SAME TEXT AS ABOVE

logical_plan after common_sub_expression_eliminate:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Projection: <exprs>
            Filter: __common_expr_3 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_3 <= Decimal128(Some(1500000000000000),30,15)
              Projection: <exprs>
                Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                  Projection: <exprs>
                    Inner Join: inventory.#<col> = date_dim.#<col>
                      Projection: <exprs>
                        Inner Join: inventory.#<col> = item.#<col>
                          Projection: <exprs>
                            Inner Join: inventory.#<col> = warehouse.#<col>
                              Projection: <exprs>
                                SubqueryAlias: inventory
                                  Projection: <exprs>
                                    TableScan: ?table? projection=[_0, _1, _2, _3]
                              Projection: <exprs>
                                SubqueryAlias: warehouse
                                  Projection: <exprs>
                                    TableScan: ?table? projection=[_0, _2]
                          Projection: <exprs>
                            SubqueryAlias: item
                              Projection: <exprs>
                                Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                  TableScan: ?table? projection=[_0, _1, _5]
                      Projection: <exprs>
                        SubqueryAlias: date_dim
                          Projection: <exprs>
                            Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                              TableScan: ?table? projection=[_0, _2]

logical_plan after optimize_projections:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Filter: __common_expr_3 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_3 <= Decimal128(Some(1500000000000000),30,15)
            Projection: <exprs>
              Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                Projection: <exprs>
                  Inner Join: inventory.#<col> = date_dim.#<col>
                    Projection: <exprs>
                      Inner Join: inventory.#<col> = item.#<col>
                        Projection: <exprs>
                          Inner Join: inventory.#<col> = warehouse.#<col>
                            Projection: <exprs>
                              SubqueryAlias: inventory
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _1, _2, _3]
                            Projection: <exprs>
                              SubqueryAlias: warehouse
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _2]
                        Projection: <exprs>
                          SubqueryAlias: item
                            Projection: <exprs>
                              Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                TableScan: ?table? projection=[_0, _1, _5]
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                            TableScan: ?table? projection=[_0, _2]

logical_plan after eliminate_nested_union:
SAME TEXT AS ABOVE

logical_plan after simplify_expressions:
SAME TEXT AS ABOVE

logical_plan after replace_distinct_aggregate:
SAME TEXT AS ABOVE

logical_plan after eliminate_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_predicate_subquery:
SAME TEXT AS ABOVE

logical_plan after scalar_subquery_to_join:
SAME TEXT AS ABOVE

logical_plan after decorrelate_lateral_join:
SAME TEXT AS ABOVE

logical_plan after extract_equijoin_predicate:
SAME TEXT AS ABOVE

logical_plan after eliminate_duplicated_expr:
SAME TEXT AS ABOVE

logical_plan after eliminate_filter:
SAME TEXT AS ABOVE

logical_plan after eliminate_cross_join:
SAME TEXT AS ABOVE

logical_plan after eliminate_limit:
SAME TEXT AS ABOVE

logical_plan after propagate_empty_relation:
SAME TEXT AS ABOVE

logical_plan after eliminate_one_union:
SAME TEXT AS ABOVE

logical_plan after filter_null_join_keys:
SAME TEXT AS ABOVE

logical_plan after eliminate_outer_join:
SAME TEXT AS ABOVE

logical_plan after push_down_limit:
SAME TEXT AS ABOVE

logical_plan after push_down_filter:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Projection: <exprs>
            Filter: CAST(CASE WHEN sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) > Float64(0) THEN CAST(sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) / CAST(sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) >= Decimal128(Some(666660000000000),30,15) AND CAST(CASE WHEN sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) > Float64(0) THEN CAST(sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) / CAST(sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END) AS Float64) ELSE Float64(NULL) END AS Decimal128(30, 15)) <= Decimal128(Some(1500000000000000),30,15)
              Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                Projection: <exprs>
                  Inner Join: inventory.#<col> = date_dim.#<col>
                    Projection: <exprs>
                      Inner Join: inventory.#<col> = item.#<col>
                        Projection: <exprs>
                          Inner Join: inventory.#<col> = warehouse.#<col>
                            Projection: <exprs>
                              SubqueryAlias: inventory
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _1, _2, _3]
                            Projection: <exprs>
                              SubqueryAlias: warehouse
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _2]
                        Projection: <exprs>
                          SubqueryAlias: item
                            Projection: <exprs>
                              Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                TableScan: ?table? projection=[_0, _1, _5]
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                            TableScan: ?table? projection=[_0, _2]

logical_plan after single_distinct_aggregation_to_group_by:
SAME TEXT AS ABOVE

logical_plan after eliminate_group_by_constant:
SAME TEXT AS ABOVE

logical_plan after common_sub_expression_eliminate:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Projection: <exprs>
            Projection: <exprs>
              Filter: __common_expr_4 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_4 <= Decimal128(Some(1500000000000000),30,15)
                Projection: <exprs>
                  Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                    Projection: <exprs>
                      Inner Join: inventory.#<col> = date_dim.#<col>
                        Projection: <exprs>
                          Inner Join: inventory.#<col> = item.#<col>
                            Projection: <exprs>
                              Inner Join: inventory.#<col> = warehouse.#<col>
                                Projection: <exprs>
                                  SubqueryAlias: inventory
                                    Projection: <exprs>
                                      TableScan: ?table? projection=[_0, _1, _2, _3]
                                Projection: <exprs>
                                  SubqueryAlias: warehouse
                                    Projection: <exprs>
                                      TableScan: ?table? projection=[_0, _2]
                            Projection: <exprs>
                              SubqueryAlias: item
                                Projection: <exprs>
                                  Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                    TableScan: ?table? projection=[_0, _1, _5]
                        Projection: <exprs>
                          SubqueryAlias: date_dim
                            Projection: <exprs>
                              Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                                TableScan: ?table? projection=[_0, _2]

logical_plan after optimize_projections:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Filter: __common_expr_4 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_4 <= Decimal128(Some(1500000000000000),30,15)
            Projection: <exprs>
              Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                Projection: <exprs>
                  Inner Join: inventory.#<col> = date_dim.#<col>
                    Projection: <exprs>
                      Inner Join: inventory.#<col> = item.#<col>
                        Projection: <exprs>
                          Inner Join: inventory.#<col> = warehouse.#<col>
                            Projection: <exprs>
                              SubqueryAlias: inventory
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _1, _2, _3]
                            Projection: <exprs>
                              SubqueryAlias: warehouse
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _2]
                        Projection: <exprs>
                          SubqueryAlias: item
                            Projection: <exprs>
                              Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                TableScan: ?table? projection=[_0, _1, _5]
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                            TableScan: ?table? projection=[_0, _2]

logical_plan:
Sort: <exprs>
  Projection: <exprs>
    SubqueryAlias: x
      Projection: <exprs>
        Projection: <exprs>
          Filter: __common_expr_4 >= Decimal128(Some(666660000000000),30,15) AND __common_expr_4 <= Decimal128(Some(1500000000000000),30,15)
            Projection: <exprs>
              Aggregate: groupBy=[[warehouse.#<col>, item.#<col>]], aggr=[[sum(CASE WHEN __common_expr_2 < Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN __common_expr_2 >= Date32("2000-05-19") THEN inventory.#<col> ELSE Float64(0) END) AS sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]]
                Projection: <exprs>
                  Inner Join: inventory.#<col> = date_dim.#<col>
                    Projection: <exprs>
                      Inner Join: inventory.#<col> = item.#<col>
                        Projection: <exprs>
                          Inner Join: inventory.#<col> = warehouse.#<col>
                            Projection: <exprs>
                              SubqueryAlias: inventory
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _1, _2, _3]
                            Projection: <exprs>
                              SubqueryAlias: warehouse
                                Projection: <exprs>
                                  TableScan: ?table? projection=[_0, _2]
                        Projection: <exprs>
                          SubqueryAlias: item
                            Projection: <exprs>
                              Filter: ?table?._5 >= Decimal128(Some(99),4,2) AND ?table?._5 <= Decimal128(Some(149),4,2)
                                TableScan: ?table? projection=[_0, _1, _5]
                    Projection: <exprs>
                      SubqueryAlias: date_dim
                        Projection: <exprs>
                          Filter: ?table?._2 >= Date32("2000-04-19") AND ?table?._2 <= Date32("2000-06-18")
                            TableScan: ?table? projection=[_0, _2]

initial_physical_plan:
SortExec: [<sort_exprs>], fetch=100
  SortExec: expr=[<exprs>]
    ProjectionExec: expr=[<exprs>]
      CoalesceBatchesExec: target_batch_size=8192
        FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
          ProjectionExec: expr=[<exprs>]
            AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
              CoalesceBatchesExec: target_batch_size=8192
                RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192
                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                          CoalescePartitionsExec
                            CoalesceBatchesExec: target_batch_size=8192
                              HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                CoalescePartitionsExec
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                        ProjectionExec: expr=[<exprs>]
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                        RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192
                              FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                  DataSourceExec: partitions=1, partition_sizes=[<sizes>]


initial_physical_plan_with_stats:
SortExec: [<sort_exprs>]
  SortExec: expr=[<exprs>]
    ProjectionExec: expr=[<exprs>]
      CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
        FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
          ProjectionExec: expr=[<exprs>]
            AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
              CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
                RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]:),(Col[1]:),(Col[2]:),(Col[3]:)]]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(1162)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Inexact(0)),(Col[3]: Null=Inexact(0))]]
                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(1162)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Inexact(0)),(Col[3]: Null=Inexact(0))]]
                          CoalescePartitionsExec, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(1162)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Inexact(0))]]
                            CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(1162)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Inexact(0))]]
                              HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(1162)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Inexact(0))]]
                                CoalescePartitionsExec, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(1162)),(Col[3]: Null=Exact(0))]]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(1162))]]
                                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>], statistics=[Rows=Inexact(1), Bytes=Absent, [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(1162))]]
                                        ProjectionExec: expr=[<exprs>]
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(1), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0))]]
                                        RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, statistics=[Rows=Exact(23490), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(1162))]]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(23490), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(0)),(Col[3]: Null=Exact(1162))]]
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, statistics=[Rows=Inexact(36), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(36), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                                      FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>], statistics=[Rows=Inexact(36), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(180), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0)),(Col[2]: Null=Exact(0))]]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192, statistics=[Rows=Inexact(14610), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                              FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18, statistics=[Rows=Inexact(14610), Bytes=Inexact(<bytes>), [(Col[0]: Null=Inexact(0)),(Col[1]: Null=Inexact(0))]]
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, statistics=[Rows=Exact(73049), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0))]]
                                  DataSourceExec: partitions=1, partition_sizes=[<sizes>], statistics=[Rows=Exact(73049), Bytes=Exact(<bytes>), [(Col[0]: Null=Exact(0)),(Col[1]: Null=Exact(0))]]


initial_physical_plan_with_schema:
SortExec: [<sort_exprs>]
  SortExec: expr=[<exprs>]
    ProjectionExec: expr=[<exprs>]
      CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END):Float64;N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END):Float64;N]
        FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>], schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END):Float64;N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END):Float64;N]
          ProjectionExec: expr=[<exprs>]
            AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)], schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END):Float64;N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END):Float64;N]
              CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)[sum]:Float64;N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)[sum]:Float64;N]
                RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>, schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)[sum]:Float64;N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)[sum]:Float64;N]
                  AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)], schema=[#<col>:Utf8;N, #<col>:Utf8;N, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)[sum]:Float64;N, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)[sum]:Float64;N]
                    ProjectionExec: expr=[<exprs>]
                      CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Float64;N, #<col>:Utf8;N, #<col>:Utf8;N, #<col>:Date32;N]
                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>], schema=[#<col>:Float64;N, #<col>:Utf8;N, #<col>:Utf8;N, #<col>:Date32;N]
                          CoalescePartitionsExec, schema=[#<col>:Int64;N, #<col>:Float64;N, #<col>:Utf8;N, #<col>:Utf8;N]
                            CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Int64;N, #<col>:Float64;N, #<col>:Utf8;N, #<col>:Utf8;N]
                              HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>], schema=[#<col>:Int64;N, #<col>:Float64;N, #<col>:Utf8;N, #<col>:Utf8;N]
                                CoalescePartitionsExec, schema=[#<col>:Int64;N, #<col>:Int64;N, #<col>:Float64;N, #<col>:Utf8;N]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192, schema=[#<col>:Utf8;N, #<col>:Int64;N, #<col>:Int64;N, #<col>:Float64;N]
                                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>], schema=[#<col>:Utf8;N, #<col>:Int64;N, #<col>:Int64;N, #<col>:Float64;N]
                                        ProjectionExec: expr=[<exprs>]
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _2:Utf8;N]
                                        RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, schema=[#<col>:Int64;N, #<col>:Int64;N, #<col>:Int64;N, #<col>:Float64;N]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _1:Int64;N, _2:Int64;N, _3:Float64;N]
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, schema=[#<col>:Int64;N, #<col>:Utf8;N]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192, schema=[_0:Int64;N, _1:Utf8;N]
                                      FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>], schema=[_0:Int64;N, _1:Utf8;N]
                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _1:Utf8;N, _5:Decimal128(4, 2);N]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192, schema=[_0:Int64;N, _2:Date32;N]
                              FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18, schema=[_0:Int64;N, _2:Date32;N]
                                RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>, schema=[_0:Int64;N, _2:Date32;N]
                                  DataSourceExec: partitions=1, partition_sizes=[<sizes>], schema=[_0:Int64;N, _2:Date32;N]


physical_plan after OutputRequirements:
OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        CoalesceBatchesExec: target_batch_size=8192
          FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
            ProjectionExec: expr=[<exprs>]
              AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                CoalesceBatchesExec: target_batch_size=8192
                  RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                    AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                      ProjectionExec: expr=[<exprs>]
                        CoalesceBatchesExec: target_batch_size=8192
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                            CoalescePartitionsExec
                              CoalesceBatchesExec: target_batch_size=8192
                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                  CoalescePartitionsExec
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                            ProjectionExec: expr=[<exprs>]
                              CoalesceBatchesExec: target_batch_size=8192
                                FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after aggregate_statistics:
SAME TEXT AS ABOVE

physical_plan after join_selection:
SAME TEXT AS ABOVE

physical_plan after LimitedDistinctAggregation:
SAME TEXT AS ABOVE

physical_plan after FilterPushdown:
SAME TEXT AS ABOVE

physical_plan after EnforceDistribution:
OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
  SortExec: expr=[<exprs>]
    CoalescePartitionsExec
      SortExec: expr=[<exprs>]
        ProjectionExec: expr=[<exprs>]
          CoalesceBatchesExec: target_batch_size=8192
            FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
              ProjectionExec: expr=[<exprs>]
                AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                  RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                    CoalesceBatchesExec: target_batch_size=8192
                      AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                        ProjectionExec: expr=[<exprs>]
                          CoalesceBatchesExec: target_batch_size=8192
                            HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                              CoalescePartitionsExec
                                CoalesceBatchesExec: target_batch_size=8192
                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                    CoalescePartitionsExec
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                            RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                              ProjectionExec: expr=[<exprs>]
                                                DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      ProjectionExec: expr=[<exprs>]
                                        CoalesceBatchesExec: target_batch_size=8192
                                          FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                              ProjectionExec: expr=[<exprs>]
                                CoalesceBatchesExec: target_batch_size=8192
                                  FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after CombinePartialFinalAggregate:
SAME TEXT AS ABOVE

physical_plan after EnforceSorting:
OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        CoalesceBatchesExec: target_batch_size=8192
          FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
            ProjectionExec: expr=[<exprs>]
              AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                  CoalesceBatchesExec: target_batch_size=8192
                    AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                      ProjectionExec: expr=[<exprs>]
                        CoalesceBatchesExec: target_batch_size=8192
                          HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                            CoalescePartitionsExec
                              CoalesceBatchesExec: target_batch_size=8192
                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                  CoalescePartitionsExec
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                          ProjectionExec: expr=[<exprs>]
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            ProjectionExec: expr=[<exprs>]
                                              DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                    ProjectionExec: expr=[<exprs>]
                                      CoalesceBatchesExec: target_batch_size=8192
                                        FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                            ProjectionExec: expr=[<exprs>]
                              CoalesceBatchesExec: target_batch_size=8192
                                FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after OptimizeAggregateOrder:
SAME TEXT AS ABOVE

physical_plan after ProjectionPushdown:
SAME TEXT AS ABOVE

physical_plan after coalesce_batches:
OutputRequirementExec: order_by=[(#<col>, asc), (#<col>, asc)], dist_by=SinglePartition
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        CoalesceBatchesExec: target_batch_size=8192
          CoalesceBatchesExec: target_batch_size=8192
            FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
              ProjectionExec: expr=[<exprs>]
                AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                  CoalesceBatchesExec: target_batch_size=8192
                    RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                      CoalesceBatchesExec: target_batch_size=8192
                        AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192
                              CoalesceBatchesExec: target_batch_size=8192
                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                  CoalescePartitionsExec
                                    CoalesceBatchesExec: target_batch_size=8192
                                      CoalesceBatchesExec: target_batch_size=8192
                                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                          CoalescePartitionsExec
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                                    ProjectionExec: expr=[<exprs>]
                                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      CoalesceBatchesExec: target_batch_size=8192
                                        FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after coalesce_async_exec_input:
SAME TEXT AS ABOVE

physical_plan after OutputRequirements:
SortExec: [<sort_exprs>], fetch=100
  SortExec: expr=[<exprs>]
    ProjectionExec: expr=[<exprs>]
      CoalesceBatchesExec: target_batch_size=8192
        CoalesceBatchesExec: target_batch_size=8192
          FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
            ProjectionExec: expr=[<exprs>]
              AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                CoalesceBatchesExec: target_batch_size=8192
                  RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                    CoalesceBatchesExec: target_batch_size=8192
                      AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                        ProjectionExec: expr=[<exprs>]
                          CoalesceBatchesExec: target_batch_size=8192
                            CoalesceBatchesExec: target_batch_size=8192
                              HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                CoalescePartitionsExec
                                  CoalesceBatchesExec: target_batch_size=8192
                                    CoalesceBatchesExec: target_batch_size=8192
                                      HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                        CoalescePartitionsExec
                                          ProjectionExec: expr=[<exprs>]
                                            CoalesceBatchesExec: target_batch_size=8192
                                              CoalesceBatchesExec: target_batch_size=8192
                                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                                  ProjectionExec: expr=[<exprs>]
                                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                  RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                    ProjectionExec: expr=[<exprs>]
                                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                        RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                          ProjectionExec: expr=[<exprs>]
                                            CoalesceBatchesExec: target_batch_size=8192
                                              CoalesceBatchesExec: target_batch_size=8192
                                                FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                                  DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                ProjectionExec: expr=[<exprs>]
                                  CoalesceBatchesExec: target_batch_size=8192
                                    CoalesceBatchesExec: target_batch_size=8192
                                      FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                        RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                          DataSourceExec: partitions=1, partition_sizes=[<sizes>]


physical_plan after LimitAggregation:
SAME TEXT AS ABOVE

physical_plan after LimitPushPastWindows:
SAME TEXT AS ABOVE

physical_plan after LimitPushdown:
SAME TEXT AS ABOVE

physical_plan after ProjectionPushdown:
SAME TEXT AS ABOVE

physical_plan after EnsureCooperative:
SAME TEXT AS ABOVE

physical_plan after FilterPushdown(Post):
SAME TEXT AS ABOVE

physical_plan after RewriteExplicitRepartition:
SAME TEXT AS ABOVE

physical_plan after SanityCheckPlan:
SAME TEXT AS ABOVE

physical_plan:
ProjectionExec: expr=[<exprs>]
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        CoalesceBatchesExec: target_batch_size=8192
          CoalesceBatchesExec: target_batch_size=8192
            FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
              ProjectionExec: expr=[<exprs>]
                AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                  CoalesceBatchesExec: target_batch_size=8192
                    RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                      CoalesceBatchesExec: target_batch_size=8192
                        AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192
                              CoalesceBatchesExec: target_batch_size=8192
                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                  CoalescePartitionsExec
                                    CoalesceBatchesExec: target_batch_size=8192
                                      CoalesceBatchesExec: target_batch_size=8192
                                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                          CoalescePartitionsExec
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                                    ProjectionExec: expr=[<exprs>]
                                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      CoalesceBatchesExec: target_batch_size=8192
                                        FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]


== Physical Plan ==
ProjectionExec: expr=[<exprs>]
  SortExec: [<sort_exprs>], fetch=100
    SortExec: expr=[<exprs>]
      ProjectionExec: expr=[<exprs>]
        CoalesceBatchesExec: target_batch_size=8192
          CoalesceBatchesExec: target_batch_size=8192
            FilterExec: __common_expr_4@<id> >= Some(666660000000000),30,15 AND __common_expr_4@<id> <= Some(1500000000000000),30,15, projection=[#<col>, #<col>, sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>, sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)@<id>]
              ProjectionExec: expr=[<exprs>]
                AggregateExec: mode=FinalPartitioned, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                  CoalesceBatchesExec: target_batch_size=8192
                    RepartitionExec: partitioning=Hash([#<col>, #<col>], <partitions>), input_partitions=<partitions>
                      CoalesceBatchesExec: target_batch_size=8192
                        AggregateExec: mode=Partial, gby=[#<col> as #<col>, #<col> as #<col>], aggr=[sum(CASE WHEN date_dim.#<col> < spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END), sum(CASE WHEN date_dim.#<col> >= spark_date(Utf8("2000-05-19")) THEN inventory.#<col> WHEN Boolean(true) THEN Int32(0) END)]
                          ProjectionExec: expr=[<exprs>]
                            CoalesceBatchesExec: target_batch_size=8192
                              CoalesceBatchesExec: target_batch_size=8192
                                HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                  CoalescePartitionsExec
                                    CoalesceBatchesExec: target_batch_size=8192
                                      CoalesceBatchesExec: target_batch_size=8192
                                        HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                          CoalescePartitionsExec
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(#<col>, #<col>)], projection=[#<col>, #<col>, #<col>, #<col>]
                                                    ProjectionExec: expr=[<exprs>]
                                                      DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                                    RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                                      ProjectionExec: expr=[<exprs>]
                                                        DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            ProjectionExec: expr=[<exprs>]
                                              CoalesceBatchesExec: target_batch_size=8192
                                                CoalesceBatchesExec: target_batch_size=8192
                                                  FilterExec: _5@<id> >= Some(99),4,2 AND _5@<id> <= Some(149),4,2, projection=[_0@<id>, _1@<id>]
                                                    DataSourceExec: partitions=1, partition_sizes=[<sizes>]
                                  ProjectionExec: expr=[<exprs>]
                                    CoalesceBatchesExec: target_batch_size=8192
                                      CoalesceBatchesExec: target_batch_size=8192
                                        FilterExec: _2@<id> >= 2000-04-19 AND _2@<id> <= 2000-06-18
                                          RepartitionExec: partitioning=RoundRobinBatch(<partitions>), input_partitions=<partitions>
                                            DataSourceExec: partitions=1, partition_sizes=[<sizes>]