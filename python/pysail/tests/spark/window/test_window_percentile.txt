>>> from pyspark.sql import functions as sf, types as T

# OVER tests
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...         ORDER BY x
...         ) AS running_p50
...     FROM (VALUES (0), (1), (2), (3), (4)) AS t(x);
... """).show()
+---+-----------+
|  x|running_p50|
+---+-----------+
|  0|        0.0|
|  1|        0.5|
|  2|        1.0|
|  3|        1.5|
|  4|        2.0|
+---+-----------+

>>> spark.sql("""
... SELECT
...     x,
...     percentile(x, 1.0) OVER (
...         ORDER BY x
...         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
...     ) AS running_p50
... FROM VALUES (0), (1), (2), (3), (4), (5) AS t(x);
... """).show()
+---+-----------+
|  x|running_p50|
+---+-----------+
|  0|        0.0|
|  1|        1.0|
|  2|        2.0|
|  3|        3.0|
|  4|        4.0|
|  5|        5.0|
+---+-----------+
>>> spark.sql("""
... SELECT
...     x,
...     percentile(x, 0.5) OVER (
...         ORDER BY x
...         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
...     ) AS running_p50
... FROM VALUES (0), (1), (2), (3), (4), (5) AS t(x);
... """).show()
+---+-----------+
|  x|running_p50|
+---+-----------+
|  0|        0.0|
|  1|        0.5|
|  2|        1.0|
|  3|        1.5|
|  4|        2.0|
|  5|        2.5|
+---+-----------+

>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
...         ) AS sliding_p50
...     FROM VALUES (0), (1), (2), (3), (4), (5) AS t(x)
... """).show()
+---+-----------+
|  x|sliding_p50|
+---+-----------+
|  0|        0.0|
|  1|        0.5|
|  2|        1.0|
|  3|        2.0|
|  4|        3.0|
|  5|        4.0|
+---+-----------+

# Test with PARTITION BY
>>> spark.sql("""
...     SELECT
...         group,
...         x,
...         percentile(x, 0.5) OVER (
...             PARTITION BY group
...             ORDER BY x
...         ) AS running_p50
...     FROM VALUES ('A', 1), ('A', 2), ('A', 3), ('B', 10), ('B', 20), ('B', 30) AS t(group, x)
...     ORDER BY group, x
... """).show()
+-----+---+-----------+
|group|  x|running_p50|
+-----+---+-----------+
|    A|  1|        1.0|
|    A|  2|        1.5|
|    A|  3|        2.0|
|    B| 10|       10.0|
|    B| 20|       15.0|
|    B| 30|       20.0|
+-----+---+-----------+

# Test with PARTITION BY and sliding window
>>> spark.sql("""
...     SELECT
...         group,
...         x,
...         percentile(x, 0.5) OVER (
...             PARTITION BY group
...             ORDER BY x
...             ROWS BETWEEN 1 PRECEDING AND CURRENT ROW
...         ) AS sliding_p50
...     FROM VALUES ('A', 1), ('A', 2), ('A', 3), ('A', 4), ('B', 10), ('B', 20), ('B', 30) AS t(group, x)
...     ORDER BY group, x
... """).show()
+-----+---+-----------+
|group|  x|sliding_p50|
+-----+---+-----------+
|    A|  1|        1.0|
|    A|  2|        1.5|
|    A|  3|        2.5|
|    A|  4|        3.5|
|    B| 10|       10.0|
|    B| 20|       15.0|
|    B| 30|       25.0|
+-----+---+-----------+

# Test with ROWS BETWEEN ... AND ... FOLLOWING
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
...         ) AS centered_p50
...     FROM VALUES (0), (10), (20), (30), (40) AS t(x)
... """).show()
+---+------------+
|  x|centered_p50|
+---+------------+
|  0|         5.0|
| 10|        10.0|
| 20|        20.0|
| 30|        30.0|
| 40|        35.0|
+---+------------+

# Test with ROWS BETWEEN CURRENT ROW AND ... FOLLOWING
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING
...         ) AS forward_p50
...     FROM VALUES (0), (1), (2), (3), (4) AS t(x)
... """).show()
+---+-----------+
|  x|forward_p50|
+---+-----------+
|  0|        1.0|
|  1|        2.0|
|  2|        3.0|
|  3|        3.5|
|  4|        4.0|
+---+-----------+

# Test window with intervals (YearMonth)
>>> spark.sql("""
...     SELECT
...         col,
...         percentile(col, 0.5) OVER (
...             ORDER BY col
...         ) AS running_median
...     FROM VALUES (interval '0 months'), (interval '10 months'), (interval '20 months') AS t(col)
... """).show(truncate=False)
+-------------------+-----------------------------+
|col                |running_median               |
+-------------------+-----------------------------+
|INTERVAL '0' MONTH |INTERVAL '0-0' YEAR TO MONTH |
|INTERVAL '10' MONTH|INTERVAL '0-5' YEAR TO MONTH |
|INTERVAL '20' MONTH|INTERVAL '0-10' YEAR TO MONTH|
+-------------------+-----------------------------+

# Test window with intervals (DayTime)

>>> spark.sql("""
...     SELECT
...         col,
...         percentile(col, 0.5) OVER (
...             ORDER BY col
...         ) AS running_median
...     FROM VALUES (interval '0 seconds'), (interval '10 seconds'), (interval '20 seconds') AS t(col)
... """).show(truncate=False)
+--------------------+-----------------------------------+
|col                 |running_median                     |
+--------------------+-----------------------------------+
|INTERVAL '00' SECOND|INTERVAL '0 00:00:00' DAY TO SECOND|
|INTERVAL '10' SECOND|INTERVAL '0 00:00:05' DAY TO SECOND|
|INTERVAL '20' SECOND|INTERVAL '0 00:00:10' DAY TO SECOND|
+--------------------+-----------------------------------+

# Test window with different percentiles
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.25) OVER (ORDER BY x) AS p25,
...         percentile(x, 0.50) OVER (ORDER BY x) AS p50,
...         percentile(x, 0.75) OVER (ORDER BY x) AS p75
...     FROM VALUES (0), (10), (20), (30), (40) AS t(x)
... """).show()
+---+----+----+----+
|  x| p25| p50| p75|
+---+----+----+----+
|  0| 0.0| 0.0| 0.0|
| 10| 2.5| 5.0| 7.5|
| 20| 5.0|10.0|15.0|
| 30| 7.5|15.0|22.5|
| 40|10.0|20.0|30.0|
+---+----+----+----+

# Test window with NULLs
>>> spark.sql("""
...     SELECT
...         x,
...         percentile(x, 0.5) OVER (
...             ORDER BY x
...             ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
...         ) AS running_median
...     FROM VALUES (0), (NULL), (10), (NULL), (20), (30) AS t(x)
... """).show()
+----+--------------+
|   x|running_median|
+----+--------------+
|NULL|          NULL|
|NULL|          NULL|
|   0|           0.0|
|  10|           5.0|
|  20|          10.0|
|  30|          15.0|
+----+--------------+

# Test PARTITION BY with multiple partitions and different percentiles
>>> spark.sql("""
...     SELECT
...         category,
...         value,
...         percentile(value, 0.5) OVER (PARTITION BY category ORDER BY value) AS p50,
...         percentile(value, 0.9) OVER (PARTITION BY category ORDER BY value) AS p90
...     FROM VALUES
...         ('A', 1), ('A', 2), ('A', 3), ('A', 4), ('A', 5),
...         ('B', 10), ('B', 20), ('B', 30)
...     AS t(category, value)
...     ORDER BY category, value
... """).show()
+--------+-----+----+------------------+
|category|value| p50|               p90|
+--------+-----+----+------------------+
|       A|    1| 1.0|               1.0|
|       A|    2| 1.5|               1.9|
|       A|    3| 2.0|2.8000000000000003|
|       A|    4| 2.5|               3.7|
|       A|    5| 3.0|               4.6|
|       B|   10|10.0|              10.0|
|       B|   20|15.0|              19.0|
|       B|   30|20.0|              28.0|
+--------+-----+----+------------------+
