>>> from pyspark.sql import functions as sf

>>> df = spark.createDataFrame([(6000, 15), (1990, 2)], ["a", "b"])
>>> df.select(sf.try_divide(df.a, df.b).alias('r')).show()
+-----+
|    r|
+-----+
|400.0|
|995.0|
+-----+

>>> spark.range(1).select(sf.try_divide(sf.make_interval(days=sf.lit(1)), sf.lit(2))).show()
+-------------------------------------------------+
|try_divide(make_interval(0, 0, 0, 1, 0, 0, 0), 2)|
+-------------------------------------------------+
|                                         12 hours|
+-------------------------------------------------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(hours=sf.lit(10)), sf.lit(4)).alias("result")
... ).show(truncate=False)
+------------------+
|result            |
+------------------+
|2 hours 30 minutes|
+------------------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(days=sf.lit(3), hours=sf.lit(12)), sf.lit(3)).alias("result")
... ).show(truncate=False)
+--------------+
|result        |
+--------------+
|1 days 4 hours|
+--------------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(secs=sf.lit(10.5)), sf.lit(2)).alias("result")
... ).show(truncate=False)
+------------+
|result      |
+------------+
|5.25 seconds|
+------------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(weeks=sf.lit(1)), sf.lit(2)).alias("result")
... ).show(truncate=False)
+---------------+
|result         |
+---------------+
|3 days 12 hours|
+---------------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(days=sf.lit(1)), sf.lit(None).cast("int")).alias("result")
... ).show(truncate=False)
+------+
|result|
+------+
|NULL  |
+------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(days=sf.lit(1)), sf.lit(0)).alias("result")
... ).show(truncate=False)
+------+
|result|
+------+
|NULL  |
+------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(days=sf.lit(1)), sf.lit(2)).alias("result")
... ).show(truncate=False)
+--------+
|result  |
+--------+
|12 hours|
+--------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(days=sf.lit(-1)), sf.lit(2)).alias("result")
... ).show(truncate=False)
+---------+
|result   |
+---------+
|-12 hours|
+---------+

>>> spark.range(1).select(
...     sf.try_divide(sf.make_interval(days=sf.lit(1), mins=sf.lit(90)), sf.lit(2)).alias("result")
... ).show(truncate=False)
+-------------------+
|result             |
+-------------------+
|12 hours 45 minutes|
+-------------------+

>>> spark.sql("SELECT try_divide(make_ym_interval(1, 6), 2) AS result").show(truncate=False)
+----------------------------+
|result                      |
+----------------------------+
|INTERVAL '0-9' YEAR TO MONTH|
+----------------------------+

>>> spark.range(1).selectExpr("try_divide(make_ym_interval(1, 6), 2) AS result").show(truncate=False)
+----------------------------+
|result                      |
+----------------------------+
|INTERVAL '0-9' YEAR TO MONTH|
+----------------------------+

>>> spark.range(1).select(sf.try_divide(sf.make_interval(secs=sf.lit(1)), sf.lit(2))).show()
+-------------------------------------------------+
|try_divide(make_interval(0, 0, 0, 0, 0, 0, 1), 2)|
+-------------------------------------------------+
|                                      0.5 seconds|
+-------------------------------------------------+
