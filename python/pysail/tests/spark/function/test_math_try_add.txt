>>> from pyspark.sql.types import StructType, StructField, IntegerType
>>> import pyspark.sql.functions as sf

>>> schema = StructType([StructField("birth", IntegerType(), True),StructField("age", IntegerType(), True),])
>>> df = spark.createDataFrame([(1982, 15),(1990, 2),(None, 10),(2147483647, 1),(-2147483648,-1),],schema=schema)
>>> df.select("*", sf.try_add("birth", "age").alias("birth_plus_age")).show()
+-----------+---+--------------+
|      birth|age|birth_plus_age|
+-----------+---+--------------+
|       1982| 15|          1997|
|       1990|  2|          1992|
|       NULL| 10|          NULL|
| 2147483647|  1|          NULL|
|-2147483648| -1|          NULL|
+-----------+---+--------------+

>>> df = spark.sql("""
... SELECT
...   try_add(DATE '2015-09-30', 1) as d1,
...   try_add(DATE '2000-01-01', 366) as d2,
...   try_add(DATE '2021-01-01', 1) as d3,
...   try_add(NULL, 100) as d4
... """)
>>> df.show()
+----------+----------+----------+----+
|        d1|        d2|        d3|  d4|
+----------+----------+----------+----+
|2015-10-01|2001-01-01|2021-01-02|NULL|
+----------+----------+----------+----+

>>> df = spark.sql("""
... SELECT
...   try_add(DATE '2015-01-31', INTERVAL 1 MONTH) as d1,
...   try_add(DATE '2020-02-29', INTERVAL 12 MONTH) as d2,
...   try_add(NULL, INTERVAL 3 MONTH) as d3
... """)
>>> df.show()
+----------+----------+----+
|        d1|        d2|  d3|
+----------+----------+----+
|2015-02-28|2021-02-28|NULL|
+----------+----------+----+

>>> df = spark.sql("""
... SELECT
...   try_add(DATE '2000-07-31', INTERVAL -1 MONTH) as d1,
...   try_add(DATE '2021-01-31', INTERVAL -1 MONTH) as d2
... """)
>>> df.show()
+----------+----------+
|        d1|        d2|
+----------+----------+
|2000-06-30|2020-12-31|
+----------+----------+

>>> df = spark.sql("""
... SELECT
...   try_add(INTERVAL '1' YEAR, INTERVAL '2' YEAR) as result
... """)
>>> # INTERVAL '3' YEAR            <-- sail
>>> # INTERVAL '3-0' YEAR TO MONTH <-- PySpark

>>> spark.conf.set("spark.sql.session.timeZone", "UTC")
>>> df = spark.sql("""
... SELECT
...   try_add(TIMESTAMP '2021-01-01 00:00:00', INTERVAL 1 DAY) as result
... """)
>>> df.show(truncate=False)
+-------------------+
|result             |
+-------------------+
|2021-01-02 00:00:00|
+-------------------+
