# ============================================================================
# UNIFORM FUNCTION - Random Number Generation
# ============================================================================
#
# IMPLEMENTATION DIFFERENCES:
# ---------------------------
# Spark (Apache Spark 4.0+):
#   - RNG: java.util.Random (Linear Congruential Generator)
#   - Integer Type: Int32 (integer in schema)
#   - Algorithm: LCG with seed
#   - Values: Deterministic per seed with Java's RNG
#
# Sail (Rust/DataFusion):
#   - RNG: rand::rngs::StdRng (ChaCha20-based RNG)
#   - Integer Type: Int32 (integer in schema)
#   - Algorithm: ChaCha20 cryptographic RNG
#   - Values: Deterministic per seed with Rust's RNG
#
# RESULT: Generated values differ between Spark and Sail due to different
#         RNG implementations, but both are statistically uniform and
#         reproducible with the same seed in their respective environments.
# ============================================================================

# Note: Sail generates 18 with seed 0 (Rust StdRng/ChaCha20)
#       Spark generates 17 with seed 0 (Java Random/LCG)
>>> spark.sql("SELECT uniform(10, 20, 0) AS result").show()
+------+
|result|
+------+
|    18|
+------+

>>> spark.sql("SELECT uniform(0, 100, 42) AS result").show()
+------+
|result|
+------+
|    13|
+------+

>>> spark.sql("SELECT uniform(5.5, 10.5, 123) AS result").show()
+------+
|result|
+------+
|   6.2|
+------+

>>> spark.sql("SELECT uniform(5, 105, -3) AS result").show()
+------+
|result|
+------+
|    17|
+------+

>>> spark.sql("SELECT uniform(10, 20) AS result").printSchema()
root
 |-- result: integer (nullable = false)

>>> spark.sql("SELECT uniform(5.5, 10.5, 123) AS result").printSchema()
root
 |-- result: decimal(3,1) (nullable = false)

>>> spark.sql("SELECT uniform(5.5, 10, 123) AS result").printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("SELECT uniform(10, 20, 0) AS result").printSchema()
root
 |-- result: integer (nullable = false)

>>> INT_MAX = 2147483647
>>> spark.sql(f"SELECT uniform({INT_MAX}, {INT_MAX}, 0) AS result").printSchema()
root
 |-- result: integer (nullable = false)

>>> spark.sql(f"SELECT uniform({INT_MAX}, 21474836471, 0) AS result").printSchema()
root
 |-- result: long (nullable = false)

spark.sql("""
  SELECT uniform(
    CAST(2147483648 AS BIGINT),
    CAST(9223372036854775807 AS BIGINT),
    42
  ) AS result
""").printSchema()
root
 |-- result: long (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     CAST(9223372036854775807 AS BIGINT),
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: long (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     CAST(9223372036854775807 AS DECIMAL(20,0)),
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(20,0) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     1234567890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1234567890,
...     1.2,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     12345.67890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(10,5) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     12.34567890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(10,8) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     12.34567890,
...     9999,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(10,8) (nullable = false)

# Spark's Decimal Type Rule: When one decimal has significantly larger precision,
# use that decimal's type completely (precision AND scale), not max(scale).
# Input: 1.2 is Decimal(2,1) and 12345678901234567890 is Decimal(20,0)
# Result: Since p2 (20) >> p1 (2), use Decimal(20,0) completely (not max(1,0)=1)
>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     12345678901234567890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(20,0) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     CAST(12345678901234567890 AS DECIMAL(20,0)),
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(20,0) (nullable = false)

# Decimal + Int32 → uses Decimal type (ignores Int32)
# INT_MAX (2147483647) is Int32, so it gets coerced based on Decimal presence
>>> INT_MAX = 2**31 - 1
>>> spark.sql(f"""
...   SELECT uniform(
...     1.2,
...     {INT_MAX},
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

# Decimal + BIGINT → uses Decimal type (ignores Int64)
>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     CAST(9223372036854775807 AS BIGINT),
...     43
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("SELECT uniform(5.65, 100.0, 123) AS result").printSchema()
root
 |-- result: decimal(4,1) (nullable = false)

# With ANSI mode enabled (default), decimal overflow raises exception
# Note: The exception is Py4JJavaError wrapping SparkArithmeticException
# NUMERIC_VALUE_OUT_OF_RANGE: 6195.975878639691 cannot be represented as Decimal(10, 8)
#>>> try:  # doctest: +SKIP
#...     spark.sql("SELECT uniform(12.34567890, 9999, 42) AS result").show()
#... except Exception as e:
#...     # In real execution, this raises:
#...     # Py4JJavaError: org.apache.spark.SparkArithmeticException
#...     # [NUMERIC_VALUE_OUT_OF_RANGE.WITH_SUGGESTION]
#...     print("Exception caught: NUMERIC_VALUE_OUT_OF_RANGE")
#Exception caught: NUMERIC_VALUE_OUT_OF_RANGE

# >>> spark.conf.set("spark.sql.ansi.enabled", "false")
# >>> spark.sql("""
# ...   SELECT uniform(
# ...     12.34567890,
# ...     9999,
# ...     42
# ...   ) AS result
# ... """).show()
# +------+
# |result|
# +------+
# |  NULL|
# +------+

# Test Int8 (TINYINT)
>>> spark.sql("SELECT uniform(CAST(10 AS TINYINT), CAST(20 AS TINYINT), 0) AS result").printSchema()
root
 |-- result: byte (nullable = false)

# Test Int16 (SMALLINT)
>>> spark.sql("SELECT uniform(CAST(100 AS SMALLINT), CAST(200 AS SMALLINT), 0) AS result").printSchema()
root
 |-- result: short (nullable = false)

# Test Float (FLOAT)
>>> spark.sql("SELECT uniform(CAST(5.5 AS FLOAT), CAST(10.5 AS FLOAT), 123) AS result").printSchema()
root
 |-- result: float (nullable = false)

# Test Int32 (INT)
>>> spark.sql("SELECT uniform(10, 20, 0) AS result").printSchema()
root
 |-- result: integer (nullable = false)

# Test Int64 (BIGINT)
>>> spark.sql("SELECT uniform(CAST(2147483648 AS BIGINT), CAST(9223372036854775807 AS BIGINT), 42) AS result").printSchema()
root
 |-- result: long (nullable = false)

# Test Double (DOUBLE)
>>> spark.sql("SELECT uniform(5.5, 10.5, 123) AS result").printSchema()
root
 |-- result: decimal(3,1) (nullable = false)
