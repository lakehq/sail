# ============================================================================
# UNIFORM FUNCTION - Random Number Generation
# ============================================================================
#
# IMPLEMENTATION DIFFERENCES:
# ---------------------------
# Spark (Apache Spark 4.0+):
#   - RNG: java.util.Random (Linear Congruential Generator)
#   - Integer Type: Int32 (integer in schema)
#   - Algorithm: LCG with seed
#   - Values: Deterministic per seed with Java's RNG
#
# Sail (Rust/DataFusion):
#   - RNG: rand::rngs::StdRng (ChaCha20-based RNG)
#   - Integer Type: Int32 (integer in schema)
#   - Algorithm: ChaCha20 cryptographic RNG
#   - Values: Deterministic per seed with Rust's RNG
#
# RESULT: Generated values differ between Spark and Sail due to different
#         RNG implementations, but both are statistically uniform and
#         reproducible with the same seed in their respective environments.
# ============================================================================

# Note: Sail generates 18 with seed 0 (Rust StdRng/ChaCha20)
#       Spark generates 17 with seed 0 (Java Random/LCG)
>>> spark.sql("SELECT uniform(10, 20, 0) AS result").show()
+------+
|result|
+------+
|    18|
+------+

>>> spark.sql("SELECT uniform(0, 100, 42) AS result").show()
+------+
|result|
+------+
|    13|
+------+

>>> spark.sql("SELECT uniform(5.5, 10.5, 123) AS result").show()
+------+
|result|
+------+
|   6.4|
+------+

>>> spark.sql("SELECT uniform(5, 105, -3) AS result").show()
+------+
|result|
+------+
|    17|
+------+

>>> spark.sql("SELECT uniform(10, 20) AS result").printSchema()
root
 |-- result: integer (nullable = false)

>>> spark.sql("SELECT uniform(5.5, 10.5, 123) AS result").printSchema()
root
 |-- result: decimal(3,1) (nullable = false)

>>> spark.sql("SELECT uniform(5.5, 10, 123) AS result").printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("SELECT uniform(10, 20, 0) AS result").printSchema()
root
 |-- result: integer (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     CAST(2147483648 AS BIGINT),
...     CAST(9223372036854775807 AS BIGINT),
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: long (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     CAST(9223372036854775807 AS BIGINT),
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: long (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     CAST(9223372036854775807 AS DECIMAL(20,0)),
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(20,0) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     1234567890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1234567890,
...     1.2,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     12345.67890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(10,5) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1,
...     12.34567890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(10,8) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     12.34567890,
...     9999,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(10,8) (nullable = false)

# check here spark |-- result: decimal(20,0) (nullable = false)
>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     12345678901234567890,
...     42
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(20,1) (nullable = false)

>>> spark.sql("""
...   SELECT uniform(
...     1.2,
...     CAST(9223372036854775807 AS BIGINT),
...     43
...   ) AS result
... """).printSchema()
root
 |-- result: decimal(2,1) (nullable = false)


# With ANSI mode enabled (default), decimal overflow raises exception
# Note: The exception is Py4JJavaError wrapping SparkArithmeticException
# NUMERIC_VALUE_OUT_OF_RANGE: 6195.975878639691 cannot be represented as Decimal(10, 8)
#>>> try:  # doctest: +SKIP
#...     spark.sql("SELECT uniform(12.34567890, 9999, 42) AS result").show()
#... except Exception as e:
#...     # In real execution, this raises:
#...     # Py4JJavaError: org.apache.spark.SparkArithmeticException
#...     # [NUMERIC_VALUE_OUT_OF_RANGE.WITH_SUGGESTION]
#...     print("Exception caught: NUMERIC_VALUE_OUT_OF_RANGE")
#Exception caught: NUMERIC_VALUE_OUT_OF_RANGE

# >>> spark.conf.set("spark.sql.ansi.enabled", "false")
# >>> spark.sql("""
# ...   SELECT uniform(
# ...     12.34567890,
# ...     9999,
# ...     42
# ...   ) AS result
# ... """).show()
# +------+
# |result|
# +------+
# |  NULL|
# +------+
