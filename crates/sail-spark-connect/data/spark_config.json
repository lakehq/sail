{
  "sparkVersion": "4.1.0",
  "comment": "This is a generated file. Do not edit it directly.",
  "entries": [
    {
      "key": "spark.python.sql.dataFrameDebugging.enabled",
      "doc": "Enable the DataFrame debugging. This feature is enabled by default, but has a non-trivial performance overhead because of the stack trace collection.",
      "defaultValue": "true",
      "isStatic": true
    },
    {
      "key": "spark.sql.adaptive.advisoryPartitionSizeInBytes",
      "doc": "The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.",
      "fallback": "spark.sql.adaptive.shuffle.targetPostShuffleInputSize"
    },
    {
      "key": "spark.sql.adaptive.applyFinalStageShuffleOptimizations",
      "doc": "Configures whether adaptive query execution (if enabled) should apply shuffle coalescing and local shuffle read optimization for the final query stage.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.autoBroadcastJoinThreshold",
      "doc": "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. The default value is same with spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework."
    },
    {
      "key": "spark.sql.adaptive.coalescePartitions.enabled",
      "doc": "When true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.coalescePartitions.initialPartitionNum",
      "doc": "The initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true."
    },
    {
      "key": "spark.sql.adaptive.coalescePartitions.minPartitionNum",
      "doc": "(deprecated) The suggested (not guaranteed) minimum number of shuffle partitions after coalescing. If not set, the default value is the default parallelism of the Spark cluster. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.",
      "deprecated": {
        "version": "3.2",
        "comment": "Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead."
      }
    },
    {
      "key": "spark.sql.adaptive.coalescePartitions.minPartitionSize",
      "doc": "The minimum size of shuffle partitions after coalescing. This is useful when the adaptively calculated target size is too small during partition coalescing.",
      "defaultValue": "1MB"
    },
    {
      "key": "spark.sql.adaptive.coalescePartitions.parallelismFirst",
      "doc": "When true, Spark does not respect the target size specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes' (default 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark cluster. The calculated size is usually smaller than the configured target size. This is to maximize the parallelism and avoid performance regressions when enabling adaptive query execution. It's recommended to set this config to false on a busy cluster to make resource utilization more efficient (not many small tasks).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.customCostEvaluatorClass",
      "doc": "The custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own SimpleCostEvaluator by default."
    },
    {
      "key": "spark.sql.adaptive.enabled",
      "doc": "When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.fetchShuffleBlocksInBatch",
      "doc": "Whether to fetch the contiguous shuffle blocks in batch. Instead of fetching blocks one by one, fetching contiguous shuffle blocks for the same map task in batch can reduce IO and improve performance. Note, multiple contiguous blocks exist in single fetch request only happen when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true. This feature also depends on a relocatable serializer, the concatenation support codec in use, the new version shuffle fetch protocol and io encryption is disabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.forceApply",
      "doc": "Adaptive query execution is skipped when the query does not have exchanges or sub-queries. By setting this config to true (together with 'spark.sql.adaptive.enabled' set to true), Spark will force apply adaptive query execution for all supported queries.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.adaptive.forceOptimizeSkewedJoin",
      "doc": "When true, force enable OptimizeSkewedJoin even if it introduces extra shuffle.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.adaptive.localShuffleReader.enabled",
      "doc": "When true and 'spark.sql.adaptive.enabled' is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.logLevel",
      "doc": "Configures the log level for adaptive execution logging of plan changes. The value can be ERROR, WARN, INFO, DEBUG, TRACE.",
      "defaultValue": "DEBUG"
    },
    {
      "key": "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold",
      "doc": "Configures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than spark.sql.adaptive.advisoryPartitionSizeInBytes and all the partition size are not larger than this config, join selection prefer to use shuffled hash join instead of sort merge join regardless of the value of spark.sql.join.preferSortMergeJoin.",
      "defaultValue": "0b"
    },
    {
      "key": "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin",
      "doc": "The relation with a non-empty partition ratio lower than this config will not be considered as the build side of a broadcast-hash join in adaptive execution regardless of its size.This configuration only has an effect when 'spark.sql.adaptive.enabled' is true.",
      "defaultValue": "0.2"
    },
    {
      "key": "spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled",
      "doc": "When true and 'spark.sql.adaptive.enabled' is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid data skew.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.optimizer.excludedRules",
      "doc": "Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded."
    },
    {
      "key": "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor",
      "doc": "A partition will be merged during splitting if its size is small than this factor multiply spark.sql.adaptive.advisoryPartitionSizeInBytes.",
      "defaultValue": "0.2"
    },
    {
      "key": "spark.sql.adaptive.shuffle.targetPostShuffleInputSize",
      "doc": "(Deprecated since Spark 3.0)",
      "defaultValue": "64MB",
      "deprecated": {
        "version": "3.0",
        "comment": "Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it."
      }
    },
    {
      "key": "spark.sql.adaptive.skewJoin.enabled",
      "doc": "When true and 'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.adaptive.skewJoin.skewedPartitionFactor",
      "doc": "A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'",
      "defaultValue": "5.0"
    },
    {
      "key": "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes",
      "doc": "A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Ideally this config should be set larger than 'spark.sql.adaptive.advisoryPartitionSizeInBytes'.",
      "defaultValue": "256MB"
    },
    {
      "key": "spark.sql.adaptive.streaming.stateless.enabled",
      "doc": "When true, enable adaptive query execution for stateless streaming query. To enable this config, `spark.sql.adaptive.enabled` needs to be also enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.addPartitionInBatch.size",
      "doc": "The number of partitions to be handled in one turn when use `AlterTableAddPartitionCommand` or `RepairTableCommand` to add partitions into table. The smaller batch size is, the less memory is required for the real handler, e.g. Hive Metastore.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.allowNamedFunctionArguments",
      "doc": "If true, Spark will turn on support for named parameters for all functions that has it implemented.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.alwaysInlineCommonExpr",
      "doc": "When true, always inline common expressions instead of using the WITH expression. This may lead to duplicated expressions and the config should only be enabled if you hit bugs caused by the WITH expression.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.allowSubqueryExpressionsInLambdasOrHigherOrderFunctions",
      "doc": "When set to false, the analyzer will throw an error if a subquery expression appears in a lambda function or higher-order function. When set to true, it restores the legacy behavior of allowing subquery eexpressions in lambda functions or higher-order functions.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.canonicalization.multiCommutativeOpMemoryOptThreshold",
      "doc": "The minimum number of operands in a commutative expression tree to invoke the MultiCommutativeOp memory optimization during canonicalization.",
      "defaultValue": "3"
    },
    {
      "key": "spark.sql.analyzer.dontDeduplicateExpressionIfExprIdInOutput",
      "doc": "DeduplicateRelations shouldn't remap expressions to new ExprIds if old ExprId still exists in output.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.failAmbiguousSelfJoin",
      "doc": "When true, fail the Dataset query if it contains ambiguous self-join.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.maxIterations",
      "doc": "The max number of iterations the analyzer runs.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.analyzer.preferColumnOverLcaInArrayIndex",
      "doc": "When true, prefer the column from the underlying relation over the lateral column alias reference with the same name (see SPARK-53734).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.scalarSubqueryAllowGroupByColumnEqualToConstant",
      "doc": "When set to true, allow scalar subqueries with group-by on a column that also  has an equality filter with a constant (SPARK-48557).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.dualRunSampleRate",
      "doc": "Represents the rate of queries that will be run in both fixed-point and single-pass mode (dual run). It should be taken into account that the sample rate is not a strict percentage (in tests we don't sample). It is determined whether query should be run in dual run mode by comparing a random value with the value of this flag.",
      "defaultValue": "0.001"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.dualRunWithLegacy",
      "doc": "When true, run both analyzers to check if single-pass Analyzer correctly produces the same analyzed plan as the fixed-point Analyzer for the existing set of features defined in the ResolverGuard",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.enabled",
      "doc": "When true, use the single-pass Resolver instead of the fixed-point Analyzer. This is an alternative Analyzer framework, which resolves the parsed logical plan in a single post-order traversal. It uses ExpressionResolver to resolve expressions and NameScope to control the visibility of names. In contrast to the current fixed-point framework, subsequent in-tree traversals are disallowed. Most of the fixed-point Analyzer code is reused in the form of specific node transformation functions (AliasResolution.resolve, FunctionResolution.resolveFunction, etc). This feature is currently under development.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.enabledTentatively",
      "doc": "When true, use the single-pass Resolver instead of the fixed-point Analyzer only if a SQL query or a DataFrame program is fully supported by the single-pass Analyzer. This is an alternative Analyzer framework, which resolves the parsed logical plan in a single post-order traversal. It uses ExpressionResolver to resolve expressions and NameScope to control the visibility of names. In contrast to the current fixed-point framework, subsequent in-tree traversals are disallowed. Most of the fixed-point Analyzer code is reused in the form of specific node transformation functions (AliasResolution.resolve, FunctionResolution.resolveFunction, etc).This feature is currently under development.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.exposeResolverGuardFailure",
      "doc": "When true, any failure thrown from ResolverGuard will be exposed as a query failure. Otherwise we just assume that the ResolverGuard returned false and the query is not supported by the single-pass Analyzer. This is important to make dual-runs unnoticeable in production.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.preventUsingAliasesFromNonDirectChildren",
      "doc": "When true, in Sort/Having/Filter expressions allow replacing of these expressions only with semantically equal aliased expressions from direct children. This is necessary in order to stay compatible with fixed-point, but the functionality and correctness remain the same. Because enabling this case would break some cases that are supported in single-pass but not in fixed-point, this flag should only be used to hide false positive logical plan mismatches during testing.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.relationBridging.enabled",
      "doc": "When set to true, the single-pass Resolver will reuse the relation metadata that was previously resolved in fixed-point run. This makes sense only in ANALYZER_DUAL_RUN_LEGACY_AND_SINGLE_PASS_RESOLVER mode. In that case HybridAnalyzer enables the AnalyzerBridgeState and passes it to the single-pass Analyzer after the fixed-point run is complete. Single-pass Resolver uses this AnalyzerBridgeState to construct a special RelationMetadataProvider implementation - BridgedRelationMetadataProvider. This component simply reuses cached relation metadata and avoids any blocking calls (catalog RPCs or table metadata reads).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.returnSinglePassResultInDualRun",
      "doc": "When true, return the result of the single-pass resolver as the result of the dual run analysis (which is used if the ANALYZER_DUAL_RUN_LEGACY_AND_SINGLE_PASS_RESOLVER flag value is true). Otherwise, return the result of the fixed-point analyzer.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.runExtendedResolutionChecks",
      "doc": "When true, run `extendedResolutionChecks` after the main analysis.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.runHeavyExtendedResolutionChecks",
      "doc": "When true, run heavy `extendedResolutionChecks` after the main analysis. Otherwise skip them. Heavy check either involves a network call changing external persistent storage, or changes a global state. For example, `ViewSyncSchemaToMetaStore` calls alter table.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.throwFromResolverGuard",
      "doc": "When set to true, ResolverGuard will throw a descriptive error on unsupported features.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.analyzer.singlePassResolver.validationEnabled",
      "doc": "When true, validate the Resolver output with ResolutionValidator. The ResolutionValidator validates the resolved logical plan tree in one pass and asserts the internal contracts. It uses the ExpressionResolutionValidator internally to validate resolved expression trees in the same manner.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.sqlFunctionResolution.applyConfOverrides",
      "doc": "When true, applies the conf overrides for certain feature flags during the resolution of user-defined sql table valued functions, consistent with view resolution.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.unionIsResolvedWhenDuplicatesPerChildResolved",
      "doc": "When true, union should only be resolved once there are no duplicate attributes in each branch.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.analyzer.uniqueNecessaryMetadataColumns",
      "doc": "When this conf is enabled, AddMetadataColumns rule should only add necessary metadata columns and only if those columns are not already present in the project list.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.ansi.doubleQuotedIdentifiers",
      "doc": "When true and 'spark.sql.ansi.enabled' is true, Spark SQL reads literals enclosed in double quoted (\") as identifiers. When false they are read as string literals.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.ansi.enabled",
      "doc": "When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results when the inputs to a SQL operator/function are invalid. For full details of this dialect, you can find them in the section \"ANSI Compliance\" of Spark's documentation. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL's style",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.ansi.enforceReservedKeywords",
      "doc": "When true and 'spark.sql.ansi.enabled' is true, the Spark SQL parser enforces the ANSI reserved keywords and forbids SQL queries that use reserved keywords as alias names and/or identifiers for table, view, function, etc.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.ansi.relationPrecedence",
      "doc": "When true and 'spark.sql.ansi.enabled' is true, JOIN takes precedence over comma when combining relation. For example, `t1, t2 JOIN t3` should result to `t1 X (t2 X t3)`. If the config is false, the result is `(t1 X t2) X t3`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.ansi.strictIndexOperator",
      "doc": "(Removed)",
      "defaultValue": "true",
      "removed": {
        "version": "3.4.0",
        "comment": "This was an internal configuration. It is not needed anymore since Spark SQL always returns null when getting a map value with a non-existing key. See SPARK-40066 for more details."
      }
    },
    {
      "key": "spark.sql.artifact.copyFromLocalToFs.allowDestLocal",
      "doc": "\nAllow `spark.copyFromLocalToFs` destination to be local file system\n path on spark driver node when\n`spark.sql.artifact.copyFromLocalToFs.allowDestLocal` is true.\nThis will allow user to overwrite arbitrary file on spark\ndriver node we should only enable it for testing purpose.\n"
    },
    {
      "key": "spark.sql.artifact.isolation.alwaysApplyClassloader",
      "doc": "When enabled, the classloader holding per-session artifacts will always be applied during SQL executions (useful for Spark Connect). When disabled, the classloader will be applied only when any artifact is added to the session.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.artifact.isolation.enabled",
      "doc": "When enabled for a Spark Session, artifacts (such as JARs, files, archives) added to this session are isolated from other sessions within the same Spark instance. When disabled for a session, artifacts added to this session are visible to other sessions that have this config disabled. This config can only be set during the creation of a Spark Session and will have no effect when changed in the middle of session usage.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.assumeAnsiFalseIfNotPersisted.enabled",
      "doc": "If enabled, assume ANSI mode is false if not persisted during view or UDF creation. Otherwise use the default value.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.autoBroadcastJoinThreshold",
      "doc": "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled.",
      "defaultValue": "10MB"
    },
    {
      "key": "spark.sql.avro.compression.codec",
      "doc": "Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2, xz and zstandard. Default codec is snappy.",
      "defaultValue": "snappy"
    },
    {
      "key": "spark.sql.avro.datetimeRebaseModeInRead",
      "doc": "When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Avro files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Avro files is unknown.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.avro.datetimeRebaseModeInWrite",
      "doc": "When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Avro files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.avro.deflate.level",
      "doc": "Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.",
      "defaultValue": "-1"
    },
    {
      "key": "spark.sql.avro.filterPushdown.enabled",
      "doc": "When true, enable filter pushdown to Avro datasource.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.avro.xz.level",
      "doc": "Compression level for the xz codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive The default value is 6.",
      "defaultValue": "6"
    },
    {
      "key": "spark.sql.avro.zstandard.bufferPool.enabled",
      "doc": "If true, enable buffer pool of ZSTD JNI library when writing of AVRO files",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.avro.zstandard.level",
      "doc": "Compression level for the zstandard codec used in writing of AVRO files. ",
      "defaultValue": "3"
    },
    {
      "key": "spark.sql.binaryOutputStyle",
      "doc": "The output style used display binary data. Valid values are 'UTF-8', 'BASIC', 'BASE64', 'HEX', and 'HEX_DISCRETE'."
    },
    {
      "key": "spark.sql.broadcastExchange.maxThreadThreshold",
      "doc": "The maximum degree of parallelism to fetch and broadcast the table. If we encounter memory issue like frequently full GC or OOM when broadcast table we can decrease this number in order to reduce memory usage. Notice the number should be carefully chosen since decreasing parallelism might cause longer waiting for other broadcasting. Also, increasing parallelism may cause memory problem.",
      "defaultValue": "128",
      "isStatic": true
    },
    {
      "key": "spark.sql.broadcastTimeout",
      "doc": "Timeout in seconds for the broadcast wait time in broadcast joins.",
      "defaultValue": "300"
    },
    {
      "key": "spark.sql.bucketing.coalesceBucketsInJoin.enabled",
      "doc": "When true, if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio",
      "doc": "The ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied. This configuration only has an effect when 'spark.sql.bucketing.coalesceBucketsInJoin.enabled' is set to true.",
      "defaultValue": "4"
    },
    {
      "key": "spark.sql.cache.serializer",
      "doc": "The name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.",
      "defaultValue": "org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer",
      "isStatic": true
    },
    {
      "key": "spark.sql.cartesianProductExec.buffer.in.memory.threshold",
      "doc": "Threshold for number of rows guaranteed to be held in memory by the cartesian product operator",
      "defaultValue": "4096"
    },
    {
      "key": "spark.sql.cartesianProductExec.buffer.spill.size.threshold",
      "doc": "Threshold for size of rows to be spilled by cartesian product operator",
      "fallback": "spark.shuffle.spill.maxSizeInBytesForSpillThreshold"
    },
    {
      "key": "spark.sql.cartesianProductExec.buffer.spill.threshold",
      "doc": "Threshold for number of rows to be spilled by cartesian product operator",
      "defaultValue": "2147483647"
    },
    {
      "key": "spark.sql.caseSensitive",
      "doc": "Whether the query analyzer should be case sensitive or not. Default to case insensitive. It is highly discouraged to turn on case sensitive mode.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.catalog.spark_catalog",
      "doc": "A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This catalog shares its identifier namespace with the spark_catalog and must be consistent with it; for example, if a table can be loaded by the spark_catalog, this catalog must also return the table metadata. To delegate operations to the spark_catalog, implementations can extend 'CatalogExtension'. The value should be either 'builtin' which represents the spark's builit-in V2SessionCatalog, or a fully qualified class name of the catalog implementation.",
      "defaultValue": "builtin"
    },
    {
      "key": "spark.sql.catalog.spark_catalog.defaultDatabase",
      "doc": "The default database for session catalog.",
      "defaultValue": "default",
      "isStatic": true
    },
    {
      "key": "spark.sql.catalogImplementation",
      "doc": "",
      "defaultValue": "in-memory",
      "isStatic": true
    },
    {
      "key": "spark.sql.cbo.enabled",
      "doc": "Enables CBO for estimation of plan statistics when set true.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.cbo.joinReorder.card.weight",
      "doc": "The weight of the ratio of cardinalities (number of rows) in the cost comparison function. The ratio of sizes in bytes has weight 1 - this value. The weighted geometric mean of these ratios is used to decide which of the candidate plans will be chosen by the CBO.",
      "defaultValue": "0.7"
    },
    {
      "key": "spark.sql.cbo.joinReorder.dp.star.filter",
      "doc": "Applies star-join filter heuristics to cost based join enumeration.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.cbo.joinReorder.dp.threshold",
      "doc": "The maximum number of joined nodes allowed in the dynamic programming algorithm.",
      "defaultValue": "12"
    },
    {
      "key": "spark.sql.cbo.joinReorder.enabled",
      "doc": "Enables join reorder in CBO.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.cbo.planStats.enabled",
      "doc": "When true, the logical plan will fetch row counts and column statistics from catalog.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.cbo.starJoinFTRatio",
      "doc": "Specifies the upper limit of the ratio between the largest fact tables for a star join to be considered. ",
      "defaultValue": "0.9"
    },
    {
      "key": "spark.sql.cbo.starSchemaDetection",
      "doc": "When true, it enables join reordering based on star schema detection. ",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.charAsVarchar",
      "doc": "When true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CHAR type columns/fields. Existing tables with CHAR type columns/fields are not affected by this config.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.chunkBase64String.enabled",
      "doc": "Whether to truncate string generated by the `Base64` function. When true, base64 strings generated by the base64 function are chunked into lines of at most 76 characters. When false, the base64 strings are not chunked.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.classic.shuffleDependency.fileCleanup.enabled",
      "doc": "When enabled, shuffle files will be cleaned up at the end of classic SQL executions. Note that this cleanup may cause stage retries and regenerate shuffle files if the same dataframe reference is executed again.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.cli.print.header",
      "doc": "When set to true, spark-sql CLI prints the names of the columns in query output.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.codegen.aggregate.fastHashMap.capacityBit",
      "doc": "Capacity for the max number of rows to be held in memory by the fast hash aggregate product operator. The bit is not for actual value, but the actual numBuckets is determined by loadFactor (e.g: default bit value 16 , the actual numBuckets is ((1 << 16) / 0.5).",
      "defaultValue": "16"
    },
    {
      "key": "spark.sql.codegen.aggregate.map.twolevel.enabled",
      "doc": "Enable two-level aggregate hash map. When enabled, records will first be inserted/looked-up at a 1st-level, small, fast map, and then fallback to a 2nd-level, larger, slower map when 1st level is full or keys cannot be found. When disabled, records go directly to the 2nd level.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.aggregate.map.twolevel.partialOnly",
      "doc": "Enable two-level aggregate hash map for partial aggregate only, because final aggregate might get more distinct keys compared to partial aggregate. Overhead of looking up 1st-level map might dominate when having a lot of distinct keys.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.aggregate.map.vectorized.enable",
      "doc": "Enable vectorized aggregate hash map. This is for testing/benchmarking only.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.codegen.aggregate.sortAggregate.enabled",
      "doc": "When true, enable code-gen for sort aggregate.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.aggregate.splitAggregateFunc.enabled",
      "doc": "When true, the code generator would split aggregate code into individual methods instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.broadcastCleanedSourceThreshold",
      "doc": "A threshold (in string length) to determine if we should make the generated code a broadcast variable in whole stage codegen. To disable this, set the threshold to < 0; otherwise if the size is above the threshold, it'll use broadcast variable. Note that maximum string length allowed in Java is Integer.MAX_VALUE, so anything above it would be meaningless. The default value is set to -1 (disabled by default).",
      "defaultValue": "-1"
    },
    {
      "key": "spark.sql.codegen.cache.maxEntries",
      "doc": "When nonzero, enable caching of generated classes for operators and expressions. All jobs share the cache that can use up to the specified number for generated classes.",
      "defaultValue": "100",
      "isStatic": true
    },
    {
      "key": "spark.sql.codegen.comments",
      "doc": "When true, put comment in the generated code. Since computing huge comments can be extremely expensive in certain cases, such as deeply-nested expressions which operate over inputs with wide schemas, default is false.",
      "defaultValue": "false",
      "isStatic": true
    },
    {
      "key": "spark.sql.codegen.factoryMode",
      "doc": "This config determines the fallback behavior of several codegen generators during tests. `FALLBACK` means trying codegen first and then falling back to interpreted if any compile error happens. Disabling fallback if `CODEGEN_ONLY`. `NO_CODEGEN` skips codegen and goes interpreted path always. Note that this configuration is only for the internal usage, and NOT supposed to be set by end users.",
      "defaultValue": "FALLBACK"
    },
    {
      "key": "spark.sql.codegen.fallback",
      "doc": "When true, (whole stage) codegen could be temporary disabled for the part of query that fail to compile generated code",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.hugeMethodLimit",
      "doc": "The maximum bytecode size of a single compiled Java function generated by whole-stage codegen. When the compiled function exceeds this threshold, the whole-stage codegen is deactivated for this subtree of the current query plan. The default value is 65535, which is the largest bytecode size possible for a valid Java method. When running on HotSpot, it may be preferable to set the value to 8000 to match HotSpot's implementation.",
      "defaultValue": "65535"
    },
    {
      "key": "spark.sql.codegen.join.buildSideOuterShuffledHashJoin.enabled",
      "doc": "When true, enable code-gen for an OUTER shuffled hash join where outer side is the build side.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.join.existenceSortMergeJoin.enabled",
      "doc": "When true, enable code-gen for Existence sort merge join.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.join.fullOuterShuffledHashJoin.enabled",
      "doc": "When true, enable code-gen for FULL OUTER shuffled hash join.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.join.fullOuterSortMergeJoin.enabled",
      "doc": "When true, enable code-gen for FULL OUTER sort merge join.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.logLevel",
      "doc": "Configures the log level for logging of codegen. The value can be ERROR, WARN, INFO, DEBUG, TRACE.",
      "defaultValue": "DEBUG"
    },
    {
      "key": "spark.sql.codegen.logging.maxLines",
      "doc": "The maximum number of codegen lines to log when errors occur. Use -1 for unlimited.",
      "defaultValue": "1000"
    },
    {
      "key": "spark.sql.codegen.maxFields",
      "doc": "The maximum number of fields (including nested fields) that will be supported before deactivating whole-stage codegen.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.codegen.methodSplitThreshold",
      "doc": "The threshold of source-code splitting in the codegen. When the number of characters in a single Java function (without comment) exceeds the threshold, the function will be automatically split to multiple smaller ones. We cannot know how many bytecode will be generated, so use the code length as metric. When running on HotSpot, a function's bytecode should not go beyond 8KB, otherwise it will not be JITted; it also should not be too small, otherwise there will be many function calls.",
      "defaultValue": "1024"
    },
    {
      "key": "spark.sql.codegen.splitConsumeFuncByOperator",
      "doc": "When true, whole stage codegen would put the logic of consuming rows of each physical operator into individual methods, instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.useIdInClassName",
      "doc": "When true, embed the (whole-stage) codegen stage ID into the class name of the generated class as a suffix",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.codegen.wholeStage",
      "doc": "When true, the whole stage (of multiple operators) will be compiled into single java method.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.columnNameOfCorruptRecord",
      "doc": "The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.",
      "defaultValue": "_corrupt_record"
    },
    {
      "key": "spark.sql.columnVector.offheap.enabled",
      "doc": "When true, use OffHeapColumnVector in ColumnarBatch.",
      "fallback": "spark.memory.offHeap.enabled"
    },
    {
      "key": "spark.sql.connect.shuffleDependency.fileCleanup.enabled",
      "doc": "When enabled, shuffle files will be cleaned up at the end of Spark Connect SQL executions.",
      "fallback": "spark.sql.shuffleDependency.fileCleanup.enabled"
    },
    {
      "key": "spark.sql.constraintPropagation.enabled",
      "doc": "When true, the query optimizer will infer and propagate data constraints in the query plan to optimize them. Constraint propagation can sometimes be computationally expensive for certain kinds of query plans (such as those with a large number of predicates and aliases) which might negatively impact overall runtime.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.crossJoin.enabled",
      "doc": "When false, we will throw an error if a query contains a cartesian product without explicit CROSS JOIN syntax.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.csv.filterPushdown.enabled",
      "doc": "When true, enable filter pushdown to CSV datasource.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.csv.parser.columnPruning.enabled",
      "doc": "If it is set to true, column names of the requested schema are passed to CSV parser. Other column values can be ignored during parsing even if they are malformed.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.csv.parser.inputBufferSize",
      "doc": "If it is set, it configures the buffer size of CSV input during parsing. It is the same as inputBufferSize option in CSV which has a higher priority. Note that this is a workaround for the parsing library's regression, and this configuration is internal and supposed to be removed in the near future."
    },
    {
      "key": "spark.sql.cteRecursionAnchorRowsLimitToConvertToLocalRelation",
      "doc": "Maximum number of rows that the anchor in a recursive CTE can return for it to beconverted to a localRelation.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.cteRecursionLevelLimit",
      "doc": "Maximum level of recursion that is allowed while executing a recursive CTE definition.If a query does not get exhausted before reaching this limit it fails. Use -1 for unlimited.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.cteRecursionRowLimit",
      "doc": "Maximum number of rows that can be returned when executing a recursive CTE definition.If a query does not get exhausted before reaching this limit it fails. Use -1 for unlimited.",
      "defaultValue": "1000000"
    },
    {
      "key": "spark.sql.cteRelationDefMaxRows.enabled",
      "doc": "When set to true, CTERelationDef.maxRows would output the correct value from the child plan. This is necessary for correct scalar subquery validation in the single-pass Analyzer.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.dataFrameQueryContext.enabled",
      "doc": "Enable the DataFrame query context. This feature is enabled by default, but has a non-trivial performance overhead because of the stack trace collection.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.dataSource.alwaysCreateV2Predicate",
      "doc": "When true, the v2 push-down framework always wraps the expression that returns boolean type with a v2 Predicate so that it can be pushed down.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.dataSource.skipAssertOnPredicatePushdown",
      "doc": "Enable skipping assert when expression in not translated to predicate.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.dataframeCache.logLevel",
      "doc": "Configures the log level of Dataframe cache operations, including adding and removing entries from Dataframe cache, hit and miss on cache application. This log should only be used for debugging purposes and not in the production environment, since it generates a large amount of logs.",
      "defaultValue": "TRACE"
    },
    {
      "key": "spark.sql.datetime.java8API.enabled",
      "doc": "If the configuration property is set to true, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp and java.sql.Date are used for the same purpose.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.debug",
      "doc": "Only used for internal debugging. Not all functions are supported when it is enabled.",
      "defaultValue": "false",
      "isStatic": true
    },
    {
      "key": "spark.sql.debug.maxToStringFields",
      "doc": "Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a  \"... N more fields\" placeholder.",
      "defaultValue": "25"
    },
    {
      "key": "spark.sql.decimalOperations.allowPrecisionLoss",
      "doc": "When true (default), establishing the result type of an arithmetic operation happens according to Hive behavior and SQL ANSI 2011 specification, i.e. rounding the decimal part of the result if an exact representation is not possible. Otherwise, NULL is returned in those cases, as previously.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.defaultCacheStorageLevel",
      "doc": "The default storage level of `dataset.cache()`, `catalog.cacheTable()` and sql query `CACHE TABLE t`.",
      "defaultValue": "MEMORY_AND_DISK"
    },
    {
      "key": "spark.sql.defaultCatalog",
      "doc": "Name of the default catalog. This will be the current catalog if users have not explicitly set the current catalog yet.",
      "defaultValue": "spark_catalog"
    },
    {
      "key": "spark.sql.defaultColumn.allowedProviders",
      "doc": "List of table providers wherein SQL commands are permitted to assign DEFAULT column values. Comma-separated list, whitespace ignored, case-insensitive. If an asterisk appears after any table provider in this list, any command may assign DEFAULT column except `ALTER TABLE ... ADD COLUMN`. Otherwise, if no asterisk appears, all commands are permitted. This is useful because in order for such `ALTER TABLE ... ADD COLUMN` commands to work, the target data source must include support for substituting in the provided values when the corresponding fields are not present in storage.",
      "defaultValue": "csv,json,orc,parquet"
    },
    {
      "key": "spark.sql.defaultColumn.enabled",
      "doc": "When true, allow CREATE TABLE, REPLACE TABLE, and ALTER COLUMN statements to set or update default values for specific columns. Following INSERT, MERGE, and UPDATE statements may then omit these values and their values will be injected automatically instead.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.defaultColumn.useNullsForMissingDefaultValues",
      "doc": "When true, and DEFAULT columns are enabled, allow INSERT INTO commands with user-specified lists of fewer columns than the target table to behave as if they had specified DEFAULT for all remaining columns instead, in order.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.defaultSizeInBytes",
      "doc": "The default table size used in query planning. By default, it is set to Long.MaxValue which is larger than `spark.sql.autoBroadcastJoinThreshold` to be more conservative. That is to say by default the optimizer will not choose to broadcast a table unless it knows for sure its size is small enough.",
      "defaultValue": "9223372036854775807b"
    },
    {
      "key": "spark.sql.defaultUrlStreamHandlerFactory.enabled",
      "doc": "When true, register Hadoop's FsUrlStreamHandlerFactory to support ADD JAR against HDFS locations. It should be disabled when a different stream protocol handler should be registered to support a particular protocol type, or if Hadoop's FsUrlStreamHandlerFactory conflicts with other protocol types such as `http` or `https`. See also SPARK-25694 and HADOOP-14598.",
      "defaultValue": "true",
      "isStatic": true
    },
    {
      "key": "spark.sql.enforceTypeCoercionBeforeUnionDeduplication.enabled",
      "doc": "When set to true, we enforce type coercion to run before deduplication of UNION children outputs. Otherwise, order is relative to rule ordering.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.error.messageFormat",
      "doc": "When PRETTY, the error message consists of textual representation of error class, message and query context. The MINIMAL and STANDARD formats are pretty JSON formats where STANDARD includes an additional JSON field `message`. This configuration property influences on error messages of Thrift Server and SQL CLI while running queries.",
      "defaultValue": "PRETTY"
    },
    {
      "key": "spark.sql.event.truncate.length",
      "doc": "Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.",
      "defaultValue": "2147483647",
      "isStatic": true
    },
    {
      "key": "spark.sql.exchange.reuse",
      "doc": "When true, the planner will try to find out duplicated exchanges and re-use them.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.arrow.compression.codec",
      "doc": "Compression codec used to compress Arrow IPC data when transferring data between JVM and Python processes (e.g., toPandas, toArrow). This can significantly reduce memory usage and network bandwidth when transferring large datasets. Supported codecs: 'none' (no compression), 'zstd' (Zstandard), 'lz4' (LZ4). Note that compression may add CPU overhead but can provide substantial memory savings especially for datasets with high compression ratios.",
      "defaultValue": "none"
    },
    {
      "key": "spark.sql.execution.arrow.compression.zstd.level",
      "doc": "Compression level for Zstandard (zstd) codec when compressing Arrow IPC data. This config is only used when spark.sql.execution.arrow.compression.codec is set to 'zstd'. Negative values provide ultra-fast compression with lower compression ratios. Positive values provide normal to maximum compression, with higher values giving better compression but slower speed. The default value 3 provides a good balance between compression speed and compression ratio.",
      "defaultValue": "3"
    },
    {
      "key": "spark.sql.execution.arrow.enabled",
      "doc": "(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)",
      "defaultValue": "false",
      "deprecated": {
        "version": "3.0",
        "comment": "Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it."
      }
    },
    {
      "key": "spark.sql.execution.arrow.fallback.enabled",
      "doc": "(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.fallback.enabled'.)",
      "defaultValue": "true",
      "deprecated": {
        "version": "3.0",
        "comment": "Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it."
      }
    },
    {
      "key": "spark.sql.execution.arrow.localRelationThreshold",
      "doc": "When converting Arrow batches to Spark DataFrame, local collections are used in the driver side if the byte size of Arrow batches is smaller than this threshold. Otherwise, the Arrow batches are sent and deserialized to Spark internal rows in the executors.",
      "defaultValue": "48MB"
    },
    {
      "key": "spark.sql.execution.arrow.maxBytesPerBatch",
      "doc": "When using Apache Arrow, limit the maximum bytes in each batch that can be written to a single ArrowRecordBatch in memory. Unlike 'spark.sql.execution.arrow.maxRecordsPerBatch', this configuration does not work for createDataFrame/toPandas with Arrow/pandas instances. See also spark.sql.execution.arrow.maxRecordsPerBatch. If both are set, each batch is created when any condition of both is met.",
      "defaultValue": "64MB"
    },
    {
      "key": "spark.sql.execution.arrow.maxBytesPerOutputBatch",
      "doc": "When using Apache Arrow, limit the maximum bytes that can be output in a single ArrowRecordBatch to the downstream operator. If set to zero or negative there is no limit. Note that the complete ArrowRecordBatch is actually created but the number of bytes is limited when sending it to the downstream operator. This is used to avoid large batches being sent to the downstream operator including the columnar-based operator implemented by third-party libraries. Spark will try to create batches with the size equal or less than this value. Normally it should not happen, but if in extreme case that even one record is still larger than this value, Spark will create a batch with one record.",
      "defaultValue": "-1b"
    },
    {
      "key": "spark.sql.execution.arrow.maxRecordsPerBatch",
      "doc": "When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit. See also spark.sql.execution.arrow.maxBytesPerBatch. If both are set, each batch is created when any condition of both is met.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.execution.arrow.maxRecordsPerOutputBatch",
      "doc": "When using Apache Arrow, limit the maximum number of records that can be output in a single ArrowRecordBatch to the downstream operator. If set to zero or negative there is no limit. Note that the complete ArrowRecordBatch is actually created but the number of records is limited when sending it to the downstream operator. This is used to avoid large batches being sent to the downstream operator including the columnar-based operator implemented by third-party libraries.",
      "defaultValue": "-1"
    },
    {
      "key": "spark.sql.execution.arrow.pyspark.enabled",
      "doc": "When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to: 1. pyspark.sql.DataFrame.toPandas. 2. pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame or a NumPy ndarray. The following data type is unsupported: ArrayType of TimestampType.",
      "fallback": "spark.sql.execution.arrow.enabled"
    },
    {
      "key": "spark.sql.execution.arrow.pyspark.fallback.enabled",
      "doc": "When true, optimizations enabled by 'spark.sql.execution.arrow.pyspark.enabled' will fallback automatically to non-optimized implementations if an error occurs.",
      "fallback": "spark.sql.execution.arrow.fallback.enabled"
    },
    {
      "key": "spark.sql.execution.arrow.pyspark.selfDestruct.enabled",
      "doc": "(Experimental) When true, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time. This optimization applies to: pyspark.sql.DataFrame.toPandas when 'spark.sql.execution.arrow.pyspark.enabled' is set.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.arrow.pyspark.validateSchema.enabled",
      "doc": "When true, validate the schema of Arrow batches returned by mapInArrow, mapInPandas and DataSource against the expected schema to ensure that they are compatible.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.arrow.sparkr.enabled",
      "doc": "When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to: 1. createDataFrame when its input is an R DataFrame 2. collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.arrow.transformWithStateInPySpark.maxStateRecordsPerBatch",
      "doc": "When using TransformWithState in PySpark (both Python Row and Pandas), limit the maximum number of state records that can be written to a single ArrowRecordBatch in memory.",
      "defaultValue": "10000",
      "alternatives": [
        "spark.sql.execution.arrow.transformWithStateInPandas.maxRecordsPerBatch"
      ]
    },
    {
      "key": "spark.sql.execution.arrow.useLargeVarTypes",
      "doc": "When using Apache Arrow, use large variable width vectors for string and binary types. Regular string and binary types have a 2GiB limit for a column in a single record batch. Large variable types remove this limitation at the cost of higher memory usage per value.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.broadcastHashJoin.outputPartitioningExpandLimit",
      "doc": "The maximum number of partitionings that a HashPartitioning can be expanded to. This configuration is applicable only for BroadcastHashJoin inner joins and can be set to '0' to disable this feature.",
      "defaultValue": "8"
    },
    {
      "key": "spark.sql.execution.datasources.hadoopLineRecordReader.enabled",
      "doc": "Enable the imported Hadoop's LineRecordReader. This was imported and renamed to HadoopLineRecordReader to add support for compression option and other future codecs like ZSTD, etc. Setting the conf to false will use the LineRecordReader class from the hadoop jar instead of the imported one.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.fastFailOnFileFormatOutput",
      "doc": "Whether to fast fail task execution when writing output to FileFormat datasource. If this is enabled, in `FileFormatWriter` we will catch `FileAlreadyExistsException` and fast fail output task without further task retry. Only enabling this if you know the `FileAlreadyExistsException` of the output task is unrecoverable, i.e., further task attempts won't be able to success. If the `FileAlreadyExistsException` might be recoverable, you should keep this as disabled and let Spark to retry output tasks. This is disabled by default.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.interruptOnCancel",
      "doc": "When true, all running tasks will be interrupted if one cancels a query.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.pandas.convertToArrowArraySafely",
      "doc": "When true, Arrow will perform safe type conversion when converting Pandas.Series to Arrow array during serialization. Arrow will raise errors when detecting unsafe type conversion like overflow. When false, disabling Arrow's type check and do type conversions anyway. This config only works for Arrow 0.11.0+.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.pandas.inferPandasDictAsMap",
      "doc": "When true, spark.createDataFrame will infer dict from Pandas DataFrame as a MapType. When false, spark.createDataFrame infers dict from Pandas DataFrame as a StructType which is default inferring from PyArrow.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.pandas.respectSessionTimeZone",
      "doc": "(Removed)",
      "defaultValue": "true",
      "removed": {
        "version": "3.0.0",
        "comment": "The non-default behavior is considered as a bug, see SPARK-22395. The config was deprecated since Spark 2.3."
      }
    },
    {
      "key": "spark.sql.execution.pandas.structHandlingMode",
      "doc": "The conversion mode of struct type when creating pandas DataFrame. When \"legacy\", 1. when Arrow optimization is disabled, convert to Row object, 2. when Arrow optimization is enabled, convert to dict or raise an Exception if there are duplicated nested field names. When \"row\", convert to Row object regardless of Arrow optimization. When \"dict\", convert to dict and use suffixed key names, e.g., a_0, a_1, if there are duplicated nested field names, regardless of Arrow optimization.",
      "defaultValue": "legacy"
    },
    {
      "key": "spark.sql.execution.pandas.udf.buffer.size",
      "doc": "Same as `spark.buffer.size` but only applies to Pandas UDF executions. If it is not set, the fallback is `spark.buffer.size`. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.",
      "fallback": "spark.buffer.size"
    },
    {
      "key": "spark.sql.execution.pyspark.binaryAsBytes",
      "doc": "When true, BinaryType is consistently mapped to bytes in PySpark. When false, restores the PySpark behavior before 4.1.0. Before 4.1.0, BinaryType is mapped to bytearray for regular UDF and UDTF without Arrow optimization, DataFrame APIs (both Spark Classic and Spark Connect), and data sources; BinaryType is mapped to bytes for Arrow-optimized UDF and UDTF with legacy pandas conversion.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.pyspark.python",
      "doc": "Python binary executable to use for PySpark in executors when running Python UDF, pandas UDF and pandas function APIs. If not set, it falls back to 'spark.pyspark.python' by default."
    },
    {
      "key": "spark.sql.execution.pyspark.udf.daemonKillWorkerOnFlushFailure",
      "doc": "Same as spark.python.daemon.killWorkerOnFlushFailure for Python execution with DataFrame and SQL. It can change during runtime.",
      "fallback": "spark.python.daemon.killWorkerOnFlushFailure"
    },
    {
      "key": "spark.sql.execution.pyspark.udf.faulthandler.enabled",
      "doc": "Same as spark.python.worker.faulthandler.enabled for Python execution with DataFrame and SQL. It can change during runtime.",
      "fallback": "spark.python.worker.faulthandler.enabled"
    },
    {
      "key": "spark.sql.execution.pyspark.udf.hideTraceback.enabled",
      "doc": "When true, only show the message of the exception from Python UDFs, hiding the stack trace. If this is enabled, simplifiedTraceback has no effect.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.pyspark.udf.idleTimeoutSeconds",
      "doc": "Same as spark.python.worker.idleTimeoutSeconds for Python execution with DataFrame and SQL. It can change during runtime.",
      "fallback": "spark.python.worker.idleTimeoutSeconds"
    },
    {
      "key": "spark.sql.execution.pyspark.udf.killOnIdleTimeout",
      "doc": "Same as spark.python.worker.killOnIdleTimeout for Python execution with DataFrame and SQL. It can change during runtime.",
      "fallback": "spark.python.worker.killOnIdleTimeout"
    },
    {
      "key": "spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled",
      "doc": "When true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs. Note that this works only with CPython 3.7+.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.pyspark.udf.tracebackDumpIntervalSeconds",
      "doc": "Same as spark.python.worker.tracebackDumpIntervalSeconds for Python execution with DataFrame and SQL. It can change during runtime.",
      "fallback": "spark.python.worker.tracebackDumpIntervalSeconds"
    },
    {
      "key": "spark.sql.execution.python.udf.buffer.size",
      "doc": "Same as `spark.buffer.size` but only applies to Python UDF executions. If it is not set, the fallback is `spark.buffer.size`.",
      "fallback": "spark.buffer.size"
    },
    {
      "key": "spark.sql.execution.python.udf.maxRecordsPerBatch",
      "doc": "When using Python UDFs, limit the maximum number of records that can be batched for serialization/deserialization.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.execution.pythonUDF.arrow.concurrency.level",
      "doc": "The level of concurrency to execute Arrow-optimized Python UDF. This can be useful if Python UDFs use I/O intensively."
    },
    {
      "key": "spark.sql.execution.pythonUDF.arrow.enabled",
      "doc": "Enable Arrow optimization in regular Python UDFs. This optimization can only be enabled when the given function takes at least one argument.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.pythonUDF.arrow.legacy.fallbackOnUDT",
      "doc": "When true, Arrow-optimized Python UDF will fallback to the regular UDF when its input or output is UDT.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.pythonUDF.pandas.intToDecimalCoercionEnabled",
      "doc": "When true, convert int to Decimal python objects before converting Pandas.Series to Arrow array during serialization.Disabled by default, impacts performance.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.pythonUDTF.arrow.enabled",
      "doc": "Enable Arrow optimization for Python UDTFs.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.rangeExchange.sampleSizePerPartition",
      "doc": "Number of points to sample per partition in order to determine the range boundaries for range partitioning, typically used in global sorting (without limit).",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.execution.removeRedundantProjects",
      "doc": "Whether to remove redundant project exec node based on children's output and ordering requirement.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.removeRedundantSorts",
      "doc": "Whether to remove redundant physical sort node",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.replaceHashWithSortAgg",
      "doc": "Whether to replace hash aggregate node with sort aggregate based on children's ordering",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.execution.reuseSubquery",
      "doc": "When true, the planner will try to find out duplicated subqueries and re-use them.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.sortBeforeRepartition",
      "doc": "When perform a repartition following a shuffle, the output row ordering would be nondeterministic. If some downstream stages fail and some tasks of the repartition stage retry, these tasks may generate different data, and that can lead to correctness issues. Turn on this config to insert a local sort before actually doing repartition to generate consistent repartition results. The performance of repartition() may go down since we insert extra local sort before it.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.topKSortFallbackThreshold",
      "doc": "In SQL queries with a SORT followed by a LIMIT like 'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort in memory, otherwise do a global sort which spills to disk if necessary.",
      "defaultValue": "2147483632"
    },
    {
      "key": "spark.sql.execution.useObjectHashAggregateExec",
      "doc": "Decides if we use ObjectHashAggregateExec",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.execution.usePartitionEvaluator",
      "doc": "When true, use PartitionEvaluator to execute SQL operators.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.expressionTreeChangeLog.level",
      "doc": "Configures the log level for logging the change from the unresolved expression tree to the resolved expression tree in the single-pass bottom-up Resolver. The value can be ERROR, WARN, INFO, DEBUG, TRACE.",
      "defaultValue": "TRACE"
    },
    {
      "key": "spark.sql.extendedExplainProviders",
      "doc": "A comma-separated list of classes that implement the org.apache.spark.sql.ExtendedExplainGenerator trait. If provided, Spark will print extended plan information from the providers in explain plan and in the UI"
    },
    {
      "key": "spark.sql.extensions",
      "doc": "A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.",
      "isStatic": true
    },
    {
      "key": "spark.sql.extensions.test.loadFromCp",
      "doc": "Flag that determines if we should load extensions from the classpath using the SparkSessionExtensionsProvider mechanism. This is a test only flag.",
      "defaultValue": "true",
      "isStatic": true
    },
    {
      "key": "spark.sql.files.ignoreCorruptFiles",
      "doc": "Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.files.ignoreInvalidPartitionPaths",
      "doc": "Whether to ignore invalid partition paths that do not match <column>=<value>. When the option is enabled, table with two partition directories 'table/invalid' and 'table/col=1' will only load the latter directory and ignore the invalid partition",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.files.ignoreMissingFiles",
      "doc": "Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.files.maxPartitionBytes",
      "doc": "The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.",
      "defaultValue": "128MB"
    },
    {
      "key": "spark.sql.files.maxPartitionNum",
      "doc": "The suggested (not guaranteed) maximum number of split file partitions. If it is set, Spark will rescale each partition to make the number of partitions is close to this value if the initial number of partitions exceeds this value. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."
    },
    {
      "key": "spark.sql.files.maxRecordsPerFile",
      "doc": "Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.",
      "defaultValue": "0"
    },
    {
      "key": "spark.sql.files.minPartitionNum",
      "doc": "The suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is `spark.sql.leafNodeDefaultParallelism`. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."
    },
    {
      "key": "spark.sql.files.openCostInBytes",
      "doc": "The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It's better to over estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first). This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.",
      "defaultValue": "4MB"
    },
    {
      "key": "spark.sql.files.supportSecondOffsetFormat",
      "doc": "When set to true, datetime formatter used for csv, json and xml will support zone offsets that have seconds in it. e.g. LA timezone offset prior to 1883 was -07:52:58. When this flag is not set we lose seconds information.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.filesourceTableRelationCacheSize",
      "doc": "The maximum size of the cache that maps qualified table names to table relation plans.",
      "defaultValue": "1000",
      "isStatic": true
    },
    {
      "key": "spark.sql.fromJsonForceNullableSchema",
      "doc": "(Removed)",
      "defaultValue": "true",
      "removed": {
        "version": "3.0.0",
        "comment": "It was removed to prevent errors like SPARK-23173 for non-default value."
      }
    },
    {
      "key": "spark.sql.function.concatBinaryAsString",
      "doc": "When this option is set to false and all inputs are binary, `functions.concat` returns an output as binary. Otherwise, it returns as a string.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.function.eltOutputAsString",
      "doc": "When this option is set to false and all inputs are binary, `elt` returns an output as binary. Otherwise, it returns as a string.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.geospatial.enabled",
      "doc": "When true, enables geospatial types (GEOGRAPHY/GEOMETRY) and ST functions.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.globalTempDatabase",
      "doc": "",
      "defaultValue": "global_temp",
      "isStatic": true
    },
    {
      "key": "spark.sql.groupByAliases",
      "doc": "When true, aliases in a select list can be used in group by clauses. When false, an analysis exception is thrown in the case.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.groupByOrdinal",
      "doc": "When true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.hive.advancedPartitionPredicatePushdown.enabled",
      "doc": "When true, advanced partition predicate pushdown into Hive metastore is enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.hive.caseSensitiveInferenceMode",
      "doc": "Sets the action to take when a case-sensitive schema cannot be read from a Hive Serde table's properties when reading the table with Spark native data sources. Valid options include INFER_AND_SAVE (infer the case-sensitive schema from the underlying data files and write it back to the table properties), INFER_ONLY (infer the schema but don't attempt to write it to the table properties) and NEVER_INFER (the default mode-- fallback to using the case-insensitive metastore schema instead of inferring).",
      "defaultValue": "NEVER_INFER"
    },
    {
      "key": "spark.sql.hive.convertCTAS",
      "doc": "When true, a table created by a Hive CTAS statement (no USING clause) without specifying any storage property will be converted to a data source table, using the data source set by spark.sql.sources.default.",
      "defaultValue": "false",
      "deprecated": {
        "version": "3.1",
        "comment": "Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
      }
    },
    {
      "key": "spark.sql.hive.dropPartitionByName.enabled",
      "doc": "When true, Spark will get partition name rather than partition object to drop partition, which can improve the performance of drop partition.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.hive.filesourcePartitionFileCacheSize",
      "doc": "When nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.",
      "defaultValue": "262144000"
    },
    {
      "key": "spark.sql.hive.gatherFastStats",
      "doc": "When true, fast stats (number of files and total size of all files) will be gathered in parallel while repairing table partitions to avoid the sequential listing in Hive metastore.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.hive.manageFilesourcePartitions",
      "doc": "When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is set to true.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.hive.metastorePartitionPruning",
      "doc": "When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.hive.metastorePartitionPruningFallbackOnException",
      "doc": "Whether to fallback to get all partitions from Hive metastore and perform partition pruning on Spark client side, when encountering MetaException from the metastore. Note that Spark query performance may degrade if this is enabled and there are many partitions to be listed. If this is disabled, Spark will fail the query instead.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.hive.metastorePartitionPruningFastFallback",
      "doc": "When this config is enabled, if the predicates are not supported by Hive or Spark does fallback due to encountering MetaException from the metastore, Spark will instead prune partitions by getting the partition names first and then evaluating the filter expressions on the client side. Note that the predicates with TimeZoneAwareExpression is not supported.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.hive.metastorePartitionPruningInSetThreshold",
      "doc": "The threshold of set size for InSet predicate when pruning partitions through Hive Metastore. When the set size exceeds the threshold, we rewrite the InSet predicate to be greater than or equal to the minimum value in set and less than or equal to the maximum value in set. Larger values may cause Hive Metastore stack overflow. But for InSet inside Not with values exceeding the threshold, we won't push it to Hive Metastore.",
      "defaultValue": "1000"
    },
    {
      "key": "spark.sql.hive.tablePropertyLengthThreshold",
      "doc": "The maximum length allowed in a single cell when storing Spark-specific information in Hive's metastore as table properties. Currently it covers 2 things: the schema's JSON string, the histogram of column statistics."
    },
    {
      "key": "spark.sql.hive.thriftServer.singleSession",
      "doc": "When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.",
      "defaultValue": "false",
      "isStatic": true
    },
    {
      "key": "spark.sql.hive.verifyPartitionPath",
      "doc": "(Removed)",
      "defaultValue": "false",
      "removed": {
        "version": "4.0.0",
        "comment": "This config was replaced by 'spark.sql.files.ignoreMissingFiles'."
      }
    },
    {
      "key": "spark.sql.icu.caseMappings.enabled",
      "doc": "When enabled we use the ICU library (instead of the JVM) to implement case mappings for strings under UTF8_BINARY collation.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.inMemoryColumnarStorage.batchSize",
      "doc": "Controls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.inMemoryColumnarStorage.compressed",
      "doc": "When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.inMemoryColumnarStorage.enableVectorizedReader",
      "doc": "Enables vectorized reader for columnar caching.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio",
      "doc": "When spark.sql.inMemoryColumnarStorage.hugeVectorThreshold <= 0 or the required memory is smaller than spark.sql.inMemoryColumnarStorage.hugeVectorThreshold, spark reserves required memory * 2 memory; otherwise, spark reserves required memory * this ratio memory, and will release this column vector memory before reading the next batch rows.",
      "defaultValue": "1.2"
    },
    {
      "key": "spark.sql.inMemoryColumnarStorage.hugeVectorThreshold",
      "doc": "When the required memory is larger than this, spark reserves required memory * spark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio memory next time and release this column vector memory before reading the next batch rows. -1 means disabling the optimization.",
      "defaultValue": "-1b"
    },
    {
      "key": "spark.sql.inMemoryColumnarStorage.partitionPruning",
      "doc": "When true, enable partition pruning for in-memory columnar tables.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.inMemoryTableScanStatistics.enable",
      "doc": "When true, enable in-memory table scan accumulators.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.join.preferSortMergeJoin",
      "doc": "When true, prefer sort merge join over shuffled hash join. Sort merge join consumes less memory than shuffled hash join and it works efficiently when both join tables are large. On the other hand, shuffled hash join can improve performance (e.g., of full outer joins) when one of join tables is much smaller.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.json.enableExactStringParsing",
      "doc": "When set to true, string columns extracted from JSON objects will be extracted exactly as they appear in the input string, with no changes",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.json.enablePartialResults",
      "doc": "When set to true, enables partial results for structs, maps, and arrays in JSON when one or more fields do not match the schema",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.json.filterPushdown.enabled",
      "doc": "When true, enable filter pushdown to JSON datasource.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.json.useUnsafeRow",
      "doc": "When set to true, use UnsafeRow to represent struct result in the JSON parser. It can be overwritten by the JSON option `useUnsafeRow`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.jsonGenerator.ignoreNullFields",
      "doc": "Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.jsonGenerator.writeNullIfWithDefaultValue",
      "doc": "When true, when writing NULL values to columns of JSON tables with explicit DEFAULT values using INSERT, UPDATE, or MERGE commands, never skip writing the NULL values to storage, overriding spark.sql.jsonGenerator.ignoreNullFields or the ignoreNullFields option. This can be useful to enforce that inserted NULL values are present in storage to differentiate from missing data.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.lateralColumnAlias.enableImplicitResolution",
      "doc": "Enable resolving implicit lateral column alias defined in the same SELECT list. For example, with this conf turned on, for query `SELECT 1 AS a, a + 1` the `a` in `a + 1` can be resolved as the previously defined `1 AS a`. But note that table column has higher resolution priority than the lateral column alias.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.lazySetOperatorOutput.enabled",
      "doc": "When set to true, Except/Intersect/Union operator's output will be a lazy val. It is a performance optimization for querires with a large number of stacked set operators. This is because of rules like WidenSetOperationTypes that traverse the logical plan tree and call output on each Except/Intersect/Union node. Such traversal has quadratic complexity: O(number_of_nodes * (1 + 2 + 3  + ... + number_of_nodes)).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.leafNodeDefaultParallelism",
      "doc": "The default parallelism of Spark SQL leaf nodes that produce data, such as the file scan node, the local data scan node, the range node, etc. The default value of this config is 'SparkContext#defaultParallelism'."
    },
    {
      "key": "spark.sql.legacy.addSingleFileInAddFile",
      "doc": "When true, only a single file can be added using ADD FILE. If false, then users can add directory by passing directory path to ADD FILE.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowAutoGeneratedAliasForView",
      "doc": "When true, it's allowed to use a input query without explicit alias when creating a permanent view.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation",
      "doc": "(Removed)",
      "defaultValue": "false",
      "removed": {
        "version": "3.0.0",
        "comment": "It was removed to prevent loss of user data for non-default value."
      }
    },
    {
      "key": "spark.sql.legacy.allowEmptySchemaWrite",
      "doc": "When this option is set to true, validation of empty or empty nested schemas that occurs when writing into a FileFormat based data source does not happen.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowHashOnMapType",
      "doc": "When set to true, hash expressions can be applied on elements of MapType. Otherwise, an analysis exception will be thrown.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowNegativeScaleOfDecimal",
      "doc": "When set to true, negative scale of Decimal type is allowed. For example, the type of number 1E10BD under legacy mode is DecimalType(2, -9), but is Decimal(11, 0) in non legacy mode.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowNonEmptyLocationInCTAS",
      "doc": "When false, CTAS with LOCATION throws an analysis exception if the location is not empty.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowNullComparisonResultInArraySort",
      "doc": "When set to false, `array_sort` function throws an error if the comparator function returns null. If set to true, it restores the legacy behavior that handles null as zero (equal).",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowParameterlessCount",
      "doc": "When true, the SQL function 'count' is allowed to take no parameters.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowSessionVariableInPersistedView",
      "doc": "When set to true, variables can be found under identifiers in a view query. Throw otherwise.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowStarWithSingleTableIdentifierInCount",
      "doc": "When true, the SQL function 'count' is allowed to take single 'tblName.*' as parameter",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowTempViewCreationWithMultipleNameparts",
      "doc": "When true, temp view creation Dataset APIs will allow the view creation even if the view name is multiple name parts. The extra name parts will be dropped during the view creation",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowUntypedScalaUDF",
      "doc": "When set to true, user is allowed to use org.apache.spark.sql.functions. udf(f: AnyRef, dataType: DataType). Otherwise, an exception will be thrown at runtime.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.allowZeroIndexInFormatString",
      "doc": "When false, the `strfmt` in `format_string(strfmt, obj, ...)` and `printf(strfmt, obj, ...)` will no longer support to use \"0$\" to specify the first argument, the first argument should always reference by \"1$\" when use argument index to indicating the position of the argument in the argument list. This config will be removed in the future releases.",
      "defaultValue": "false",
      "deprecated": {
        "version": "4.0",
        "comment": "Increase indexes by 1 in `strfmt` of the `format_string` function. Refer to the first argument by \"1$\"."
      }
    },
    {
      "key": "spark.sql.legacy.avro.allowIncompatibleSchema",
      "doc": "When set to false, if types in Avro are encoded in the same format, but the type in the Avro schema explicitly says that the data types are different, reject reading the data type in the format to avoid returning incorrect results. When set to true, it restores the legacy behavior of allow reading the data in the format, which may return incorrect results.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.avro.datetimeRebaseModeInRead",
      "doc": "(Removed)",
      "defaultValue": "CORRECTED",
      "removed": {
        "version": "4.0.0",
        "comment": "Use 'spark.sql.avro.datetimeRebaseModeInRead' instead."
      }
    },
    {
      "key": "spark.sql.legacy.avro.datetimeRebaseModeInWrite",
      "doc": "(Removed)",
      "defaultValue": "CORRECTED",
      "removed": {
        "version": "4.0.0",
        "comment": "Use 'spark.sql.avro.datetimeRebaseModeInWrite' instead."
      }
    },
    {
      "key": "spark.sql.legacy.bangEqualsNot",
      "doc": "When set to true, '!' is a lexical equivalent for 'NOT'. That is '!' can be used outside of the documented prefix usage in a logical expression. Examples are: `expr ! IN (1, 2)` and `expr ! BETWEEN 1 AND 2`, but also `IF ! EXISTS`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.blockCreateTempTableUsingProvider",
      "doc": "If enabled, we fail legacy CREATE TEMPORARY TABLE ... USING provider during parsing.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.bucketedTableScan.outputOrdering",
      "doc": "When true, the bucketed table scan will list files during planning to figure out the output ordering, which is expensive and may make the planning quite slow.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.castComplexTypesToString.enabled",
      "doc": "When true, maps and structs are wrapped by [] in casting to strings, and NULL elements of structs/maps/arrays will be omitted while converting to strings. Otherwise, if this is false, which is the default, maps and structs are wrapped by {}, and NULL elements will be converted to \"null\".",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.charVarcharAsString",
      "doc": "When true, Spark treats CHAR/VARCHAR type the same as STRING type, which is the behavior of Spark 3.0 and earlier. This means no length check for CHAR/VARCHAR type and no padding for CHAR type when writing data to the table.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.codingErrorAction",
      "doc": "When set to true, encode/decode functions replace unmappable characters with mojibake instead of reporting coding errors.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.compareDateTimestampInTimestamp",
      "doc": "(Removed)",
      "defaultValue": "true",
      "removed": {
        "version": "3.0.0",
        "comment": "It was removed to prevent errors like SPARK-23549 for non-default value."
      }
    },
    {
      "key": "spark.sql.legacy.consecutiveStringLiterals.enabled",
      "doc": "When true, consecutive string literals separated by double quotes (e.g. 'a''b') will be parsed as concatenated strings. This preserves pre-Spark 4.0 behavior where'a''b' would be parsed as 'ab' instead of 'a'b'.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.createEmptyCollectionUsingStringType",
      "doc": "When set to true, Spark returns an empty collection with `StringType` as element type if the `array`/`map` function is called without any parameters. Otherwise, Spark returns an empty collection with `NullType` as element type.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.createHiveTableByDefault",
      "doc": "When set to true, CREATE TABLE syntax without USING or STORED AS will use Hive instead of the value of spark.sql.sources.default as the table provider.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.csv.enableDateTimeParsingFallback",
      "doc": "When true, enable legacy date/time parsing fallback in CSV"
    },
    {
      "key": "spark.sql.legacy.ctePrecedencePolicy",
      "doc": "When LEGACY, outer CTE definitions takes precedence over inner definitions. If set to EXCEPTION, AnalysisException is thrown while name conflict is detected in nested CTE. The default is CORRECTED, inner CTE definitions take precedence. This config will be removed in future versions and CORRECTED will be the only behavior.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.legacy.dataFrameWriterV2IgnorePathOption",
      "doc": "When set to true, DataFrameWriterV2 ignores the 'path' option and always write data to the default table location.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue",
      "doc": "When set to true, the key attribute resulted from running `Dataset.groupByKey` for non-struct key type, will be named as `value`, following the behavior of Spark version 2.4 and earlier.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.db2.booleanMapping.enabled",
      "doc": "When true, BooleanType maps to CHAR(1) in DB2; otherwise, BOOLEAN",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.db2.numericMapping.enabled",
      "doc": "When true, SMALLINT maps to IntegerType in DB2; otherwise, ShortType",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.decimal.retainFractionDigitsOnTruncate",
      "doc": "When set to true, we will try to retain the fraction digits first rather than integral digits as prior Spark 4.0, when getting a least common type between decimal types, and the result decimal precision exceeds the max precision.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.disableMapKeyNormalization",
      "doc": "Disables key normalization when creating a map with `ArrayBasedMapBuilder`. When set to `true` it will prevent key normalization when building a map, which will allow for values such as `-0.0` and `0.0` to be present as distinct keys.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.doLooseUpcast",
      "doc": "When true, the upcast will be loose and allows string to atomic types.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.duplicateBetweenInput",
      "doc": "When true, we use legacy between implementation. This is a flag that fixes a problem introduced by a between optimization, see ticket SPARK-49063.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.earlyEvalCurrentTime",
      "doc": "When set to true, evaluation and constant folding will happen for now() and current_timestamp() expressions before finish analysis phase. This flag will allow a bit more liberal syntax but it will sacrifice correctness - Results of now() and current_timestamp() can be different for different operations in a single query.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.emptyCurrentDBInCli",
      "doc": "When false, spark-sql CLI prints the current database in prompt.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.execution.pandas.groupedMap.assignColumnsByName",
      "doc": "When true, columns will be looked up by name if labeled with a string and fallback to use position if not. When false, a grouped map Pandas UDF will assign columns from the returned Pandas DataFrame based on position, regardless of column label type. This configuration will be deprecated in future releases.",
      "defaultValue": "true",
      "deprecated": {
        "version": "2.4",
        "comment": "The config allows to switch to the behaviour before Spark 2.4 and will be removed in the future releases."
      }
    },
    {
      "key": "spark.sql.legacy.execution.pythonUDF.pandas.conversion.enabled",
      "doc": "When true and spark.sql.execution.pythonUDF.arrow.enabled is enabled, matches the default Arrow Python UDF behavior before 4.1.0. With this behavior, extrapandas conversion happens during (de)serialization between JVM and Python workers. This matters especially when the produced output has a schema different from specified schema, resulting in a different type coercion.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.execution.pythonUDTF.pandas.conversion.enabled",
      "doc": "When true and spark.sql.execution.pythonUDTF.arrow.enabled is enabled, extra pandas conversion happens during (de)serialization between JVM and Python workers. This matters especially when the produced output has a schema different from specified schema, resulting in a different type coercion.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.exponentLiteralAsDecimal.enabled",
      "doc": "When set to true, a literal with an exponent (e.g. 1E-30) would be parsed as Decimal rather than Double.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.extraOptionsBehavior.enabled",
      "doc": "When true, the extra options will be ignored for DataFrameReader.table(). If set it to false, which is the default, Spark will check if the extra options have the same key, but the value is different with the table serde properties. If the check passes, the extra options will be merged with the serde properties as the scan options. Otherwise, an exception will be thrown.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.followThreeValuedLogicInArrayExists",
      "doc": "When true, the ArrayExists will follow the three-valued boolean logic.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.legacy.fromDayTimeString.enabled",
      "doc": "When true, the `from` bound is not taken into account in conversion of a day-time string to an interval, and the `to` bound is used to skip all interval units out of the specified range. If it is set to `false`, `ParseException` is thrown if the input does not match to the pattern defined by `from` and `to`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.groupingIdWithAppendedUserGroupBy",
      "doc": "When true, grouping_id() returns values based on grouping set columns plus user-given group-by expressions order like Spark 3.2.0, 3.2.1, 3.2.2, and 3.3.0.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.histogramNumericPropagateInputType",
      "doc": "The histogram_numeric function computes a histogram on numeric 'expr' using nb bins. The return value is an array of (x,y) pairs representing the centers of the histogram's bins. If this config is set to true, the output type of the 'x' field in the return value is propagated from the input value consumed in the aggregate function. Otherwise, 'x' always has double type.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.legacy.identifierClause",
      "doc": "When set to false, IDENTIFIER('literal') is resolved to an identifier at parse time anywhere identifiers can occur. When set to true, only the legacy  IDENTIFIER(constantExpr) clause is allowed, which evaluates the expression at analysis  and is limited to a narrow subset of scenarios.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.inSubqueryNullability",
      "doc": "When set to false, IN subquery nullability is correctly calculated based on both the left and right sides of the IN. When set to true, restores the legacy behavior that does not check the right side's nullability.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.inlineCTEInCommands",
      "doc": "If true, always inline the CTE relations for the queries in commands. This is the legacy behavior which may produce incorrect results because Spark may evaluate a CTE relation more than once, even if it's nondeterministic.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.integerGroupingId",
      "doc": "When true, grouping_id() returns int values instead of long values.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.interval.enabled",
      "doc": "When set to true, Spark SQL uses the mixed legacy interval type `CalendarIntervalType` instead of the ANSI compliant interval types `YearMonthIntervalType` and `DayTimeIntervalType`. For instance, the date subtraction expression returns `CalendarIntervalType` when the SQL config is set to `true` otherwise an ANSI interval.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.javaCharsets",
      "doc": "When set to true, the functions like `encode()` can use charsets from JDK while encoding or decoding string values. If it is false, such functions support only one of the charsets: 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16', 'UTF-32'.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.json.allowEmptyString.enabled",
      "doc": "When set to true, the parser of JSON data source treats empty strings as null for some data types such as `IntegerType`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.json.enableDateTimeParsingFallback",
      "doc": "When true, enable legacy date/time parsing fallback in JSON"
    },
    {
      "key": "spark.sql.legacy.keepCommandOutputSchema",
      "doc": "When true, Spark will keep the output schema of commands such as SHOW DATABASES unchanged.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.keepPartitionSpecAsStringLiteral",
      "doc": "If it is set to true, `PARTITION(col=05)` is parsed as a string literal of its text representation, e.g., string '05', when the partition column is string type. Otherwise, it is always parsed as a numeric literal in the partition spec.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.literal.pickMinimumPrecision",
      "doc": "When integral literal is used in decimal operations, pick a minimum precision required by the literal if this config is true, to make the resulting precision and/or scale smaller. This can reduce the possibility of precision lose and/or overflow.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.legacy.lpadRpadAlwaysReturnString",
      "doc": "When set to false, when the first argument and the optional padding pattern is a byte sequence, the result is a BINARY value. The default padding pattern in this case is the zero byte. When set to true, it restores the legacy behavior of always returning string types even for binary inputs.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.mssqlserver.datetimeoffsetMapping.enabled",
      "doc": "When true, DATETIMEOFFSET is mapped to StringType; otherwise, it is mapped to TimestampType.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.mssqlserver.numericMapping.enabled",
      "doc": "When true, use legacy MsSqlServer TINYINT, SMALLINT and REAL type mapping.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.mysql.bitArrayMapping.enabled",
      "doc": "When true, use LongType to represent MySQL BIT(n>1); otherwise, use BinaryType.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.mysql.timestampNTZMapping.enabled",
      "doc": "When true, TimestampNTZType and MySQL TIMESTAMP can be converted bidirectionally. For reading, MySQL TIMESTAMP is converted to TimestampNTZType when JDBC read option preferTimestampNTZ is true. For writing, TimestampNTZType is converted to MySQL TIMESTAMP; otherwise, DATETIME",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.negativeIndexInArrayInsert",
      "doc": "When set to true, restores the legacy behavior of `array_insert` for negative indexes - 0-based: the function inserts new element before the last one for the index -1. For example, `array_insert(['a', 'b'], -1, 'x')` returns `['a', 'x', 'b']`. When set to false, the -1 index points out to the last element, and the given example produces `['a', 'b', 'x']`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.noCharPaddingInPredicate",
      "doc": "When true, Spark will not apply char type padding for CHAR type columns in string comparison predicates, when 'spark.sql.readSideCharPadding' is false.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.notReserveProperties",
      "doc": "When true, all database and table properties are not reserved and available for create/alter syntaxes. But please be aware that the reserved properties will be silently removed.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.nullInEmptyListBehavior",
      "doc": "When set to true, restores the legacy incorrect behavior of IN expressions for NULL values IN an empty list (including IN subqueries and literal IN lists): `null IN (empty list)` should evaluate to false, but sometimes (not always) incorrectly evaluates to null in the legacy behavior."
    },
    {
      "key": "spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv",
      "doc": "When set to false, nulls are written as unquoted empty strings in CSV data source. If set to true, it restores the legacy behavior that nulls were written as quoted empty strings, `\"\"`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.oracle.timestampMapping.enabled",
      "doc": "When true, TimestampType maps to TIMESTAMP in Oracle; otherwise, TIMESTAMP WITH LOCAL TIME ZONE.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.parameterSubstitution.constantsOnly",
      "doc": "When true, limits parameter substitution to constants in DML/queries only, restoring the legacy behavior where parameter markers (? or :param) are only allowed in contexts where constant literals are expected. When false (default), parameter substitution is enabled everywhere a literal is supported, allowing parameter markers in any literal context throughout SQL parsing.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.parquet.datetimeRebaseModeInWrite",
      "doc": "(Removed)",
      "defaultValue": "CORRECTED",
      "removed": {
        "version": "4.0.0",
        "comment": "Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead."
      }
    },
    {
      "key": "spark.sql.legacy.parquet.int96RebaseModeInRead",
      "doc": "(Removed)",
      "defaultValue": "CORRECTED",
      "removed": {
        "version": "4.0.0",
        "comment": "Use 'spark.sql.parquet.int96RebaseModeInRead' instead."
      }
    },
    {
      "key": "spark.sql.legacy.parquet.int96RebaseModeInWrite",
      "doc": "(Removed)",
      "defaultValue": "CORRECTED",
      "removed": {
        "version": "4.0.0",
        "comment": "Use 'spark.sql.parquet.int96RebaseModeInWrite' instead."
      }
    },
    {
      "key": "spark.sql.legacy.parquet.nanosAsLong",
      "doc": "When true, the Parquet's nanos precision timestamps are converted to SQL long values.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.parquet.returnNullStructIfAllFieldsMissing",
      "doc": "When true, if all requested fields of a struct are missing in a parquet file, assume the struct is always null, even if other fields are present. The default behavior is to fetch and read an arbitrary non-requested field present in the file to determine struct nullness. If enabled, schema pruning may cause non-null structs to be read as null.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.parseNullPartitionSpecAsStringLiteral",
      "doc": "If it is set to true, `PARTITION(col=null)` is parsed as a string literal of its text representation, e.g., string 'null', when the partition column is string type. Otherwise, it is always parsed as a null literal in the partition spec.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.parseQueryWithoutEof",
      "doc": "When set to true, ParserInterface#parseQuery(...) is going to use base `query` grammar term without EOF resulting in some queries (like `SELECT 1 UNION SELECT 2`) to be parsed incorrectly - `UNION` will be treated as an alias, and the rest of SQL input will be thrown away.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.parser.havingWithoutGroupByAsWhere",
      "doc": "If it is set to true, the parser will treat HAVING without GROUP BY as a normal WHERE, which does not follow SQL standard.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.pathOptionBehavior.enabled",
      "doc": "When true, \"path\" option is overwritten if one path parameter is passed to DataFrameReader.load(), DataFrameWriter.save(), DataStreamReader.load(), or DataStreamWriter.start(). Also, \"path\" option is added to the overall paths if multiple path parameters are passed to DataFrameReader.load()",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.percentileDiscCalculation",
      "doc": "If true, the old bogus percentile_disc calculation is used. The old calculation incorrectly mapped the requested percentile to the sorted range of values in some cases and so returned incorrect results. Also, the new implementation is faster as it doesn't contain the interpolation logic that the old percentile_cont based one did.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.postgres.datetimeMapping.enabled",
      "doc": "When true, TimestampType maps to TIMESTAMP WITHOUT TIME ZONE in PostgreSQL for writing; otherwise, TIMESTAMP WITH TIME ZONE. When true, TIMESTAMP WITH TIME ZONE can be converted to TimestampNTZType when JDBC read option preferTimestampNTZ is true; otherwise, converted to TimestampType regardless of preferTimestampNTZ.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.raiseErrorWithoutErrorClass",
      "doc": "When set to true, restores the legacy behavior of `raise_error` and `assert_true` to not return the `[USER_RAISED_EXCEPTION]` prefix. For example, `raise_error('error!')` returns `error!` instead of `[USER_RAISED_EXCEPTION] Error!`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.readFileSourceTableCacheIgnoreOptions",
      "doc": "When set to true, reading from file source table caches the first query plan and ignores subsequent changes in query options. Otherwise, query options will be applied to the cached plan and may produce different results.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.replaceDatabricksSparkAvro.enabled",
      "doc": "If it is set to true, the data source provider com.databricks.spark.avro is mapped to the built-in but external Avro data source module for backward compatibility.",
      "defaultValue": "true",
      "deprecated": {
        "version": "3.2",
        "comment": "Use `.format(\"avro\")` in `DataFrameWriter` or `DataFrameReader` instead."
      }
    },
    {
      "key": "spark.sql.legacy.respectNullabilityInTextDatasetConversion",
      "doc": "When true, the nullability in the user-specified schema for `DataFrameReader.schema(schema).json(jsonDataset)` and `DataFrameReader.schema(schema).csv(csvDataset)` is respected. Otherwise, they are turned to a nullable schema forcibly.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.scalarSubqueryAllowGroupByNonEqualityCorrelatedPredicate",
      "doc": "When set to true, use incorrect legacy behavior for checking whether a scalar subquery with a group-by on correlated columns is allowed. See SPARK-48503",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.scalarSubqueryCountBugBehavior",
      "doc": "When set to true, restores legacy behavior of potential incorrect count bug handling for scalar subqueries.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.sessionInitWithConfigDefaults",
      "doc": "Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf defaults, dropping any overrides in its parent SparkSession.",
      "defaultValue": "false",
      "isStatic": true
    },
    {
      "key": "spark.sql.legacy.setCommandRejectsSparkCoreConfs",
      "doc": "If it is set to true, SET command will fail when the key is registered as a SparkConf entry.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.legacy.setopsPrecedence.enabled",
      "doc": "When set to true and the order of evaluation is not specified by parentheses, the set operations are performed from left to right as they appear in the query. When set to false and order of evaluation is not specified by parentheses, INTERSECT operations are performed before any UNION, EXCEPT and MINUS operations.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.sizeOfNull",
      "doc": "If it is set to false, or spark.sql.ansi.enabled is true, then size of null returns null. Otherwise, it returns -1, which was inherited from Hive.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.legacy.skipTypeValidationOnAlterPartition",
      "doc": "When true, skip validation for partition spec in ALTER PARTITION. E.g., `ALTER TABLE .. ADD PARTITION(p='a')` would work even the partition type is int. Besides, this config will also be used to skip type validation on partition spec when reading partitioned table. E.g., if the table partition spec is added without type validation, it might not be read correctly with the type validation. When false, the behavior follows spark.sql.storeAssignmentPolicy",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.statisticalAggregate",
      "doc": "When set to true, statistical aggregate function returns Double.NaN if divide by zero occurred during expression evaluation, otherwise, it returns null. Before version 3.1.0, it returns NaN in divideByZero case by default.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.storeAnalyzedPlanForView",
      "doc": "When true, analyzed plan instead of SQL text will be stored when creating temporary view",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.timeParserPolicy",
      "doc": "When LEGACY, java.text.SimpleDateFormat is used for formatting and parsing dates/timestamps in a locale-sensitive manner, which is the approach before Spark 3.0. When set to CORRECTED, classes from java.time.* packages are used for the same purpose. When set to EXCEPTION, RuntimeException is thrown when we will get different results. The default is CORRECTED.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.legacy.truncateForEmptyRegexSplit",
      "doc": "When set to true, splitting a string of length n using an empty regex with a positive limit discards the last n - limit characters.For example: SELECT split('abcd', '', 2) returns ['a', 'b'].When set to false, the last element of the resulting array contains all input beyond the last matched regex.For example: SELECT split('abcd', '', 2) returns ['a', 'bcd'].According to the description of the split function, this should be set to false by default. See SPARK-49968 for details.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.typeCoercion.datetimeToString.enabled",
      "doc": "If it is set to true, date/timestamp will cast to string in binary comparisons with String when spark.sql.ansi.enabled is false.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.useCurrentConfigsForView",
      "doc": "When true, SQL Configs of the current active SparkSession instead of the captured ones will be applied during the parsing and analysis phases of the view resolution.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.useLegacyXMLParser",
      "doc": "When set to true, use the legacy XML parser for parsing XML files. Compared to the default parser, the legacy parser has less stringent validation checks for malformed content, but it's less memory-efficient",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.useV1Command",
      "doc": "When true, Spark will use legacy V1 SQL commands.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.v1IdentifierNoCatalog",
      "doc": "When set to false, the v1 identifier will include 'spark_catalog' as the catalog name if database is defined. When set to true, it restores the legacy behavior that does not include catalog name.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.legacy.viewSchemaBindingMode",
      "doc": "Set to false to disable the WITH SCHEMA clause for view DDL and suppress the line in DESCRIBE EXTENDED and SHOW CREATE TABLE.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.legacy.viewSchemaCompensation",
      "doc": "Set to false to revert default view schema binding mode from WITH SCHEMA COMPENSATION to WITH SCHEMA BINDING.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.lightweightPlanChangeValidation",
      "doc": "Similar to spark.sql.planChangeValidation, this validates plan changes and runs after every rule, however it is enabled by default and so it should be lightweight.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.limit.initialNumPartitions",
      "doc": "Initial number of partitions to try when executing a take on a query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run",
      "defaultValue": "1"
    },
    {
      "key": "spark.sql.limit.scaleUpFactor",
      "doc": "Minimal increase rate in number of partitions between attempts when executing a take on a query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run",
      "defaultValue": "4"
    },
    {
      "key": "spark.sql.mapKeyDedupPolicy",
      "doc": "The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.",
      "defaultValue": "EXCEPTION"
    },
    {
      "key": "spark.sql.mapZipWithUsesJavaCollections",
      "doc": "When true, the `map_zip_with` function uses Java collections instead of Scala collections. This is useful for avoiding NaN equality issues.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.maven.additionalRemoteRepositories",
      "doc": "A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.",
      "defaultValue": "https://maven-central.storage-download.googleapis.com/maven2/"
    },
    {
      "key": "spark.sql.maxBroadcastTableSize",
      "doc": "The maximum table size in bytes that can be broadcast in broadcast joins.",
      "defaultValue": "8589934592b"
    },
    {
      "key": "spark.sql.maxConcurrentOutputFileWriters",
      "doc": "Maximum number of output file writers to use concurrently. If number of writers needed reaches this limit, task will sort rest of output then writing them.",
      "defaultValue": "0"
    },
    {
      "key": "spark.sql.maxMetadataStringLength",
      "doc": "Maximum number of characters to output for a metadata string. e.g. file location in `DataSourceScanExec`, every value will be abbreviated if exceed length.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.maxPlanStringLength",
      "doc": "Maximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated.  The default setting always generates a full plan.  Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.",
      "defaultValue": "2147483632"
    },
    {
      "key": "spark.sql.maxSinglePartitionBytes",
      "doc": "The maximum number of bytes allowed for a single partition. Otherwise, The planner will introduce shuffle to improve parallelism.",
      "defaultValue": "128m"
    },
    {
      "key": "spark.sql.mergeNestedTypeCoercion.enabled",
      "doc": "If enabled, allow MERGE INTO to coerce source nested types if they have lessnested fields than the target table's nested types. This is experimental andthe semantics may change.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.metadataCacheTTLSeconds",
      "doc": "Time-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to `hive`, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' > 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to `true` to be applied to the partition file metadata cache.",
      "defaultValue": "-1000ms",
      "isStatic": true
    },
    {
      "key": "spark.sql.nameResolutionLog.level",
      "doc": "Configures the log level for logging the name resolution in the single-pass bottom-up Resolver. The value can be ERROR, WARN, INFO, DEBUG, TRACE.",
      "defaultValue": "TRACE"
    },
    {
      "key": "spark.sql.objectHashAggregate.sortBased.fallbackThreshold",
      "doc": "In the case of ObjectHashAggregateExec, when the size of the in-memory hash map grows too large, we will fall back to sort-based aggregation. This option sets a row count threshold for the size of the hash map.",
      "defaultValue": "128"
    },
    {
      "key": "spark.sql.operatorPipeSyntaxEnabled",
      "doc": "If true, enable operator pipe syntax for Apache Spark SQL. This uses the operator pipe marker |> to indicate separation between clauses of SQL in a manner that describes the sequence of steps that the query performs in a composable fashion.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizeNullAwareAntiJoin",
      "doc": "When true, NULL-aware anti join execution will be planed into BroadcastHashJoinExec with flag isNullAwareAntiJoin enabled, optimized from O(M*N) calculation into O(M) calculation using Hash lookup instead of Looping lookup. Only support for singleColumn NAAJ for now.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.avoidCollapseUDFWithExpensiveExpr",
      "doc": "Whether to avoid collapsing projections that would duplicate expensive expressions in UDFs.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.canChangeCachedPlanOutputPartitioning",
      "doc": "Whether to forcibly enable some optimization rules that can change the output partitioning of a cached query when executing it for caching. If it is set to true, queries may need an extra shuffle to read the cached data. This configuration is disabled by default. The optimization rule enabled by this configuration is spark.sql.adaptive.applyFinalStageShuffleOptimizations.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.collapseProjectAlwaysInline",
      "doc": "Whether to always collapse two adjacent projections and inline expressions even if it causes extra duplication.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.datasourceV2ExprFolding",
      "doc": "When this config is set to true, do safe constant folding for the expressions before translation and pushdown.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.datasourceV2JoinPushdown",
      "doc": "When this config is set to true, join is tried to be pushed downfor DSv2 data sources in V2ScanRelationPushdown optimization rule.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.decorrelateExistsSubqueryLegacyIncorrectCountHandling.enabled",
      "doc": "If enabled, revert to legacy incorrect behavior for certain EXISTS/IN subqueries with COUNT or similar aggregates.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.decorrelateInnerQuery.enabled",
      "doc": "Decorrelate inner query by eliminating correlated references and build domain joins.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelateJoinPredicate.enabled",
      "doc": "Decorrelate scalar and lateral subqueries with correlated references in join predicates. This configuration is only effective when 'spark.sql.optimizer.decorrelateInnerQuery.enabled' is true.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelateLimit.enabled",
      "doc": "Decorrelate subqueries with correlation under LIMIT.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelateOffset.enabled",
      "doc": "Decorrelate subqueries with correlation under LIMIT with OFFSET.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelatePredicateSubqueriesInJoinPredicate.enabled",
      "doc": "Decorrelate predicate (in and exists) subqueries with correlated references in join predicates.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelateSetOps.enabled",
      "doc": "Decorrelate subqueries with correlation under set operators.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelateSubqueryLegacyIncorrectCountHandling.enabled",
      "doc": "If enabled, revert to legacy incorrect behavior for certain subqueries with COUNT or similar aggregates: see SPARK-43098.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.decorrelateSubqueryPreventConstantHoldingForCountBug.enabled",
      "doc": "If enabled, prevents constant folding in subqueries that contain a COUNT-bug-susceptible Aggregate.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.decorrelateUnionOrSetOpUnderLimit.enabled",
      "doc": "Decorrelate UNION or SET operation under LIMIT operator. If not enabled,revert to legacy incorrect behavior for certain subqueries with correlation underUNION/SET operator with a LIMIT operator above it.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.disableHints",
      "doc": "When true, the optimizer will disable user-specified hints that are additional directives for better planning of a query.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.dynamicPartitionPruning.enabled",
      "doc": "When true, we will generate predicate for partition column when it's used as join key",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio",
      "doc": "When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.",
      "defaultValue": "0.5"
    },
    {
      "key": "spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly",
      "doc": "When true, dynamic partition pruning will only apply when the broadcast exchange of a broadcast hash join operation can be reused as the dynamic pruning filter.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.dynamicPartitionPruning.useStats",
      "doc": "When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.enableCsvExpressionOptimization",
      "doc": "Whether to optimize CSV expressions in SQL optimizer. It includes pruning unnecessary columns from from_csv.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.enableJsonExpressionOptimization",
      "doc": "Whether to optimize JSON expressions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifying from_json + to_json, to_json + named_struct(from_json.col1, from_json.col2, ....).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.excludeSubqueryRefsFromRemoveRedundantAliases.enabled",
      "doc": "When true, exclude the references from the subquery expressions (in, exists, etc.) while removing redundant aliases.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.excludedRules",
      "doc": "Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded."
    },
    {
      "key": "spark.sql.optimizer.expression.nestedPruning.enabled",
      "doc": "Prune nested fields from expressions in an operator which are unnecessary in satisfying a query. Note that this optimization doesn't prune nested fields from physical data source scanning. For pruning nested fields from scanning, please use `spark.sql.optimizer.nestedSchemaPruning.enabled` config.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.expressionProjectionCandidateLimit",
      "doc": "The maximum number of the candidate of output expressions whose alias are replaced. It can preserve the output partitioning and ordering. Negative value means disable this optimization.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.optimizer.inSetConversionThreshold",
      "doc": "The threshold of set size for InSet conversion.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.optimizer.inSetSwitchThreshold",
      "doc": "Configures the max set size in InSet for which Spark will generate code with switch statements. This is applicable only to bytes, shorts, ints, dates.",
      "defaultValue": "400"
    },
    {
      "key": "spark.sql.optimizer.maxIterations",
      "doc": "The max number of iterations the optimizer runs.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.optimizer.metadataOnly",
      "doc": "When true, enable the metadata-only query optimization that use the table's metadata to produce the partition columns instead of table scans. It applies when all the columns scanned are partition columns and the query has an aggregate operator that satisfies distinct semantics. By default the optimization is disabled, and deprecated as of Spark 3.0 since it may return incorrect results when the files are empty, see also SPARK-26709. It will be removed in the future releases. If you must use, use 'SparkSessionExtensions' instead to inject it as a custom rule.",
      "defaultValue": "false",
      "deprecated": {
        "version": "3.0",
        "comment": "Avoid to depend on this optimization to prevent a potential correctness issue. If you must use, use 'SparkSessionExtensions' instead to inject it as a custom rule."
      }
    },
    {
      "key": "spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources",
      "doc": "A comma-separated list of data source short names or fully qualified data source implementation class names for which Spark tries to push down predicates for nested columns and/or names containing `dots` to data sources. This configuration is only effective with file-based data sources in DSv1. Currently, Parquet and ORC implement both optimizations. The other data sources don't support this feature yet. So the default value is 'parquet,orc'.",
      "defaultValue": "parquet,orc"
    },
    {
      "key": "spark.sql.optimizer.nestedSchemaPruning.enabled",
      "doc": "Prune nested fields from a logical relation's output which are unnecessary in satisfying a query. This optimization allows columnar file format readers to avoid reading unnecessary nested column data. Currently Parquet and ORC are the data sources that implement this optimization.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.optimizeOneRowRelationSubquery",
      "doc": "When true, the optimizer will inline subqueries with OneRowRelation as leaf nodes.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.optimizeOneRowRelationSubquery.alwaysInline",
      "doc": "When true, the optimizer will always inline single row subqueries even if it causes extra duplication. It only takes effect when spark.sql.optimizer.optimizeOneRowRelationSubquery is set to true.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.optimizeUncorrelatedInSubqueriesInJoinCondition.enabled",
      "doc": "When true, optimize uncorrelated IN subqueries in join predicates by rewriting them to joins. This interacts with spark.sql.legacy.nullInEmptyListBehavior because it can rewrite IN predicates.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.planChangeLog.batches",
      "doc": "(Removed)",
      "defaultValue": "",
      "removed": {
        "version": "3.1.0",
        "comment": "Please use `spark.sql.planChangeLog.batches` instead."
      }
    },
    {
      "key": "spark.sql.optimizer.planChangeLog.level",
      "doc": "(Removed)",
      "defaultValue": "trace",
      "removed": {
        "version": "3.1.0",
        "comment": "Please use `spark.sql.planChangeLog.level` instead."
      }
    },
    {
      "key": "spark.sql.optimizer.planChangeLog.rules",
      "doc": "(Removed)",
      "defaultValue": "",
      "removed": {
        "version": "3.1.0",
        "comment": "Please use `spark.sql.planChangeLog.rules` instead."
      }
    },
    {
      "key": "spark.sql.optimizer.plannedWrite.enabled",
      "doc": "When set to true, Spark optimizer will add logical sort operators to V1 write commands if needed so that `FileFormatWriter` does not need to insert physical sorts.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.preserveAliasMetadataWhenCollapsingProjects",
      "doc": "When true, make sure to explicitly copy the metadata of the aliases from lower project list.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.propagateDistinctKeys.enabled",
      "doc": "When true, the query optimizer will propagate a set of distinct attributes from the current node and use it to optimize query.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan",
      "doc": "Allow PruneFilters to remove streaming subplans when we encounter a false filter. This flag is to restore prior buggy behavior for broken pipelines.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.optimizer.pullHintsIntoSubqueries",
      "doc": "Pull hints into subqueries in EliminateResolvedHint if enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.pullOutNestedDataOuterRefExpressions.enabled",
      "doc": "Handle correlation over nested data extract expressions by pulling out the expression into the outer plan. This enables correlation on map attributes for example.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.replaceExceptWithFilter",
      "doc": "When true, the apply function of the rule verifies whether the right node of the except operation is of type Filter or Project followed by Filter. If yes, the rule further verifies 1) Excluding the filter operations from the right (as well as the left node, if any) on the top, whether both the nodes evaluates to a same result. 2) The left and right nodes don't contain any SubqueryExpressions. 3) The output column names of the left node are distinct. If all the conditions are met, the rule will replace the except operation with a Filter by flipping the filter condition(s) of the right node.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold",
      "doc": "Byte size threshold of the Bloom filter application side plan's aggregated scan size. Aggregated scan byte size of the Bloom filter application side needs to be over this value to inject a bloom filter.",
      "defaultValue": "10GB"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.creationSideThreshold",
      "doc": "Size threshold of the bloom filter creation side plan. Estimated size needs to be under this value to try to inject bloom filter.",
      "defaultValue": "10MB"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.enabled",
      "doc": "When true and if one side of a shuffle join has a selective predicate, we attempt to insert a bloom filter in the other side to reduce the amount of shuffle data.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.expectedNumItems",
      "doc": "The default number of expected items for the runtime bloomfilter",
      "defaultValue": "1000000"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.maxNumBits",
      "doc": "The max number of bits to use for the runtime bloom filter",
      "defaultValue": "67108864"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.maxNumItems",
      "doc": "The max allowed number of expected items for the runtime bloom filter",
      "defaultValue": "4000000"
    },
    {
      "key": "spark.sql.optimizer.runtime.bloomFilter.numBits",
      "doc": "The default number of bits to use for the runtime bloom filter",
      "defaultValue": "8388608"
    },
    {
      "key": "spark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled",
      "doc": "Enables runtime group filtering for group-based row-level operations. Data sources that replace groups of data (e.g. files, partitions) may prune entire groups using provided data source filters when planning a row-level operation scan. However, such filtering is limited as not all expressions can be converted into data source filters and some expressions can only be evaluated by Spark (e.g. subqueries). Since rewriting groups is expensive, Spark can execute a query at runtime to find what records match the condition of the row-level operation. The information about matching records will be passed back to the row-level operation scan, allowing data sources to discard groups that don't have to be rewritten.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.runtimeFilter.number.threshold",
      "doc": "The total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OOMs with too many Bloom filters.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled",
      "doc": "(Removed)",
      "defaultValue": "false",
      "removed": {
        "version": "4.0.0",
        "comment": "This optimizer config is useless as runtime filter cannot be an IN subquery now."
      }
    },
    {
      "key": "spark.sql.optimizer.scalarSubqueryUseSingleJoin",
      "doc": "When set to true, use LEFT_SINGLE join for correlated scalar subqueries where optimizer can't prove that only 1 row will be returned",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.serializer.nestedSchemaPruning.enabled",
      "doc": "Prune nested fields from object serialization operator which are unnecessary in satisfying a query. This optimization allows object serializers to avoid executing unnecessary nested expressions.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.optimizer.windowGroupLimitThreshold",
      "doc": "Threshold for triggering `InsertWindowGroupLimit`. 0 means the output results is empty. -1 means disabling the optimization.",
      "defaultValue": "1000"
    },
    {
      "key": "spark.sql.optimizer.wrapExistsInAggregateFunction",
      "doc": "When true, the optimizer will wrap newly introduced `exists` attributes in an aggregate function to ensure that Aggregate nodes preserve semantic invariant that each variable among agg expressions appears either in grouping expressions or belongs to and aggregate function.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.orc.aggregatePushdown",
      "doc": "If true, aggregates will be pushed down to ORC for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any ORC file footer, exception would be thrown.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.orc.columnarReaderBatchSize",
      "doc": "The number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.",
      "defaultValue": "4096"
    },
    {
      "key": "spark.sql.orc.columnarWriterBatchSize",
      "doc": "The number of rows to include in a orc vectorized writer batch. The number should be carefully chosen to minimize overhead and avoid OOMs in writing data.",
      "defaultValue": "1024"
    },
    {
      "key": "spark.sql.orc.compression.codec",
      "doc": "Sets the compression codec used when writing ORC files. If either `compression` or `orc.compress` is specified in the table-specific options/properties, the precedence would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`. Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd, lz4, brotli.",
      "defaultValue": "zstd"
    },
    {
      "key": "spark.sql.orc.enableNestedColumnVectorizedReader",
      "doc": "Enables vectorized orc decoding for nested column.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.orc.enableVectorizedReader",
      "doc": "Enables vectorized orc decoding.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.orc.filterPushdown",
      "doc": "When true, enable filter pushdown for ORC files.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.orc.impl",
      "doc": "When native, use the native version of ORC support instead of the ORC library in Hive. It is 'hive' by default prior to Spark 2.4.",
      "defaultValue": "native"
    },
    {
      "key": "spark.sql.orc.mergeSchema",
      "doc": "When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.orderByOrdinal",
      "doc": "When true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.orderingAwareLimitOffset",
      "doc": "When set to true, a local sort will be inserted between GlobalLimitExec and single-partition ShuffleExchangeExec, if the underlying plan produces sorted data. This is because shuffle reader in Spark fetches shuffle blocks in a random order and can not preserve the data ordering, while LIMIT/OFFSET must preserve ordering.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.aggregatePushdown",
      "doc": "If true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any Parquet file footer, exception would be thrown.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.binaryAsString",
      "doc": "Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.columnarReaderBatchSize",
      "doc": "The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.",
      "defaultValue": "4096"
    },
    {
      "key": "spark.sql.parquet.compression.codec",
      "doc": "Sets the compression codec used when writing Parquet files. If either `compression` or `parquet.compression` is specified in the table-specific options/properties, the precedence would be `compression`, `parquet.compression`, `spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd.",
      "defaultValue": "snappy"
    },
    {
      "key": "spark.sql.parquet.datetimeRebaseModeInRead",
      "doc": "When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown. This config influences on reads of the following parquet logical types: DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS. The INT96 type has the separate config: spark.sql.parquet.int96RebaseModeInRead.",
      "defaultValue": "CORRECTED",
      "alternatives": [
        "spark.sql.legacy.parquet.datetimeRebaseModeInRead"
      ]
    },
    {
      "key": "spark.sql.parquet.datetimeRebaseModeInWrite",
      "doc": "When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config influences on writes of the following parquet logical types: DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS. The INT96 type has the separate config: spark.sql.parquet.int96RebaseModeInWrite.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.parquet.enableNestedColumnVectorizedReader",
      "doc": "Enables vectorized Parquet decoding for nested columns (e.g., struct, list, map). Requires spark.sql.parquet.enableVectorizedReader to be enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.enableNullTypeVectorizedReader",
      "doc": "Enables vectorized Parquet reader support for NullType columns.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.enableVectorizedReader",
      "doc": "Enables vectorized parquet decoding.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.fieldId.read.enabled",
      "doc": "Field ID is a native field of the Parquet schema spec. When enabled, Parquet readers will use field IDs (if present) in the requested Spark schema to look up Parquet fields instead of using column names",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.fieldId.read.ignoreMissing",
      "doc": "When the Parquet file doesn't have any field IDs but the Spark read schema is using field IDs to read, we will silently return nulls when this flag is enabled, or error otherwise.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.fieldId.write.enabled",
      "doc": "Field ID is a native field of the Parquet schema spec. When enabled, Parquet writers will populate the field Id metadata (if present) in the Spark schema to the Parquet schema.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.filterPushdown",
      "doc": "Enables Parquet filter push-down optimization when set to true.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.filterPushdown.date",
      "doc": "If true, enables Parquet filter push-down optimization for Date. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.filterPushdown.decimal",
      "doc": "If true, enables Parquet filter push-down optimization for Decimal. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.filterPushdown.string.startsWith",
      "doc": "If true, enables Parquet filter push-down optimization for string startsWith function. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.filterPushdown.stringPredicate",
      "doc": "If true, enables Parquet filter push-down optimization for string predicate such as startsWith/endsWith/contains function. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled.",
      "fallback": "spark.sql.parquet.filterPushdown.string.startsWith"
    },
    {
      "key": "spark.sql.parquet.filterPushdown.timestamp",
      "doc": "If true, enables Parquet filter push-down optimization for Timestamp. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and Timestamp stored as TIMESTAMP_MICROS or TIMESTAMP_MILLIS type.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.ignoreVariantAnnotation",
      "doc": "When true, ignore the variant logical type annotation and treat the Parquet column in the same way as the underlying struct type",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.inferTimestampNTZ.enabled",
      "doc": "When enabled, Parquet timestamp columns with annotation isAdjustedToUTC = false are inferred as TIMESTAMP_NTZ type during schema inference. Otherwise, all the Parquet timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the output schema into Parquet's footer metadata on file writing and leverages it on file reading. Thus this configuration only affects the schema inference on Parquet files which are not written by Spark.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.int64AsTimestampMillis",
      "doc": "(Removed)",
      "defaultValue": "false",
      "removed": {
        "version": "3.0.0",
        "comment": "The config was deprecated since Spark 2.3. Use 'spark.sql.parquet.outputTimestampType' instead of it."
      }
    },
    {
      "key": "spark.sql.parquet.int96AsTimestamp",
      "doc": "Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.int96RebaseModeInRead",
      "doc": "When LEGACY, Spark will rebase INT96 timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files. When CORRECTED, Spark will not do rebase and read the timestamps as it is. When EXCEPTION, Spark will fail the reading if it sees ancient timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.parquet.int96RebaseModeInWrite",
      "doc": "When LEGACY, Spark will rebase INT96 timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files. When CORRECTED, Spark will not do rebase and write the timestamps as it is. When EXCEPTION, Spark will fail the writing if it sees ancient timestamps that are ambiguous between the two calendars.",
      "defaultValue": "CORRECTED"
    },
    {
      "key": "spark.sql.parquet.int96TimestampConversion",
      "doc": "This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.mergeSchema",
      "doc": "When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.output.committer.class",
      "doc": "The output committer class used by Parquet. The specified class needs to be a subclass of org.apache.hadoop.mapreduce.OutputCommitter. Typically, it's also a subclass of org.apache.parquet.hadoop.ParquetOutputCommitter. If it is not, then metadata summaries will never be created, irrespective of the value of parquet.summary.metadata.level",
      "defaultValue": "org.apache.parquet.hadoop.ParquetOutputCommitter"
    },
    {
      "key": "spark.sql.parquet.outputTimestampType",
      "doc": "Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.",
      "defaultValue": "INT96"
    },
    {
      "key": "spark.sql.parquet.pushdown.inFilterThreshold",
      "doc": "For IN predicate, Parquet filter will push-down a set of OR clauses if its number of values not exceeds this threshold. Otherwise, Parquet filter will push-down a value greater than or equal to its minimum value and less than or equal to its maximum value. By setting this value to 0 this feature can be disabled. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.parquet.recordLevelFilter.enabled",
      "doc": "If true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.respectSummaryFiles",
      "doc": "When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parquet.variant.annotateLogicalType.enabled",
      "doc": "When enabled, Spark annotates the variant groups written to Parquet as the parquet variant logical type.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parquet.writeLegacyFormat",
      "doc": "If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parser.eagerEvalOfUnresolvedInlineTable",
      "doc": "Controls whether we optimize the ASTree that gets generated when parsing VALUES lists (UnresolvedInlineTable) by eagerly evaluating it in the AST Builder.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.parser.escapedStringLiterals",
      "doc": "When true, string literals (including regex patterns) remain escaped in our SQL parser. The default is false since Spark 2.0. Setting it to true can restore the behavior prior to Spark 2.0.",
      "defaultValue": "false",
      "deprecated": {
        "version": "4.0",
        "comment": "Use raw string literals with the `r` prefix instead. "
      }
    },
    {
      "key": "spark.sql.parser.manageParserCaches",
      "doc": "When true, we install our own ANTLR caches to manage memory usage. When false, we use the\ndefault ANTLR caches. Dependency for\n`spark.sql.parser.parserDfaCacheFlushThreshold`.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.parser.parserDfaCacheFlushRatio",
      "doc": "Like `spark.sql.parser.parserDfaCacheFlushThreshold`, but uses a threshold that is a\nlinear function of the memory allocated to the driver process. Represents the percentage\nof the driver memory that the DFA cache can consume before it is flushed.\n\nEstimates the memory used by the DFA cache, assuming each state consumes\n`AbstractParser.BYTES_PER_DFA_STATE` bytes. If this value exceeds the product of the\ndriver memory with the config value (interpreted as a percentage), the cache is flushed.\n\nActive values should be in the range 0-100, and a negative value disables the feature.\nIf both this config and `spark.sql.parser.parserDfaCacheFlushThreshold` are set, the\ncache is flushed if either condition is met.\nRequires `spark.sql.parser.manageParserCaches` to be true to take effect.\n",
      "defaultValue": "-1.0"
    },
    {
      "key": "spark.sql.parser.parserDfaCacheFlushThreshold",
      "doc": "When positive, release ANTLR caches after parsing a SQL query when the number of states\nin the DFA cache exceeds the value of the config. DFA states empirically consume about\n`AbstractParser.BYTES_PER_DFA_STATE` bytes of memory each.\n\nANTLR parsers retain a DFA cache designed to speed up parsing future input. However,\nthere is no limit to how large this cache can become. Parsing large SQL statements can\nlead to an accumulation of objects in the cache that are unlikely to be reused, causing\nhigh GC overhead and eventually OOMs.\n\nIf this config is set to a negative value, it is ignored.\nIf both this config and `spark.sql.parser.parserDfaCacheFlushRatio` are set, the\ncache is flushed if either condition is met.\nRequires `spark.sql.parser.manageParserCaches` to be true to take effect.\n\nCan significantly slow down parsing in exchange for better memory stability.\n",
      "defaultValue": "-1"
    },
    {
      "key": "spark.sql.parser.quotedRegexColumnNames",
      "doc": "When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.pipelines.event.queue.capacity",
      "doc": "Capacity of the event queue used in pipelined execution. When the queue is full, non-terminal FlowProgressEvents will be dropped.",
      "defaultValue": "1000"
    },
    {
      "key": "spark.sql.pipelines.execution.maxConcurrentFlows",
      "doc": "Max number of flows to execute at once. Used to tune performance for triggered pipelines. Has no effect on continuous pipelines.",
      "defaultValue": "16"
    },
    {
      "key": "spark.sql.pipelines.execution.streamstate.pollingInterval",
      "doc": "Interval in seconds at which the stream state is polled for changes. This is used to check if the stream has failed and needs to be restarted.",
      "defaultValue": "1000ms"
    },
    {
      "key": "spark.sql.pipelines.execution.watchdog.maxRetryTime",
      "doc": "Maximum time interval in seconds at which flows will be restarted.",
      "defaultValue": "3600000ms"
    },
    {
      "key": "spark.sql.pipelines.execution.watchdog.minRetryTime",
      "doc": "Initial duration in seconds between the time when we notice a flow has failed and when we try to restart the flow. The interval between flow restarts doubles with every stream failure up to the maximum value set in `pipelines.execution.watchdog.maxRetryTime`.",
      "defaultValue": "5000ms"
    },
    {
      "key": "spark.sql.pipelines.maxFlowRetryAttempts",
      "doc": "Maximum number of times a flow can be retried",
      "defaultValue": "2"
    },
    {
      "key": "spark.sql.pipelines.timeoutMsForTerminationJoinAndLock",
      "doc": "Timeout in milliseconds to grab a lock for stopping update - default is 1hr.",
      "defaultValue": "3600000ms"
    },
    {
      "key": "spark.sql.pivotMaxValues",
      "doc": "When doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.planChangeLog.batches",
      "doc": "Configures a list of batches for logging plan changes, in which the batches are specified by their batch names and separated by comma."
    },
    {
      "key": "spark.sql.planChangeLog.level",
      "doc": "Configures the log level for logging the change from the original plan to the new plan after a rule or batch is applied. The value can be ERROR, WARN, INFO, DEBUG, TRACE.",
      "defaultValue": "TRACE"
    },
    {
      "key": "spark.sql.planChangeLog.rules",
      "doc": "Configures a list of rules for logging plan changes, in which the rules are specified by their rule names and separated by comma."
    },
    {
      "key": "spark.sql.planChangeValidation",
      "doc": "If true, Spark will validate all the plan changes made by analyzer/optimizer and other catalyst rules, to make sure every rule returns a valid plan",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.planner.pythonExecution.memory",
      "doc": "Specifies the memory allocation for executing Python code in Spark driver, in MiB. When set, it caps the memory for Python execution to the specified amount. If not set, Spark will not limit Python's memory usage and it is up to the application to avoid exceeding the overhead memory space shared with other non-JVM processes.\nNote: Windows does not support resource limiting and actual resource is not limited on MacOS."
    },
    {
      "key": "spark.sql.preserveCharVarcharTypeInfo",
      "doc": "When true, Spark does not replace CHAR/VARCHAR types the STRING type, which is the default behavior of Spark 3.0 and earlier versions. This means the length checks for CHAR/VARCHAR types is enforced and CHAR type is also properly padded.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.prioritizeOrdinalResolutionInSort.enabled",
      "doc": "When set to true, we prioritize ordinal resolution in Sort over other expressions. Otherwise, no order is enforced.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.pyspark.inferNestedDictAsStruct.enabled",
      "doc": "PySpark's SparkSession.createDataFrame infers the nested dict as a map by default. When it set to true, it infers the nested dict as a struct.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.pyspark.jvmStacktrace.enabled",
      "doc": "When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled to hide JVM stacktrace and shows a Python-friendly exception only. Note that this is independent from log level settings.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled",
      "doc": "PySpark's SparkSession.createDataFrame infers the element type of an array from all values in the array by default. If this config is set to true, it restores the legacy behavior of only inferring the type from the first array element.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled",
      "doc": "PySpark's SparkSession.createDataFrame infers the key/value types of a map from all pairs in the map by default. If this config is set to true, it restores the legacy behavior of only inferring the type from the first non-null pair.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.pyspark.plotting.max_rows",
      "doc": "The visual limit on plots. If set to 1000 for top-n-based plots (pie, bar, barh), the first 1000 data points will be used for plotting. For sampled-based plots (scatter, area, line), 1000 data points will be randomly sampled.",
      "defaultValue": "1000"
    },
    {
      "key": "spark.sql.pyspark.udf.profiler",
      "doc": "Configure the Python/Pandas UDF profiler by enabling or disabling it with the option to choose between \"perf\" and \"memory\" types, or unsetting the config disables the profiler. This is disabled by default."
    },
    {
      "key": "spark.sql.pyspark.worker.logging.enabled",
      "doc": "When set to true, this configuration enables comprehensive logging within Python worker processes that execute User-Defined Functions (UDFs), User-Defined Table Functions (UDTFs), and other Python-based operations in Spark SQL.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.python.filterPushdown.enabled",
      "doc": "When true, enable filter pushdown to Python datasource, at the cost of running Python worker one additional time during planning.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.queryExecutionListeners",
      "doc": "List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.",
      "isStatic": true
    },
    {
      "key": "spark.sql.readSideCharPadding",
      "doc": "When true, Spark applies string padding when reading CHAR type columns/fields, in addition to the write-side padding. This config is true by default to better enforce CHAR type semantic in cases such as external tables.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.redaction.options.regex",
      "doc": "Regex to decide which keys in a Spark SQL command's options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.",
      "defaultValue": "(?i)url"
    },
    {
      "key": "spark.sql.redaction.string.regex",
      "doc": "Regex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from `spark.redaction.string.regex` is used.",
      "fallback": "spark.redaction.string.regex"
    },
    {
      "key": "spark.sql.repl.eagerEval.enabled",
      "doc": "Enables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is supported in PySpark and SparkR. In PySpark, for the notebooks like Jupyter, the HTML table (generated by _repr_html_) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned outputs are showed similar to R data.frame would.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.repl.eagerEval.maxNumRows",
      "doc": "The max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).",
      "defaultValue": "20"
    },
    {
      "key": "spark.sql.repl.eagerEval.truncate",
      "doc": "The max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.",
      "defaultValue": "20"
    },
    {
      "key": "spark.sql.requireAllClusterKeysForCoPartition",
      "doc": "When true, the planner requires all the clustering keys as the hash partition keys of the children, to eliminate the shuffles for the operator that needs its children to be co-partitioned, such as JOIN node. This is to avoid data skews which can lead to significant performance regression if shuffles are eliminated.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.requireAllClusterKeysForDistribution",
      "doc": "When true, the planner requires all the clustering keys as the partition keys (with same ordering) of the children, to eliminate the shuffle for the operator that requires its children be clustered distributed, such as AGGREGATE and WINDOW node. This is to avoid data skews which can lead to significant performance regression if shuffle is eliminated.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.resultQueryStage.maxThreadThreshold",
      "doc": "The maximum degree of parallelism to execute ResultQueryStageExec in AQE",
      "defaultValue": "1024",
      "isStatic": true
    },
    {
      "key": "spark.sql.retainGroupColumns",
      "doc": "",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.runCollationTypeCastsBeforeAliasAssignment.enabled",
      "doc": "When set to true, rules like ResolveAliases or ResolveAggregateFunctions will run CollationTypeCasts before alias assignment. This is necessary for correct alias generation.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.runSQLOnFiles",
      "doc": "When true, we could use `datasource`.`path` as table in SQL query.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.scriptTransformation.exitTimeoutInSeconds",
      "doc": "Timeout for executor to wait for the termination of transformation script when EOF.",
      "defaultValue": "10000ms"
    },
    {
      "key": "spark.sql.scripting.continueHandlerEnabled",
      "doc": "EXPERIMENTAL FEATURE/WORK IN PROGRESS: SQL Scripting CONTINUE HANDLER feature is under development and still not working as intended. This feature switch is intended to be used internally for development and testing, not by end users. YOU ARE ADVISED AGAINST USING THIS FEATURE AS ITS NOT FINISHED.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.scripting.enabled",
      "doc": "SQL Scripting feature is under development and its use should be done under this feature flag. SQL Scripting enables users to write procedural SQL including control flow and error handling.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.selfJoinAutoResolveAmbiguity",
      "doc": "",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.session.localRelationBatchOfChunksSizeBytes",
      "doc": "Limit on how much memory the client can use when uploading a local relation to the server. The client collects multiple local relation chunks into a single batch in memory until the limit is reached, then uploads the batch to the server. This helps reduce memory pressure on the client when dealing with very large local relations because the client does not have to materialize all chunks in memory. Limits the spark.sql.session.localRelationChunkSizeBytes, a minimum of the two confs is used to determine the chunk size.",
      "defaultValue": "1073741824"
    },
    {
      "key": "spark.sql.session.localRelationCacheThreshold",
      "doc": "The threshold for the size in bytes of local relations to be cached at the driver side after serialization.",
      "defaultValue": "1048576"
    },
    {
      "key": "spark.sql.session.localRelationChunkSizeBytes",
      "doc": "The chunk size in bytes when splitting ChunkedCachedLocalRelation.data into batches. A new chunk is created when either spark.sql.session.localRelationChunkSizeBytes or spark.sql.session.localRelationChunkSizeRows is reached. Limited by the spark.sql.session.localRelationBatchOfChunksSizeBytes, a minimum of the two confs is used to determine the chunk size.",
      "defaultValue": "16777216"
    },
    {
      "key": "spark.sql.session.localRelationChunkSizeLimit",
      "doc": "Limit on how large a single chunk of a ChunkedCachedLocalRelation.data can be in bytes. If the limit is exceeded, an exception is thrown.",
      "defaultValue": "2000MB"
    },
    {
      "key": "spark.sql.session.localRelationChunkSizeRows",
      "doc": "The chunk size in number of rows when splitting ChunkedCachedLocalRelation.data into batches. A new chunk is created when either spark.sql.session.localRelationChunkSizeBytes or spark.sql.session.localRelationChunkSizeRows is reached.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.session.localRelationSizeLimit",
      "doc": "Limit on how large ChunkedCachedLocalRelation.data can be in bytes.If the limit is exceeded, an exception is thrown.",
      "defaultValue": "3GB"
    },
    {
      "key": "spark.sql.session.timeZone",
      "doc": "The ID of session local timezone in the format of either region-based zone IDs or zone offsets. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names are not recommended to use because they can be ambiguous.",
      "defaultValue": null
    },
    {
      "key": "spark.sql.sessionWindow.buffer.in.memory.threshold",
      "doc": "Threshold for number of windows guaranteed to be held in memory by the session window operator. Note that the buffer is used only for the query Spark cannot apply aggregations on determining session window.",
      "defaultValue": "4096"
    },
    {
      "key": "spark.sql.sessionWindow.buffer.spill.size.threshold",
      "doc": "Threshold for size of rows to be spilled by window operator. Note that the buffer is used only for the query Spark cannot apply aggregations on determining session window.",
      "fallback": "spark.shuffle.spill.maxSizeInBytesForSpillThreshold"
    },
    {
      "key": "spark.sql.sessionWindow.buffer.spill.threshold",
      "doc": "Threshold for number of rows to be spilled by window operator. Note that the buffer is used only for the query Spark cannot apply aggregations on determining session window.",
      "defaultValue": "2147483647"
    },
    {
      "key": "spark.sql.shuffle.orderIndependentChecksum.enableFullRetryOnMismatch",
      "doc": "Whether to retry all tasks of a consumer stage when we detect checksum mismatches with its producer stages.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.shuffle.orderIndependentChecksum.enabled",
      "doc": "Whether to calculate order independent checksum for the shuffle data or not. If enabled, Spark will calculate a checksum that is independent of the input row order for each mapper and returns the checksums from executors to driver. This is different from the checksum computed when spark.shuffle.checksum.enabled is enabled which is sensitive to shuffle data ordering to detect file corruption. While this checksum will be the same even if the shuffle row order changes and it is used to detect whether different task attempts of the same partition produce different output data or not (same set of keyValue pairs). In case the output data has changed across retries, Spark will need to retry all tasks of the consumer stages to avoid correctness issues.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.shuffle.partitions",
      "doc": "The default number of partitions to use when shuffling data for joins or aggregations.",
      "defaultValue": "200"
    },
    {
      "key": "spark.sql.shuffleDependency.fileCleanup.enabled",
      "doc": "(Deprecated since Spark 4.1, please set 'spark.sql.connect.shuffleDependency.fileCleanup.enabled'.)",
      "defaultValue": "false",
      "deprecated": {
        "version": "4.1",
        "comment": "Use 'spark.sql.connect.shuffleDependency.fileCleanup.enabled' instead."
      }
    },
    {
      "key": "spark.sql.shuffleDependency.skipMigration.enabled",
      "doc": "When enabled, shuffle dependencies for a Spark Connect SQL execution are marked at the end of the execution, and they will not be migrated during decommissions.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.shuffleExchange.maxThreadThreshold",
      "doc": "The maximum degree of parallelism for doing preparation of shuffle exchange, which includes subquery execution, file listing, etc.",
      "defaultValue": "1024",
      "isStatic": true
    },
    {
      "key": "spark.sql.shuffledHashJoinFactor",
      "doc": "The shuffle hash join can be selected if the data size of small side multiplied by this factor is still smaller than the large side.",
      "defaultValue": "3"
    },
    {
      "key": "spark.sql.sort.enableRadixSort",
      "doc": "When true, enable use of radix sort when possible. Radix sort is much faster but requires additional memory to be reserved up-front. The memory overhead may be significant when sorting very small rows (up to 50% more in this case).",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.sortMergeJoinExec.buffer.in.memory.threshold",
      "doc": "Threshold for number of rows guaranteed to be held in memory by the sort merge join operator",
      "defaultValue": "2147483632"
    },
    {
      "key": "spark.sql.sortMergeJoinExec.buffer.spill.size.threshold",
      "doc": "Threshold for size of rows to be spilled by sort merge join operator",
      "fallback": "spark.shuffle.spill.maxSizeInBytesForSpillThreshold"
    },
    {
      "key": "spark.sql.sortMergeJoinExec.buffer.spill.threshold",
      "doc": "Threshold for number of rows to be spilled by sort merge join operator",
      "defaultValue": "2147483647"
    },
    {
      "key": "spark.sql.sources.binaryFile.maxLength",
      "doc": "The max length of a file that can be read by the binary file data source. Spark will fail fast and not attempt to read the file if its length exceeds this value. The theoretical max is Int.MaxValue, though VMs might implement a smaller max.",
      "defaultValue": "2147483647"
    },
    {
      "key": "spark.sql.sources.bucketing.autoBucketedScan.enabled",
      "doc": "When true, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not take any effect.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.sources.bucketing.enabled",
      "doc": "When false, we will treat bucketed table as normal table",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.sources.bucketing.maxBuckets",
      "doc": "The maximum number of buckets allowed.",
      "defaultValue": "100000"
    },
    {
      "key": "spark.sql.sources.commitProtocolClass",
      "doc": "",
      "defaultValue": "org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol"
    },
    {
      "key": "spark.sql.sources.default",
      "doc": "The default data source to use in input/output.",
      "defaultValue": "parquet"
    },
    {
      "key": "spark.sql.sources.disabledJdbcConnProviderList",
      "doc": "Configures a list of JDBC connection providers, which are disabled. The list contains the name of the JDBC connection providers separated by comma.",
      "defaultValue": "",
      "isStatic": true
    },
    {
      "key": "spark.sql.sources.fileCompressionFactor",
      "doc": "When estimating the output data size of a table scan, multiply the file size with this factor as the estimated data size, in case the data is compressed in the file and lead to a heavily underestimated result.",
      "defaultValue": "1.0"
    },
    {
      "key": "spark.sql.sources.ignoreDataLocality",
      "doc": "If true, Spark will not fetch the block locations for each file on listing files. This speeds up file listing, but the scheduler cannot schedule tasks to take advantage of data locality. It can be particularly useful if data is read from a remote cluster so the scheduler could never take advantage of locality anyway.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.outputCommitterClass",
      "doc": ""
    },
    {
      "key": "spark.sql.sources.parallelPartitionDiscovery.parallelism",
      "doc": "The number of parallelism to list a collection of path recursively, Set the number to prevent file listing from generating too many tasks.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.sources.parallelPartitionDiscovery.threshold",
      "doc": "The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.",
      "defaultValue": "32"
    },
    {
      "key": "spark.sql.sources.partitionColumnTypeInference.enabled",
      "doc": "When true, automatically infer the data types for partitioned columns.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.sources.partitionOverwriteMode",
      "doc": "When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path).",
      "defaultValue": "STATIC"
    },
    {
      "key": "spark.sql.sources.schemaStringLengthThreshold",
      "doc": "The maximum length allowed in a single cell when storing additional schema information in Hive's metastore.",
      "defaultValue": "4000",
      "isStatic": true,
      "deprecated": {
        "version": "3.2",
        "comment": "Use 'spark.sql.hive.tablePropertyLengthThreshold' instead."
      }
    },
    {
      "key": "spark.sql.sources.useListFilesFileSystemList",
      "doc": "A comma-separated list of file system schemes to use FileSystem.listFiles API for a single root path listing",
      "defaultValue": "s3a"
    },
    {
      "key": "spark.sql.sources.useV1SourceList",
      "doc": "A comma-separated list of data source short names or fully qualified data source implementation class names for which Data Source V2 code path is disabled. These data sources will fallback to Data Source V1 code path.",
      "defaultValue": "avro,csv,json,kafka,orc,parquet,text"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled",
      "doc": "Whether to allow storage-partition join in the case where the partition transforms are compatible but not identical.  This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled and spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled to be disabled.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled",
      "doc": "Whether to allow storage-partition join in the case where join keys are a subset of the partition keys of the source tables. At planning time, Spark will group the partitions by only those keys that are in the join keys. This is currently enabled only if spark.sql.requireAllClusterKeysForDistribution is false.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.enabled",
      "doc": "Similar to spark.sql.sources.bucketing.enabled, this config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled",
      "doc": "During a storage-partitioned join, whether to allow input partitions to be partially clustered, when both sides of the join are of KeyGroupedPartitioning. At planning time, Spark will pick the side with less data size based on table statistics, group and replicate them to match the other side. This is an optimization on skew join and can help to reduce data skewness when certain partitions are assigned large amount of data. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.partition.filter.enabled",
      "doc": "Whether to filter partitions when running storage-partition join. When enabled, partitions without matches on the other side can be omitted for scanning, if allowed by the join type. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.pushPartValues.enabled",
      "doc": "Whether to pushdown common partition values when spark.sql.sources.v2.bucketing.enabled is enabled. When turned on, if both sides of a join are of KeyGroupedPartitioning and if they share compatible partition keys, even if they don't have the exact same partition values, Spark will calculate a superset of partition values and pushdown that info to scan nodes, which will use empty partitions for the missing partition values on either side. This could help to eliminate unnecessary shuffles",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.shuffle.enabled",
      "doc": "During a storage-partitioned join, whether to allow to shuffle only one side. When only one side is KeyGroupedPartitioning, if the conditions are met, spark will only shuffle the other side. This optimization will reduce the amount of data that needs to be shuffle. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.v2.bucketing.sorting.enabled",
      "doc": "When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid a shuffle if possible when sorting by those columns. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.sources.validatePartitionColumns",
      "doc": "When this option is set to true, partition column values will be validated with user-specified schema. If the validation fails, a runtime exception is thrown. When this option is set to false, the partition column value will be converted to null if it can not be casted to corresponding user-specified schema.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.stableDerivedColumnAlias.enabled",
      "doc": "Enable deriving of stable column aliases from the lexer tree instead of parse tree and form them via pretty SQL print.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.stackTracesInDataFrameContext",
      "doc": "The number of non-Spark stack traces in the captured DataFrame query context.",
      "defaultValue": "1"
    },
    {
      "key": "spark.sql.statistics.fallBackToHdfs",
      "doc": "When true, it will fall back to HDFS if the table statistics are not available from table metadata. This is useful in determining if a table is small enough to use broadcast joins. This flag is effective only for non-partitioned Hive tables. For non-partitioned data source tables, it will be automatically recalculated if table statistics are not available. For partitioned data source and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.statistics.histogram.enabled",
      "doc": "Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.statistics.histogram.numBins",
      "doc": "The number of bins when generating histograms.",
      "defaultValue": "254"
    },
    {
      "key": "spark.sql.statistics.ndv.maxError",
      "doc": "The maximum relative standard deviation allowed in HyperLogLog++ algorithm when generating column level statistics.",
      "defaultValue": "0.05"
    },
    {
      "key": "spark.sql.statistics.parallelFileListingInStatsComputation.enabled",
      "doc": "When true, SQL commands use parallel file listing, as opposed to single thread listing. This usually speeds up commands that need to list many directories.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.statistics.percentile.accuracy",
      "doc": "Accuracy of percentile approximation when generating equi-height histograms. Larger value means better accuracy. The relative error can be deduced by 1.0 / PERCENTILE_ACCURACY.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.statistics.size.autoUpdate.enabled",
      "doc": "Enables automatic update for table size once table's data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.statistics.updatePartitionStatsInAnalyzeTable.enabled",
      "doc": "When this config is enabled, Spark will also update partition statistics in analyze table command (i.e., ANALYZE TABLE .. COMPUTE STATISTICS [NOSCAN]). Note the command will also become more expensive. When this config is disabled, Spark will only update table level statistics.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.storeAssignmentPolicy",
      "doc": "When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting `string` to `int` or `double` to `boolean`. With legacy policy, Spark allows the type coercion as long as it is a valid `Cast`, which is very loose. e.g. converting `string` to `int` or `double` to `boolean` is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting `double` to `int` or `decimal` to `double` is not allowed.",
      "defaultValue": "ANSI"
    },
    {
      "key": "spark.sql.streaming.aggregation.stateFormatVersion",
      "doc": "State format version used by streaming aggregation operations in a streaming query. State between versions are tend to be incompatible, so state format version shouldn't be modified after running.",
      "defaultValue": "2"
    },
    {
      "key": "spark.sql.streaming.asyncLogPurge.enabled",
      "doc": "When true, purging the offset log and commit log of old entries will be done asynchronously.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.checkpoint.escapedPathCheck.enabled",
      "doc": "Whether to detect a streaming query may pick up an incorrect checkpoint path due to SPARK-26824.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.checkpoint.fileChecksum.enabled",
      "doc": "When true, checksum would be generated and verified for checkpoint files. This is used to detect file corruption.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.checkpoint.fileChecksum.skipCreationIfFileMissingChecksum",
      "doc": "When true, if a microbatch is retried, if a file already exists but its checksum file does not exist, the file checksum will not be created. This is useful for compatibility with files created before file checksums were enabled.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.checkpoint.renamedFileCheck.enabled",
      "doc": "When true, Spark will validate if renamed checkpoint file exists.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.checkpointLocation",
      "doc": "The default location for storing checkpoint data for streaming queries."
    },
    {
      "key": "spark.sql.streaming.commitProtocolClass",
      "doc": "",
      "defaultValue": "org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol"
    },
    {
      "key": "spark.sql.streaming.continuous.epochBacklogQueueSize",
      "doc": "The max number of entries to be stored in queue to wait for late epochs. If this parameter is exceeded by the size of the queue, stream will stop with an error.",
      "defaultValue": "10000"
    },
    {
      "key": "spark.sql.streaming.continuous.executorPollIntervalMs",
      "doc": "The interval at which continuous execution readers will poll to check whether the epoch has advanced on the driver.",
      "defaultValue": "100ms"
    },
    {
      "key": "spark.sql.streaming.continuous.executorQueueSize",
      "doc": "The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader.",
      "defaultValue": "1024"
    },
    {
      "key": "spark.sql.streaming.disabledV2MicroBatchReaders",
      "doc": "A comma-separated list of fully qualified data source register class names for which MicroBatchReadSupport is disabled. Reads from these sources will fall back to the V1 Sources.",
      "defaultValue": ""
    },
    {
      "key": "spark.sql.streaming.disabledV2Writers",
      "doc": "A comma-separated list of fully qualified data source register class names for which StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.",
      "defaultValue": ""
    },
    {
      "key": "spark.sql.streaming.fileSink.log.cleanupDelay",
      "doc": "How long that a file is guaranteed to be visible for all readers.",
      "defaultValue": "600000ms"
    },
    {
      "key": "spark.sql.streaming.fileSink.log.compactInterval",
      "doc": "Number of log files after which all the previous files are compacted into the next log file.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.streaming.fileSink.log.deletion",
      "doc": "Whether to delete the expired log files in file stream sink.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.fileSource.cleaner.numThreads",
      "doc": "Number of threads used in the file source completed file cleaner.",
      "defaultValue": "1"
    },
    {
      "key": "spark.sql.streaming.fileSource.log.cleanupDelay",
      "doc": "How long in milliseconds a file is guaranteed to be visible for all readers.",
      "defaultValue": "600000ms"
    },
    {
      "key": "spark.sql.streaming.fileSource.log.compactInterval",
      "doc": "Number of log files after which all the previous files are compacted into the next log file.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.streaming.fileSource.log.deletion",
      "doc": "Whether to delete the expired log files in file stream source.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.fileSource.schema.forceNullable",
      "doc": "When true, force the schema of streaming file source to be nullable (including all the fields). Otherwise, the schema might not be compatible with actual data, which leads to corruptions.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.fileStreamSink.ignoreMetadata",
      "doc": "If this is enabled, when Spark reads from the results of a streaming query written by `FileStreamSink`, Spark will ignore the metadata log and treat it as normal path to read, e.g. listing files using HDFS APIs.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.flatMapGroupsWithState.skipEmittingInitialStateKeys",
      "doc": "When true, the flatMapGroupsWithState operation in a streaming query will not emit results for the initial state keys of each group.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion",
      "doc": "State format version used by flatMapGroupsWithState operation in a streaming query",
      "defaultValue": "2"
    },
    {
      "key": "spark.sql.streaming.forceDeleteTempCheckpointLocation",
      "doc": "When true, enable temporary checkpoint locations force delete.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.internal.stateStore.partitions",
      "doc": "WARN: This config is used internally and is not intended to be user-facing. This config can be removed without support of compatibility in any time. DO NOT USE THIS CONFIG DIRECTLY AND USE THE CONFIG `spark.sql.shuffle.partitions`. The default number of partitions to use when shuffling data for stateful operations. If not specified, this config picks up the value of `spark.sql.shuffle.partitions`. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location."
    },
    {
      "key": "spark.sql.streaming.join.stateFormatVersion",
      "doc": "State format version used by streaming join operations in a streaming query. State between versions are tend to be incompatible, so state format version shouldn't be modified after running. Version 3 uses a single state store with virtual column families instead of four stores and is only supported with RocksDB.",
      "defaultValue": "2"
    },
    {
      "key": "spark.sql.streaming.kafka.useDeprecatedOffsetFetching",
      "doc": "When true, the deprecated Consumer based offset fetching used which could cause infinite wait in Spark queries. Such cases query restart is the only workaround. For further details please see Offset Fetching chapter of Structured Streaming Kafka Integration Guide.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.maxBatchesToRetainInMemory",
      "doc": "The maximum number of batches which will be retained in memory to avoid loading from files. The value adjusts a trade-off between memory usage vs cache miss: '2' covers both success and direct failure cases, '1' covers only success case, and '0' covers extreme case - disable cache to maximize memory size of executors.",
      "defaultValue": "2"
    },
    {
      "key": "spark.sql.streaming.metadataCache.enabled",
      "doc": "Whether the streaming HDFSMetadataLog caches the metadata of the latest two batches.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.metricsEnabled",
      "doc": "Whether Dropwizard/Codahale metrics will be reported for active streaming queries.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.minBatchesToRetain",
      "doc": "The minimum number of batches that must be retained and made recoverable.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.streaming.multipleWatermarkPolicy",
      "doc": "Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is 'min' which chooses the minimum watermark reported across multiple operators. Other alternative value is 'max' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint location.",
      "defaultValue": "min"
    },
    {
      "key": "spark.sql.streaming.noDataMicroBatches.enabled",
      "doc": "Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.noDataProgressEventInterval",
      "doc": "How long to wait before providing query idle event when there is no data",
      "defaultValue": "10000ms"
    },
    {
      "key": "spark.sql.streaming.numRecentProgressUpdates",
      "doc": "The number of progress updates to retain for a streaming query",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.streaming.optimizeOneRowPlan.enabled",
      "doc": "When true, enable OptimizeOneRowPlan rule for the case where the child is a streaming Dataset. This is a fallback flag to revert the 'incorrect' behavior, hence this configuration must not be used without understanding in depth. Use this only to quickly recover failure in existing query!",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.pollingDelay",
      "doc": "How long to delay polling new data when no data is available",
      "defaultValue": "10ms"
    },
    {
      "key": "spark.sql.streaming.ratioExtraSpaceAllowedInCheckpoint",
      "doc": "The ratio of extra space allowed for batch deletion of files when maintenance isinvoked. When value > 0, it optimizes the cost of discovering and deleting old checkpoint versions. The minimum number of stale versions we retain in checkpoint location for batch deletion is calculated by minBatchesToRetain * ratioExtraSpaceAllowedInCheckpoint.",
      "defaultValue": "0.3"
    },
    {
      "key": "spark.sql.streaming.realTimeMode.allowlistCheck",
      "doc": "Whether to check all operators, sinks used in real-time mode are in the allowlist.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.realTimeMode.minBatchDuration",
      "doc": "The minimum long-running batch duration in milliseconds for real-time mode.",
      "defaultValue": "5000ms"
    },
    {
      "key": "spark.sql.streaming.schemaInference",
      "doc": "Whether file-based streaming sources will infer its own schema",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition",
      "doc": "When true, streaming session window sorts and merge sessions in local partition prior to shuffle. This is to reduce the rows to shuffle, but only beneficial when there're lots of rows in a batch being assigned to same sessions.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.sessionWindow.stateFormatVersion",
      "doc": "State format version used by streaming session window in a streaming query. State between versions are tend to be incompatible, so state format version shouldn't be modified after running.",
      "defaultValue": "1"
    },
    {
      "key": "spark.sql.streaming.stateStore.checkpointFormatVersion",
      "doc": "The version of the approach of doing state store checkpoint",
      "defaultValue": "1"
    },
    {
      "key": "spark.sql.streaming.stateStore.commitValidation.enabled",
      "doc": "When true, Spark will validate that all StateStore instances have committed for stateful streaming queries using foreachBatch. This helps detect cases where user-defined functions in foreachBatch (e.g., show(), limit()) don't process all partitions, which can lead to incorrect results. The validation only applies to foreachBatch sinks without global aggregates or limits.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.stateStore.compression.codec",
      "doc": "The codec used to compress delta and snapshot files generated by StateStore. By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify the codec. Default codec is lz4.",
      "defaultValue": "lz4"
    },
    {
      "key": "spark.sql.streaming.stateStore.coordinatorReportSnapshotUploadLag",
      "doc": "When enabled, the state store coordinator will report state stores whose snapshot have not been uploaded for some time. See the conf snapshotLagReportInterval for the minimum time between reports, and the conf multiplierForMinVersionDiffToLog and multiplierForMinTimeDiffToLog for the logging thresholds.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.stateStore.encodingFormat",
      "doc": "The encoding format used for stateful operators to store information in the state store",
      "defaultValue": "unsaferow"
    },
    {
      "key": "spark.sql.streaming.stateStore.formatValidation.enabled",
      "doc": "When true, check if the data from state store is valid or not when running streaming queries. This can happen if the state store format has been changed. Note, the feature is only effective in the build-in HDFS state store provider now.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.stateStore.maintenanceInterval",
      "doc": "The interval in milliseconds between triggering maintenance tasks in StateStore. The maintenance task executes background maintenance task in all the loaded store providers if they are still the active instances according to the coordinator. If not, inactive instances of store providers will be closed.",
      "defaultValue": "60000ms"
    },
    {
      "key": "spark.sql.streaming.stateStore.maintenanceProcessingTimeout",
      "doc": "Timeout in seconds to wait for maintenance to process this partition.",
      "defaultValue": "30000ms"
    },
    {
      "key": "spark.sql.streaming.stateStore.maintenanceShutdownTimeout",
      "doc": "Timeout in seconds for maintenance pool operations to complete on shutdown",
      "defaultValue": "300000ms"
    },
    {
      "key": "spark.sql.streaming.stateStore.maxLaggingStoresToReport",
      "doc": "Maximum number of state stores the coordinator will report as trailing in snapshot uploads. Stores are selected based on the most lagging behind in snapshot version.",
      "defaultValue": "5"
    },
    {
      "key": "spark.sql.streaming.stateStore.maxNumStateSchemaFiles",
      "doc": "The maximum number of StateSchemaV3 files allowed per operator",
      "defaultValue": "128"
    },
    {
      "key": "spark.sql.streaming.stateStore.maxVersionsToDeletePerMaintenance",
      "doc": "The maximum number of versions to delete per maintenance operation. By default, this value is set to -1, which means no limit. Note that, currently this is only supported for the RocksDB state store provider.",
      "defaultValue": "-1"
    },
    {
      "key": "spark.sql.streaming.stateStore.minDeltasForSnapshot",
      "doc": "Minimum number of state store delta files that needs to be generated before they consolidated into snapshots.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.streaming.stateStore.multiplierForMinTimeDiffToLog",
      "doc": "Determines the time threshold for logging warnings when a state store falls behind. The coordinator logs a warning when the store's uploaded snapshot timestamp trails the current time by the configured maintenance interval, times this multiplier.",
      "defaultValue": "10"
    },
    {
      "key": "spark.sql.streaming.stateStore.multiplierForMinVersionDiffToLog",
      "doc": "Determines the version threshold for logging warnings when a state store falls behind. The coordinator logs a warning when the store's uploaded snapshot version trails the query's latest version by the configured number of deltas needed to create a snapshot, times this multiplier.",
      "defaultValue": "5"
    },
    {
      "key": "spark.sql.streaming.stateStore.numStateStoreInstanceMetricsToReport",
      "doc": "Number of state store instance metrics included in streaming query progress messages per stateful operator. Instance metrics are selected based on metric-specific ordering to minimize noise in the progress report.",
      "defaultValue": "5"
    },
    {
      "key": "spark.sql.streaming.stateStore.numStateStoreMaintenanceThreads",
      "doc": "Number of threads in the thread pool that perform clean up and snapshotting tasks for stateful streaming queries. The default value is the number of cores * 0.25 so that this thread pool doesn't take too many resources away from the query and affect performance.",
      "defaultValue": "2"
    },
    {
      "key": "spark.sql.streaming.stateStore.providerClass",
      "doc": "The class used to manage state data in stateful streaming queries. This class must be a subclass of StateStoreProvider, and must have a zero-arg constructor. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location.",
      "defaultValue": "org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider"
    },
    {
      "key": "spark.sql.streaming.stateStore.rocksdb.formatVersion",
      "doc": "Set the RocksDB format version. This will be stored in the checkpoint when starting a streaming query. The checkpoint will use this RocksDB format version in the entire lifetime of the query.",
      "defaultValue": "5"
    },
    {
      "key": "spark.sql.streaming.stateStore.skipNullsForStreamStreamJoins.enabled",
      "doc": "When true, this config will skip null values in hash based stream-stream joins. The number of skipped null values will be shown as custom metric of stream join operator. If the streaming query was started with Spark 3.5 or above, please exercise caution before enabling this config since it may hide potential data loss/corruption issues.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.stateStore.snapshotLagReportInterval",
      "doc": "The minimum amount of time between the state store coordinator's reports on state store instances trailing behind in snapshot uploads.",
      "defaultValue": "300000ms"
    },
    {
      "key": "spark.sql.streaming.stateStore.stateSchemaCheck",
      "doc": "When true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.stateStore.unloadOnCommit",
      "doc": "When true, Spark will synchronously run maintenance and then close each StateStore instance on task completion. This removes the overhead of keeping every StateStore loaded indefinitely, at the cost of having to reload each StateStore every batch. Stateful applications that are failing due to resource exhaustion or that use dynamic allocation may benefit from enabling this.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.stateStore.valueStateSchemaEvolutionThreshold",
      "doc": "The maximum number of value state schema evolutions allowed per column family",
      "defaultValue": "16"
    },
    {
      "key": "spark.sql.streaming.statefulOperator.allowMultiple",
      "doc": "When true, multiple stateful operators are allowed to be present in a streaming pipeline. The support for multiple stateful operators introduces a minor (semantically correct) change in respect to late record filtering - late records are detected and filtered in respect to the watermark from the previous microbatch instead of the current one. This is a behavior change for Spark streaming pipelines and we allow users to revert to the previous behavior of late record filtering (late records are detected and filtered by comparing with the current microbatch watermark) by setting the flag value to false. In this mode, only a single stateful operator will be allowed in a streaming pipeline.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.statefulOperator.checkCorrectness.enabled",
      "doc": "When true, the stateful operators for streaming query will be checked for possible correctness issue due to global watermark. The correctness issue comes from queries containing stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. Once the issue is detected, Spark will throw analysis exception. When this config is disabled, Spark will just print warning message for users. Prior to Spark 3.1.0, the behavior is disabling this config.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.statefulOperator.useStrictDistribution",
      "doc": "The purpose of this config is only compatibility; DO NOT MANUALLY CHANGE THIS!!! When true, the stateful operator for streaming query will use StatefulOpClusteredDistribution which guarantees stable state partitioning as long as the operator provides consistent grouping keys across the lifetime of query. When false, the stateful operator for streaming query will use ClusteredDistribution which is not sufficient to guarantee stable state partitioning despite the operator provides consistent grouping keys across the lifetime of query. This config will be set to true for new streaming queries to guarantee stable state partitioning, and set to false for existing streaming queries to not break queries which are restored from existing checkpoints. Please refer SPARK-38204 for details.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.stopActiveRunOnRestart",
      "doc": "Running multiple runs of the same streaming query concurrently is not supported. If we find a concurrent active run for a streaming query (in the same or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.stopTimeout",
      "doc": "How long to wait in milliseconds for the streaming execution thread to stop when calling the streaming query's stop() method. 0 or negative values wait indefinitely.",
      "defaultValue": "0"
    },
    {
      "key": "spark.sql.streaming.streamingQueryListeners",
      "doc": "List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.",
      "isStatic": true
    },
    {
      "key": "spark.sql.streaming.transformWithState.stateSchemaVersion",
      "doc": "The version of the state schema used by the transformWithState operator",
      "defaultValue": "3"
    },
    {
      "key": "spark.sql.streaming.triggerAvailableNowWrapper.enabled",
      "doc": "Whether to use the wrapper implementation of Trigger.AvailableNow if the source does not support Trigger.AvailableNow. Enabling this allows the benefits of Trigger.AvailableNow with sources which don't support it, but some sources may show unexpected behavior including duplication, data loss, etc. So use with extreme care! The ideal direction is to persuade developers of source(s) to support Trigger.AvailableNow.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.streaming.ui.enabled",
      "doc": "Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.",
      "defaultValue": "true",
      "isStatic": true
    },
    {
      "key": "spark.sql.streaming.ui.enabledCustomMetricList",
      "doc": "Configures a list of custom metrics on Structured Streaming UI, which are enabled. The list contains the name of the custom metrics separated by comma. In aggregation only sum used. The list of supported custom metrics is state store provider specific and it can be found out for example from query progress log entry.",
      "defaultValue": "",
      "isStatic": true
    },
    {
      "key": "spark.sql.streaming.ui.retainedProgressUpdates",
      "doc": "The number of progress updates to retain for a streaming query for Structured Streaming UI.",
      "defaultValue": "100",
      "isStatic": true
    },
    {
      "key": "spark.sql.streaming.ui.retainedQueries",
      "doc": "The number of inactive queries to retain for Structured Streaming UI.",
      "defaultValue": "100",
      "isStatic": true
    },
    {
      "key": "spark.sql.streaming.unsupportedOperationCheck",
      "doc": "When true, the logical plan for streaming query will be checked for unsupported operations.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.streaming.verifyCheckpointDirectoryEmptyOnStart",
      "doc": "When true, verifies that the checkpoint directory (offsets, state, commits) is empty when first starting a streaming query. This prevents prevents sharing checkpoint directories between different queries.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.subexpressionElimination.cache.maxEntries",
      "doc": "The maximum entries of the cache used for interpreted subexpression elimination.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.subexpressionElimination.enabled",
      "doc": "When true, common subexpressions will be eliminated.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.subexpressionElimination.skipForShortcutExpr",
      "doc": "When true, shortcut eliminate subexpression with `AND`, `OR`. The subexpression may not need to eval even if it appears more than once. e.g., `if(or(a, and(b, b)))`, the expression `b` would be skipped if `a` is true.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.subquery.maxThreadThreshold",
      "doc": "The maximum degree of parallelism to execute the subquery.",
      "defaultValue": "16",
      "isStatic": true
    },
    {
      "key": "spark.sql.thriftServer.incrementalCollect",
      "doc": "When true, enable incremental collection for execution in Thrift Server.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.thriftServer.interruptOnCancel",
      "doc": "When true, all running tasks will be interrupted if one cancels a query. When false, all running tasks will remain until finished.",
      "fallback": "spark.sql.execution.interruptOnCancel"
    },
    {
      "key": "spark.sql.thriftServer.queryTimeout",
      "doc": "Set a query duration timeout in seconds in Thrift Server. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. If timeout values are set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller than this configuration value, they take precedence. If you set this timeout and prefer to cancel the queries right away without waiting task to finish, consider enabling spark.sql.thriftServer.interruptOnCancel together.",
      "defaultValue": "0ms"
    },
    {
      "key": "spark.sql.thriftserver.scheduler.pool",
      "doc": "Set a Fair Scheduler pool for a JDBC client session."
    },
    {
      "key": "spark.sql.thriftserver.ui.retainedSessions",
      "doc": "The number of SQL client sessions kept in the JDBC/ODBC web UI history.",
      "defaultValue": "200"
    },
    {
      "key": "spark.sql.thriftserver.ui.retainedStatements",
      "doc": "The number of SQL statements kept in the JDBC/ODBC web UI history.",
      "defaultValue": "200"
    },
    {
      "key": "spark.sql.timeTravelTimestampKey",
      "doc": "The option name to specify the time travel timestamp when reading a table.",
      "defaultValue": "timestampAsOf"
    },
    {
      "key": "spark.sql.timeTravelVersionKey",
      "doc": "The option name to specify the time travel table version when reading a table.",
      "defaultValue": "versionAsOf"
    },
    {
      "key": "spark.sql.timeType.enabled",
      "doc": "When true, the TIME data type is supported.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.timestampType",
      "doc": "Configures the default timestamp type of Spark SQL, including SQL DDL, Cast clause, type literal and the schema inference of data sources. Setting the configuration as TIMESTAMP_NTZ will use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as TIMESTAMP_LTZ will use TIMESTAMP WITH LOCAL TIME ZONE. Before the 3.4.0 release, Spark only supports the TIMESTAMP WITH LOCAL TIME ZONE type.",
      "defaultValue": "TIMESTAMP_LTZ"
    },
    {
      "key": "spark.sql.transposeMaxValues",
      "doc": "When doing a transpose without specifying values for the index column this is the maximum number of values that will be transposed without error.",
      "defaultValue": "500"
    },
    {
      "key": "spark.sql.truncateTable.ignorePermissionAcl.enabled",
      "doc": "When set to true, TRUNCATE TABLE command will not try to set back original permission and ACLs when re-creating the table/partition paths.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.tvf.allowMultipleTableArguments.enabled",
      "doc": "When true, allows multiple table arguments for table-valued functions, receiving the cartesian product of all the rows of these tables.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.ui.explainMode",
      "doc": "Configures the query explain mode used in the Spark SQL UI. The value can be 'simple', 'extended', 'codegen', 'cost', or 'formatted'. The default value is 'formatted'.",
      "defaultValue": "formatted"
    },
    {
      "key": "spark.sql.ui.retainedExecutions",
      "doc": "Number of executions to retain in the Spark UI.",
      "defaultValue": "1000",
      "isStatic": true
    },
    {
      "key": "spark.sql.unionOutputPartitioning",
      "doc": "When set to true, the output partitioning of UnionExec will be the same as the input partitioning if its children have same partitioning. Otherwise, it will be a default partitioning.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.useCommonExprIdForAlias",
      "doc": "When true, use the common expression ID for the alias when rewriting With expressions. Otherwise, use the index of the common expression definition. When true this avoids duplicate alias names, but is helpful to set to false for testing to ensure that alias names are consistent.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.variable.substitute",
      "doc": "This enables substitution using syntax like `${var}`, `${system:var}`, and `${env:var}`.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.variant.allowDuplicateKeys",
      "doc": "When set to false, parsing variant from JSON will throw an error if there are duplicate keys in the input JSON object. When set to true, the parser will keep the last occurrence of all fields with the same key.",
      "defaultValue": "false"
    },
    {
      "key": "spark.sql.variant.allowReadingShredded",
      "doc": "When true, the Parquet reader is allowed to read shredded or unshredded variant. When false, it only reads unshredded variant.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.variant.forceShreddingSchemaForTest",
      "doc": "FOR INTERNAL TESTING ONLY. Sets shredding schema for Variant.",
      "defaultValue": ""
    },
    {
      "key": "spark.sql.variant.inferShreddingSchema",
      "doc": "Infer shredding schema when writing Variant columns in Parquet tables.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.variant.pushVariantIntoScan",
      "doc": "When true, replace variant type in the scan schema with a struct containing requested fields.",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.variant.shredding.maxSchemaDepth",
      "doc": "Maximum depth in Variant value to traverse when inferring a schema. Any array/object below this depth will be shredded as a single binary.",
      "defaultValue": "50"
    },
    {
      "key": "spark.sql.variant.shredding.maxSchemaWidth",
      "doc": "Maximum number of shredded fields to create when inferring a schema for Variant",
      "defaultValue": "300"
    },
    {
      "key": "spark.sql.variant.writeShredding.enabled",
      "doc": "When true, the Parquet writer is allowed to write shredded variant. ",
      "defaultValue": "true"
    },
    {
      "key": "spark.sql.view.maxNestedViewDepth",
      "doc": "The maximum depth of a view reference in a nested view. A nested view may reference other nested views, the dependencies are organized in a directed acyclic graph (DAG). However the DAG depth may become too large and cause unexpected behavior. This configuration puts a limit on this: when the depth of a view exceeds this value during analysis, we terminate the resolution to avoid potential errors.",
      "defaultValue": "100"
    },
    {
      "key": "spark.sql.warehouse.dir",
      "doc": "The default location for managed databases and tables.",
      "defaultValue": null,
      "isStatic": true
    },
    {
      "key": "spark.sql.windowExec.buffer.in.memory.threshold",
      "doc": "Threshold for number of rows guaranteed to be held in memory by the window operator",
      "defaultValue": "4096"
    },
    {
      "key": "spark.sql.windowExec.buffer.spill.size.threshold",
      "doc": "Threshold for size of rows to be spilled by window operator",
      "fallback": "spark.shuffle.spill.maxSizeInBytesForSpillThreshold"
    },
    {
      "key": "spark.sql.windowExec.buffer.spill.threshold",
      "doc": "Threshold for number of rows to be spilled by window operator",
      "defaultValue": "2147483647"
    }
  ]
}
