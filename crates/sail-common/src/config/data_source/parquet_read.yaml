# References:
#   - [1] https://spark.apache.org/docs/4.0.0/sql-data-sources-parquet.html#data-source-option
#   - [2] https://github.com/apache/spark/blob/b0c2ba357bf080dd328b95e4a6402b134a641a1a/python/pyspark/sql/connect/readwriter.py#L207-L214
#   - [3] TODO: Spark global options: https://spark.apache.org/docs/4.0.0/sql-data-sources-parquet.html#configuration

- key: enable_page_index
  alias:
    - enablePageIndex
  default: "true"
  description: |
    (Reading) Whether to enable page index when reading Parquet files.
    If the value is `true`, the Parquet reader reads the page index if present.
    This can reduce I/O and the number of rows decoded.
  supported: true

- key: pruning
  default: "true"
  description: |
    (Reading) Whether to prune row groups when reading Parquet files.
    If the value is `true`, the Parquet reader attempts to skip entire row groups based
    on the predicate in the query and the metadata (minimum and maximum values) stored in
    the Parquet file.
  supported: true

- key: skip_metadata
  alias:
    - skipMetadata
  default: "true"
  description: |
    (Reading) Whether to skip the metadata when reading Parquet files.
    If the value is `true`, the Parquet reader skip the optional embedded metadata that may be in
    the file schema. This can help avoid schema conflicts when querying
    multiple Parquet files with schemas containing compatible types but different metadata.
  supported: true

- key: metadata_size_hint
  alias:
    - metadataSizeHint
  default: "0"
  description: |
    (Reading) The metadata size hint in bytes when reading Parquet files.
    If the value `n` is greater than `8`, the Parquet reader will try and fetch the last `n`
    bytes of the Parquet file optimistically. Otherwise, two reads are performed to fetch the
    metadata. The first read fetches the 8-byte Parquet footer and the second read fetches
    the metadata length encoded in the footer.
  supported: true

- key: pushdown_filters
  alias:
    - pushdownFilters
  default: "false"
  description: |
    (Reading) Whether to push down filter expressions when reading Parquet files.
    If the value is `true`, the Parquet reader applies filter expressions in decoding operations to
    reduce the number of rows decoded. This optimization is sometimes called "late materialization".
  supported: true

- key: reorder_filters
  alias:
    - reorderFilters
  default: "false"
  description: |
    (Reading) Whether to reorder filter expressions when reading Parquet files.
    If the value is `true`, the Parquet reader reorders filter expressions heuristically in decoding operations to
    minimize the cost of evaluation. If the value is `false`, the filters are applied in the same order as written in the query.
  supported: true

- key: schema_force_view_types
  alias:
    - schemaForceViewTypes
  default: "true"
  description: |
    (Reading) Whether to force view types for binary and string columns when reading Parquet files.
    If the value is `true`, the Parquet reader will read columns of the `Utf8` or `Utf8Large` types as the `Utf8View` type,
    and the `Binary` or `BinaryLarge` types as the `BinaryView` type.
  supported: true

- key: binary_as_string
  alias:
    - binaryAsString
  default: "false"
  description: |
    (Reading) Whether to read binary columns as string columns when reading Parquet files.
    If the value is `true`, the Parquet reader will read columns of
    the `Binary` or `LargeBinary` as the `Utf8` type, and the `BinaryView` type as the `Utf8View` type.
    This is helpful when reading Parquet files generated by some legacy writers, which do not correctly set
    the UTF-8 flag for strings, causing string columns to be loaded as binary columns by default.
  supported: true

- key: coerce_int96
  alias:
    - coerceInt96
  default: "us"
  description: |
    (Reading) Controls how the parquet reader interprets columns with physical type int96.
    By default, int96 columns are treated as microsecond ("us") resolution timestamps.
    This setting is useful for reading data from systems like Apache Spark, which encode microsecond timestamps
    in int96, enabling a wider date range to be written than with 64-bit timestamps at nanosecond resolution.
    Possible options are:
      - `ns`: interpret as nanoseconds,
      - `us`: interpret as microseconds,
      - `ms`: interpret as milliseconds,
      - `s`: interpret as seconds.
  supported: true

- key: bloom_filter_on_read
  alias:
    - bloomFilterOnRead
  default: "true"
  description: |
    (Reading) Whether to use available bloom filters when reading Parquet files.
  supported: true

- key: merge_schema
  alias:
    - mergeSchema
  default: ""
  description: |
    (Reading) Sets whether we should merge schemas collected from all Parquet part-files.
    This will override `sail.sql.parquet.mergeSchema`.
    Defaults to the value of the `sail.sql.parquet.mergeSchema` configuration.
  supported: false

- key: int96_rebase_mode
  alias:
    - int96RebaseMode
  default: ""
  description: |
    (Reading) The int96RebaseMode option allows to specify the rebasing mode for INT96 timestamps
    from the Julian to Proleptic Gregorian calendar.
    Currently supported modes are:
      - `EXCEPTION`: Fails in reads of ancient INT96 timestamps that are ambiguous between the two calendars.
      - `CORRECTED`: Loads INT96 timestamps without rebasing.
      - `LEGACY`: Performs rebasing of ancient timestamps from the Julian to Proleptic Gregorian calendar.
    Defaults to the value of the `sail.sql.parquet.int96RebaseModeInRead` configuration.
  supported: false

- key: datetime_rebase_mode
  alias:
    - datetimeRebaseMode
  default: ""
  description: |
    (Reading) The datetime_rebase_mode option allows to specify the rebasing mode for the values of the
    `DATE`, `TIMESTAMP_MILLIS`, `TIMESTAMP_MICROS` logical types from the Julian to Proleptic Gregorian calendar.
    Currently supported modes are:
      - `EXCEPTION`: Fails in reads of ancient dates/timestamps that are ambiguous between the two calendars.
      - `CORRECTED`: Loads dates/timestamps without rebasing.
      - `LEGACY`: Performs rebasing of ancient dates/timestamps from the Julian to Proleptic Gregorian calendar.
    Defaults to the value of the `sail.sql.parquet.datetimeRebaseModeInRead` configuration.
  supported: false