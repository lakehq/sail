- key: mode
  type: string
  default: "local"
  description: |
    The running mode for Sail. Valid values are `local`, `local-cluster`, and `kubernetes-cluster`.
    
    * In `local` mode, Sail runs in a single process, while query execution is still parallelized via threads.
    * In `local-cluster` mode, Sail starts a cluster within a single process. The driver and workers run on different threads
      in the same process and communicate with each other via RPC.
    * In `kubernetes-cluster` mode, Sail manages a cluster in Kubernetes. The driver and workers run in separate
      pods and communicate with each other via RPC.

- key: runtime.stack_size
  type: number
  default: "8388608"
  description: The stack size in bytes for each thread.

- key: runtime.enable_secondary
  type: boolean
  default: "true"
  description: |
    Use a secondary Tokio runtime to separate object storage I/O tasks from other operations.
  experimental: true

- key: cluster.enable_tls
  type: boolean
  default: "false"
  description: Whether to enable TLS for cluster communication.
  experimental: true

- key: cluster.driver_listen_host
  type: string
  default: "127.0.0.1"
  description: The host on which the driver listens.

- key: cluster.driver_listen_port
  type: number
  default: "0"
  description: |
    The port on which the driver listens.
    If the value is `0`, a random port is assigned by the operating system.

- key: cluster.driver_external_host
  type: string
  default: "127.0.0.1"
  description: The external host for the worker to connect to the driver.

- key: cluster.driver_external_port
  type: number
  default: "0"
  description: |
    The external port for the worker to connect to the driver.
    If the value is `0`, the port is assumed to be the same as the port
    on which the driver listens.

- key: cluster.worker_id
  type: number
  default: "0"
  description:
  hidden: true

- key: cluster.worker_listen_host
  type: string
  default: "127.0.0.1"
  description: The host on which the worker listens.

- key: cluster.worker_listen_port
  type: number
  default: "0"
  description: |
    The port on which the worker listens.
    If the value is `0`, a random port is assigned by the operating system.

- key: cluster.worker_external_host
  type: string
  default: "127.0.0.1"
  description: The external host for other workers to connect to the worker.

- key: cluster.worker_external_port
  type: number
  default: "0"
  description: |
    The external port for other workers to connect to the worker.
    If the value is `0`, the port is assumed to be the same as the port
    on which the worker listens.

- key: cluster.worker_initial_count
  type: number
  default: "4"
  description: The initial number of workers to launch.

- key: cluster.worker_max_count
  type: number
  default: "0"
  description: The maximum number of workers that can be launched.

- key: cluster.worker_max_idle_time_secs
  type: number
  default: "60"
  description: The maximum idle time in seconds before a worker is removed.

- key: cluster.worker_heartbeat_interval_secs
  type: number
  default: "10"
  description: The interval in seconds for worker heartbeats.
  experimental: true

- key: cluster.worker_heartbeat_timeout_secs
  type: number
  default: "120"
  description: The timeout in seconds for worker heartbeats.
  experimental: true

- key: cluster.worker_launch_timeout_secs
  type: number
  default: "120"
  description: The timeout in seconds for launching a worker.

- key: cluster.worker_task_slots
  type: number
  default: "8"
  description: The maximum number of tasks that can be launched on a worker.

- key: cluster.worker_stream_buffer
  type: number
  default: "16"
  description: The number of batches to buffer in the worker shuffle stream.
  experimental: true

- key: cluster.task_launch_timeout_secs
  type: number
  default: "120"
  description: The timeout in seconds for launching a task.

- key: cluster.job_output_buffer
  type: number
  default: "16"
  description: The number of batches to buffer in the job output stream.
  experimental: true

- key: cluster.rpc_retry_strategy.type
  type: string
  default: "fixed"
  description: |
    The retry strategy for driver and worker RPC requests.
    Valid values are `fixed` and `exponential_backoff`.
  experimental: true

- key: cluster.rpc_retry_strategy.fixed.max_count
  type: number
  default: "3"
  description: |
    The maximum number of retries for RPC requests
    when using the `fixed` retry strategy.
  experimental: true

- key: cluster.rpc_retry_strategy.fixed.delay_secs
  type: number
  default: "5"
  description: |
    The delay in seconds between retries for RPC requests
    when using the `fixed` retry strategy.
  experimental: true

- key: cluster.rpc_retry_strategy.exponential_backoff.max_count
  type: number
  default: "3"
  description: |
    The maximum number of retries for RPC requests
    when using the `exponential_backoff` retry strategy.
  experimental: true

- key: cluster.rpc_retry_strategy.exponential_backoff.initial_delay_secs
  type: number
  default: "1"
  description: |
    The initial delay in seconds between retries for RPC requests
    when using the `exponential_backoff` retry strategy.
  experimental: true

- key: cluster.rpc_retry_strategy.exponential_backoff.max_delay_secs
  type: number
  default: "5"
  description: |
    The maximum delay in seconds between retries for RPC requests
    when using the `exponential_backoff` retry strategy.
  experimental: true

- key: cluster.rpc_retry_strategy.exponential_backoff.factor
  type: number
  default: "2"
  description: |
    The factor by which the delay increases after each retry
    when using the `exponential_backoff` retry strategy.
  experimental: true

- key: execution.batch_size
  type: number
  default: "8192"
  description: The batch size for physical plan execution.
  experimental: true

- key: execution.collect_statistics
  type: boolean
  default: "true"
  description: |
    Should statistics be collected when first creating a table.
    This can slow down the initial DataFrame creation while greatly accelerating queries with certain filters.
    Has no effect after the table is created.
  experimental: true

- key: execution.use_row_number_estimates_to_optimize_partitioning
  type: boolean
  default: "false"
  description: |
    Should Sail use row number estimates at the input to decide whether increasing parallelism is beneficial or not.
    By default, only exact row numbers (not estimates) are used for this decision.
  experimental: true

- key: execution.file_listing_cache.type
  type: string
  default: "none"
  description: |
    The type of cache for file metadata when listing files.
    The cache avoids repeatedly listing file metadata,
    which may be expensive in certain situations (e.g., when using remote object storage).
    When the cache is used, updates to the underlying location may not be visible until
    the cache entry expires (controlled by `execution.file_listing_cache.ttl`).
    Valid values are `none`, `global` (for a global cache), and `session` (for a per-session cache).
  experimental: true

- key: execution.file_listing_cache.ttl
  type: number
  default: "1800"
  description: |
    The time-to-live (TTL) in seconds for cached directory listings.
    Entries expire after this duration from when they were cached,
    ensuring eventual consistency with the storage system.
    This setting is only effective when the cache is enabled.
    Setting the value to `0` disables the TTL.
    This setting can only be configured at startup and cannot be changed at runtime.
  experimental: true

- key: execution.file_listing_cache.max_entries
  type: number
  default: "10000"
  description: |
    Maximum number of directory listings to cache.
    This setting is only effective when the cache is enabled.
    Setting the value to `0` disables the limit.
    This setting can only be configured at startup and cannot be changed at runtime.
  experimental: true

- key: parquet.enable_page_index
  type: boolean
  default: "true"
  description: |
    (Reading) Whether to enable page index when reading Parquet files.
    If the value is `true`, the Parquet reader reads the page index if present.
    This can reduce I/O and the number of rows decoded.
  experimental: true

- key: parquet.pruning
  type: boolean
  default: "true"
  description: |
    (Reading) Whether to prune row groups when reading Parquet files.
    If the value is `true`, the Parquet reader attempts to skip entire row groups based
    on the predicate in the query and the metadata (minimum and maximum values) stored in
    the Parquet file.
  experimental: true

- key: parquet.skip_metadata
  type: boolean
  default: "true"
  description: |
    (Reading) Whether to skip the metadata when reading Parquet files.
    If the value is `true`, the Parquet reader skip the optional embedded metadata that may be in
    the file schema. This can help avoid schema conflicts when querying
    multiple Parquet files with schemas containing compatible types but different metadata.
  experimental: true

- key: parquet.metadata_size_hint
  type: number
  default: "524288"
  description: |
    (Reading) If specified, the reader will try to fetch the last `size_hint` bytes of the parquet file optimistically.
    If not specified, two reads are required:
      - One read to fetch the 8-byte parquet footer
      - Another to fetch the metadata length encoded in the footer

    If the metadata is larger than the hint, two reads will still be performed.
    Setting this can reduce one I/O operation per parquet file.
    Defaults to 512 KiB, which should be sufficient for most parquet files.
  experimental: true

- key: parquet.pushdown_filters
  type: boolean
  default: "false"
  description: |
    (Reading) Whether to push down filter expressions when reading Parquet files.
    If the value is `true`, the Parquet reader applies filter expressions in decoding operations to
    reduce the number of rows decoded. This optimization is sometimes called "late materialization".
  experimental: true

- key: parquet.reorder_filters
  type: boolean
  default: "false"
  description: |
    (Reading) Whether to reorder filter expressions when reading Parquet files.
    If the value is `true`, the Parquet reader reorders filter expressions heuristically in decoding operations to
    minimize the cost of evaluation. If the value is `false`, the filters are applied in the same order as written in the query.
  experimental: true

- key: parquet.schema_force_view_types
  type: boolean
  default: "true"
  description: |
    (Reading) Whether to force view types for binary and string columns when reading Parquet files.
    If the value is `true`, the Parquet reader will read columns of the `Utf8` or `Utf8Large` types as the `Utf8View` type,
    and the `Binary` or `BinaryLarge` types as the `BinaryView` type.
  experimental: true

- key: parquet.binary_as_string
  type: boolean
  default: "false"
  description: |
    (Reading) Whether to read binary columns as string columns when reading Parquet files.
    If the value is `true`, the Parquet reader will read columns of
    the `Binary` or `LargeBinary` as the `Utf8` type, and the `BinaryView` type as the `Utf8View` type.
    This is helpful when reading Parquet files generated by some legacy writers, which do not correctly set
    the UTF-8 flag for strings, causing string columns to be loaded as binary columns by default.
  experimental: true

- key: parquet.bloom_filter_on_read
  type: boolean
  default: "true"
  description: |
    (Reading) Whether to use available bloom filters when reading Parquet files.
  experimental: true

- key: parquet.max_predicate_cache_size
  type: number
  default: "104857600"
  description: |
    (Reading) The maximum predicate cache size, in bytes. When `pushdown_filters` is enabled, sets the maximum memory
    used to cache the results of predicate evaluation between filter evaluation and output generation.
    Decreasing this value will reduce memory usage, but may increase IO and CPU usage.
    Setting this value to `0` means no caching.
  experimental: true

- key: parquet.data_page_size_limit
  type: number
  default: "1048576"
  description: |
    (Writing) The best-effort maximum size of a data page in bytes.
  experimental: true

- key: parquet.write_batch_size
  type: number
  default: "1024"
  description: |
    (Writing) The Parquet writer batch size in bytes.
  experimental: true

- key: parquet.writer_version
  type: string
  default: '"1.0"'
  description: |
    (Writing) The Parquet writer version.
    Valid values are `"1.0"` and `"2.0"`.
  experimental: true

- key: parquet.skip_arrow_metadata
  type: boolean
  default: "false"
  description: |
    (Writing) Whether to skip encoding the embedded arrow metadata when writing Parquet files.
  experimental: true

- key: parquet.compression
  type: string
  default: "zstd(3)"
  description: |
    (Writing) The default Parquet compression codec.
    Valid values are `uncompressed`, `snappy`, `gzip(level)`,
    `lzo`, `brotli(level)`, `lz4`, `zstd(level)`, and `lz4_raw`,
    where `level` is an integer defining the compression level.
    These values are not case-sensitive.
  experimental: true

- key: parquet.dictionary_enabled
  type: boolean
  default: "true"
  description: |
    (Writing) Whether to enable dictionary encoding for the Parquet writer.
  experimental: true

- key: parquet.dictionary_page_size_limit
  type: number
  default: "1048576"
  description: |
    (Writing) The best-effort maximum dictionary page size in bytes for the Parquet writer.
  experimental: true

- key: parquet.statistics_enabled
  type: string
  default: "page"
  description: |
    (Writing) Whether statistics are enabled for any column for the Parquet writer.
    Valid values are `none`, `chunk`, and `page`.
    These values are not case-sensitive.
  experimental: true

- key: parquet.max_row_group_size
  type: number
  default: "1048576"
  description: |
    (Writing) The target maximum number of rows in each row group for the Parquet writer.
    Larger row groups require more memory to write, but
    can get better compression and be faster to read.
  experimental: true

- key: parquet.column_index_truncate_length
  type: number
  default: "64"
  description: |
    (Writing) The column index truncate length for the Parquet writer.
  experimental: true

- key: parquet.statistics_truncate_length
  type: number
  default: "64"
  description: |
    (Writing) The statistics truncate length for the Parquet writer.
    If the value is `0`, no truncation is applied.
  experimental: true

- key: parquet.data_page_row_count_limit
  type: number
  default: "20000"
  description: |
    (Writing) The best-effort maximum number of rows in data page for the Parquet writer.
  experimental: true

- key: parquet.encoding
  type: string
  default: ""
  description: |
    (Writing) The default encoding for any column.
    Valid values are `plain`, `plain_dictionary`, `rle`,
    `bit_packed` (deprecated), `delta_binary_packed`, `delta_length_byte_array`,
    `delta_byte_array`, `rle_dictionary`, and `byte_stream_split`.
    These values are not case sensitive.
    An empty value can also be used, which allows the Parquet writer to choose
    the encoding for each column to achieve good performance.
  experimental: true

- key: parquet.bloom_filter_on_write
  type: boolean
  default: "false"
  description: |
    (Writing) Whether to write bloom filters for all columns when writing Parquet files.
  experimental: true

- key: parquet.bloom_filter_fpp
  type: number
  default: "0.05"
  description: |
    (Writing) The false positive probability for bloom filters when writing Parquet files.
  experimental: true

- key: parquet.bloom_filter_ndv
  type: number
  default: "1000000"
  description: |
    (Writing) The number of distinct values for bloom filters when writing Parquet files.
  experimental: true

- key: parquet.allow_single_file_parallelism
  type: boolean
  default: "true"
  description: |
    (Writing) Whether to parallelize writing for each single Parquet file.
    If the value is `true`, each column in each row group in each file are serialized in parallel.
  experimental: true

- key: parquet.maximum_parallel_row_group_writers
  type: number
  default: "2"
  description: |
    (Writing) The maximum number of row group writers to use for the Parquet writer.
    This may improve performance when writing large Parquet files,
    at the expense of higher memory usage.
  experimental: true

- key: parquet.maximum_buffered_record_batches_per_stream
  type: number
  default: "16"
  description: |
    (Writing) The maximum number of buffered record batches per stream for the Parquet writer.
    This may improve performance when writing large Parquet files,
    at the expense of higher memory usage.
  experimental: true

- key: parquet.file_statistics_cache.type
  type: string
  default: "global"
  description: |
    (Reading) The type of cache for files statistics when reading Parquet files.
    This setting avoids repeatedly computing statistics,
    which may be expensive in certain situations (e.g., when using remote object storage).
    The cache is automatically invalidated when the underlying file is modified.
    Valid values are `none`, `global` (for a global cache), and `session` (for a per-session cache).
  experimental: true

- key: parquet.file_statistics_cache.ttl
  type: number
  default: "1800"
  description: |
    (Reading) The time-to-live (TTL) in seconds for cached Parquet files statistics.
    Entries expire after this duration from when they were cached,
    ensuring eventual consistency with the storage system.
    This setting is only effective when the cache is enabled.
    Setting the value to `0` disables the TTL.
    This setting can only be configured at startup and cannot be changed at runtime.
  experimental: true

- key: parquet.file_statistics_cache.max_entries
  type: number
  default: "10000"
  description: |
    (Reading) Maximum number of Parquet files statistics to cache.
    This setting is only effective when the cache is enabled.
    When the limit is reached, least recently used entries are evicted.
    Setting the value to `0` disables the limit.
    This setting can only be configured at startup and cannot be changed at runtime.
  experimental: true

- key: parquet.file_metadata_cache.type
  type: string
  default: "global"
  description: |
    (Reading) The type of cache for embedded metadata of Parquet files (footer and page metadata).
    This setting avoids repeatedly reading metadata,
    which can offer substantial performance improvements for repeated queries over large number of files.
    The cache is automatically invalidated when the underlying file is modified.
    Valid values are `none`, `global` (for a global cache), and `session` (for a per-session cache).
  experimental: true

- key: parquet.file_metadata_cache.ttl
  type: number
  default: "1800"
  description: |
    (Reading) The time-to-live (TTL) in seconds for cached Parquet files metadata.
    Entries expire after this duration from when they were cached,
    ensuring eventual consistency with the storage system.
    This setting is only effective when the cache is enabled.
    Setting the value to `0` disables the TTL.
    This setting can only be configured at startup and cannot be changed at runtime.
  experimental: true

- key: parquet.file_metadata_cache.size_limit
  type: number
  default: "0"
  description: |
    (Reading) Maximum size in bytes for the Parquet metadata cache.
    Setting the value to `0` disables the limit.
    This setting can only be configured at startup and cannot be changed at runtime.
  experimental: true

- key: kubernetes.image
  type: string
  default: "sail:latest"
  description: The container image to use for the driver and worker pods.

- key: kubernetes.image_pull_policy
  type: string
  default: "IfNotPresent"
  description: The image pull policy for the driver and worker pods.

- key: kubernetes.namespace
  type: string
  default: "default"
  description: The Kubernetes namespace in which the driver and worker pods will be created.

- key: kubernetes.driver_pod_name
  type: string
  default: ""
  description: |
    The name of the pod that runs the driver,
    or empty if the driver pod name is not known.
    This is used to set owner references for worker pods.

- key: kubernetes.worker_pod_name_prefix
  type: string
  default: "sail-worker-"
  description: |
    The prefix of the name of worker pods.
    This should usually end with a hyphen (`-`).

- key: kubernetes.worker_service_account_name
  type: string
  default: "default"
  description: The name of the service account to use for the worker pods.

- key: kubernetes.worker_pod_template
  type: string
  default: ""
  description: |
    If non-empty, a JSON string representing a Kubernetes Pod template
    with the schema of `PodTemplateSpec` in the Kubernetes API.

- key: catalog.list
  type: array
  default: '[{name="sail", type="memory", initial_database=["default"], initial_database_comment="default database"}]'
  description: |
    The list of catalogs to use. Each catalog is defined by a name and a type, along with optional parameters.
    `name` is used to refer to the catalog name, and `type` defines the catalog implementation.
  experimental: true

- key: catalog.default_catalog
  type: string
  default: "sail"
  description: |
    The name of the default catalog to use.
  experimental: true

- key: catalog.default_database
  type: array
  default: '["default"]'
  description: |
    The name of the default database (namespace) to use.
    This is a list of strings, where each string is a part of the namespace.
  experimental: true

- key: catalog.global_temporary_database
  type: array
  default: '["global_temp"]'
  description: |
    The name of the global temporary database (namespace) to use.
    This is a list of strings, where each string is a part of the namespace.
    The global temporary database cannot be changed at runtime,
    so setting the Spark configuration `spark.sql.globalTempDatabase` has no effect.
  experimental: true

- key: optimizer.enable_join_reorder
  type: boolean
  default: "false"
  description: |
    Whether to enable cost-based join reorder in the query optimizer.
  experimental: true

- key: spark.session_timeout_secs
  type: number
  default: "900"
  description: |
    The duration in seconds allowed for the session to be idle.
    If the server does not receive any requests from the client after this duration,
    the session will be removed from the server.

- key: spark.execution_heartbeat_interval_secs
  type: number
  default: "15"
  description: |
    The interval in seconds for the server to send empty response to the client
    during long-running operations. The empty response serves as a heartbeat to keep
    the session active.
  experimental: true

- key: telemetry.export_traces
  type: boolean
  default: "false"
  description: Whether to export traces to an OpenTelemetry collector.
  experimental: true

- key: telemetry.export_metrics
  type: boolean
  default: "false"
  description: Whether to export metrics to an OpenTelemetry collector.
  experimental: true

- key: telemetry.export_logs
  type: boolean
  default: "false"
  description: Whether to export logs to an OpenTelemetry collector.
  experimental: true

- key: telemetry.otlp_endpoint
  type: string
  default: "http://localhost:4317"
  description: The OTLP endpoint for telemetry data export.
  experimental: true

- key: telemetry.otlp_protocol
  type: string
  default: "grpc"
  description: |
    The OTLP protocol for telemetry data export.
    Valid values are `grpc`, `http-binary`, and `http-json`.
  experimental: true

- key: telemetry.otlp_timeout_secs
  type: number
  default: "10"
  description: The timeout in seconds for OTLP telemetry data export.
  experimental: true

- key: telemetry.traces_export_interval_secs
  type: number
  default: "5"
  description: The interval in seconds for exporting traces.
  experimental: true

- key: telemetry.metrics_export_interval_secs
  type: number
  default: "5"
  description: The interval in seconds for exporting metrics.
  experimental: true

- key: telemetry.logs_export_interval_secs
  type: number
  default: "1"
  description: The interval in seconds for exporting logs.
  experimental: true

- key: telemetry.logs_export_max_queue_size
  type: number
  default: "65536"
  description: |
    The maximum number of log records to queue for exporting.
    The log record will be dropped if the queue gets full between export intervals.
  experimental: true

- key: telemetry.logs_export_batch_size
  type: number
  default: "512"
  description: |
    The batch size for exporting logs. Multiple batches may be sent in each export interval
    if the number of queued log records exceeds the batch size.
  experimental: true
