---
title: AWS S3
rank: 2
---

# AWS S3

Sail supports reading and writing data to AWS S3 and S3-compatible object storage services using the `s3://` or `s3a://` URI schemes.

## URI Format

```
s3://bucket-name/path/to/object
s3a://bucket-name/path/to/object
```

Components:

- `s3://` or `s3a://`: Protocol scheme (both are equivalent in Sail)
- `bucket-name`: S3 bucket name
- `/path/to/object`: Object key or prefix within the bucket

Examples:

```python
# Single file
df = spark.read.parquet("s3://my-bucket/data/file.parquet")

# Directory/prefix
df = spark.read.parquet("s3://my-bucket/data/2025")
```

::: info
Sail automatically determines whether a path refers to a single object or a key prefix. For prefixes, it assumes they end with `/` and represent directories.
:::

## Credentials

All AWS credential providers work out-of-box in Sail.
You can authenticate with AWS S3 using any of the supported methods, including AWS `config` and `credentials` files,
EC2 instance profiles, environment variables, and container credentials.

Credential rotation happens automatically if you use temporary credentials.

::: info
You can refer to the [AWS documentation](https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html)
for more details about the credential providers.
:::

## Region Configuration

### Single Region

If the AWS region is configured, all S3 buckets must be in the same region. Otherwise, an error will be returned when accessing the data.

```bash
export AWS_REGION="us-east-1"
```

### Multi-Region Access

To allow inferring regions for S3 buckets and accessing S3 data in all regions, you can set the `AWS_REGION` environment variable to an empty string.

```bash
export AWS_REGION=""
```

## Reading Data

### Basic Reading

```python
# Read various formats
df_parquet = spark.read.parquet("s3://bucket/data.parquet")
df_csv = spark.read.option("header", "true").csv("s3://bucket/data.csv")
df_json = spark.read.json("s3://bucket/data.json")

# Read directory of files
df_parquet = spark.read.parquet("s3://bucket/logs/2025")
```

### Partitioned Data

```python
# Read partitioned dataset
# S3 structure: s3://bucket/data/year=2025/month=01/day=15/
df = spark.read.parquet("s3://bucket/data/")

# Filter partitions while reading
df = spark.read.parquet("s3://bucket/data/year=2025/month=*/")
```

## Writing Data

### Basic Writing

```python
# Write with different modes
df.write.mode("overwrite").parquet("s3://bucket/output/data.parquet")
df.write.mode("append").parquet("s3://bucket/output/data.parquet")

# Write with compression
df.write \
    .option("compression", "snappy") \
    .parquet("s3://bucket/output/compressed.parquet")
```

### Partitioned Writing

```python
# Partition by columns
df.write \
    .partitionBy("year", "month", "day") \
    .parquet("s3://bucket/partitioned-data/")

# Repartition
df.repartition(100).write.parquet("s3://bucket/data/")
```

### Output Formats

```python
# Various output formats
df.write.parquet("s3://bucket/data.parquet")
df.write.json("s3://bucket/data.json")
df.write.csv("s3://bucket/data.csv")

# With format-specific options
df.write \
    .option("header", "true") \
    .csv("s3://bucket/data.csv")
```

## Working with SQL

```sql
CREATE TABLE IF NOT EXISTS s3_table
USING parquet
LOCATION 's3://bucket/data/';

SELECT * FROM s3_table
WHERE year = 2025 AND month = 1;
```

## Common Use Cases

### ETL Pipeline

```python
import pyspark.sql.functions as F

raw_df = spark.read.parquet("s3://raw-bucket/events/2025/")

transformed_df = raw_df \
    .groupBy("date", "product_id", "event_type") \
    .agg(F.count("*").alias("event_count"))

transformed_df.write \
    .mode("overwrite") \
    .partitionBy("date") \
    .parquet("s3://processed-bucket/daily-aggregates/")
```

### Data Lake Pattern

```python
import pyspark.sql.functions as F

# Bronze layer (raw data)
spark.read.json("s3://data-source/api/") \
    .write.mode("append") \
    .partitionBy("ingestion_date") \
    .parquet("s3://data-lake/bronze/api-data/")

# Silver layer (cleaned data)
bronze_df = spark.read.parquet("s3://data-lake/bronze/api-data/")
silver_df = bronze_df \
    .filter(F.col("status") == "valid") \
    .withColumn("processed_date", F.current_date())
silver_df.write \
    .mode("overwrite") \
    .partitionBy("processed_date") \
    .parquet("s3://data-lake/silver/api-data/")

# Gold layer (business aggregates)
gold_df = silver_df \
    .groupBy("category", "processed_date") \
    .agg(F.sum("amount").alias("total_amount"))
gold_df.write \
    .mode("overwrite") \
    .parquet("s3://data-lake/gold/category-summary/")
```

## Public Datasets

Some datasets on S3 allow public access without an AWS account.
You can skip retrieving AWS credentials by setting the environment variable `AWS_SKIP_SIGNATURE=true`.

```bash
# Skip authentication for public buckets
export AWS_SKIP_SIGNATURE=true
```

```python
# Read public datasets
df = spark.read.parquet("s3://aws-public-blockchain/v1.0/btc/")
df = spark.read.csv("s3://nyc-tlc/trip data/yellow_tripdata_2025-*.csv")
```

::: info
`AWS_SKIP_SIGNATURE` is a Sail-specific environment variable, not part of standard AWS SDKs.
:::

## S3-Compatible Storage

### MinIO

```bash
export AWS_ACCESS_KEY_ID="smooth"
export AWS_SECRET_ACCESS_KEY="sailing"
export AWS_ENDPOINT="http://localhost:9000"
export AWS_VIRTUAL_HOSTED_STYLE_REQUEST="false"
export AWS_ALLOW_HTTP="true"
```

### Cloudflare R2

```bash
export AWS_ACCESS_KEY_ID="smooth"
export AWS_SECRET_ACCESS_KEY="sailing"
export AWS_ENDPOINT="https://[account-id].r2.cloudflarestorage.com"
export AWS_REGION="auto"
```

### Other S3-Compatible Services

```bash
# Generic S3-compatible configuration
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_ENDPOINT="https://s3-compatible-service.com"
export AWS_VIRTUAL_HOSTED_STYLE_REQUEST="true"  # or "false"
```

## Best Practices

### 1. Optimize File Sizes

Avoid too many small files (slow listing) or too few large files (poor parallelism)

### 2. Use Appropriate Formats

- **Parquet**: Best overall, especially for analytics, columnar storage, and compression
- **ORC**: Alternative columnar format, good compression
- **JSON**: For semi-structured data, larger storage footprint
- **CSV**: For compatibility, but inefficient for large datasets

### 3. Partition Strategically

```python
# Good: Low cardinality columns
df.write.partitionBy("year", "month").parquet("s3://bucket/data/")

# Bad: High cardinality columns (creates too many partitions)
# df.write.partitionBy("user_id").parquet("s3://bucket/data/")
```
