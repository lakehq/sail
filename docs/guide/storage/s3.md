---
title: AWS S3
rank: 2
---

# AWS S3

Sail supports reading and writing data to AWS S3 and S3-compatible object storage services using the `s3://`, `s3a://`, or `https://` URI schemes.
The URI can also refer to data stored in S3-compatible object storage services, such as [MinIO](https://min.io/) or services provided by other vendors.

For example, `s3://bucket/path/to/data` refers to the `path/to/data` path in the `bucket` bucket.
Sail determines whether the path refers to a single object or a key prefix.
If the path turns out to be a key prefix, we assume the key prefix is followed by `/` and represents a directory.

## URI Format

```
s3://bucket-name/path/to/object
s3a://bucket-name/path/to/object
https://bucket-name.s3.amazonaws.com/path/to/object
https://bucket-name.s3.region-code.amazonaws.com/path/to/object
https://s3.amazonaws.com/bucket-name/path/to/object
https://s3.region-code.amazonaws.com/bucket-name/path/to/object
```

Supported:

- `s3://` or `s3a://`: Protocol scheme
- `https://`: HTTPS protocol for direct S3 URLs
- `bucket-name`: S3 bucket name
- `/path/to/object`: Object key or prefix within the bucket

Examples:

```python
# Single file - S3 protocol
df = spark.read.parquet("s3://my-bucket/data/file.parquet")

# Directory/prefix - S3 protocol
df = spark.read.parquet("s3://my-bucket/data/2025")

# HTTPS URLs - virtual-hosted style
df = spark.read.parquet("https://bucket-name.s3.amazonaws.com/data/file.parquet")
df = spark.read.parquet("https://bucket-name.s3.region-code.amazonaws.com/data/file.parquet")

# HTTPS URLs - path style
df = spark.read.parquet("https://s3.amazonaws.com/my-bucket/data/file.parquet")
df = spark.read.parquet("https://s3.region-code.amazonaws.com/my-bucket/data/file.parquet")
```

## Credentials

All AWS credential providers work out-of-box in Sail.
You can authenticate with AWS S3 using any of the supported methods, including AWS `config` and `credentials` files,
EC2 instance profiles, environment variables, and container credentials.

Credential rotation happens automatically if you use temporary credentials.

::: info
You can refer to the [AWS documentation](https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html)
for more details about the credential providers.
:::

## Region Configuration

### Single Region

If the AWS region is configured, all S3 buckets must be in the same region. Otherwise, an error will be returned when accessing the data.

```bash
export AWS_REGION="us-east-1"
```

### Multi-Region Access

To allow inferring regions for S3 buckets and accessing S3 data in all regions, you can set the `AWS_REGION` environment variable to an empty string.

```bash
export AWS_REGION=""
```

## Public Datasets

Some datasets on S3 allow public access without an AWS account.
You can skip retrieving AWS credentials by setting the environment variable `AWS_SKIP_SIGNATURE=true`.

```bash
export AWS_SKIP_SIGNATURE=true
```

```python
df = spark.read.parquet("s3://public-bucket-name/path/")
```

::: info
`AWS_SKIP_SIGNATURE` is a Sail-specific environment variable, not part of standard AWS SDKs.
:::

## S3-Compatible Storage

### Cloudflare R2

```bash
export AWS_ACCESS_KEY_ID="smooth"
export AWS_SECRET_ACCESS_KEY="sailing"
export AWS_ENDPOINT="https://[account-id].r2.cloudflarestorage.com"
```

### MinIO and Other S3-Compatible Services

```bash
export AWS_ACCESS_KEY_ID="smooth"
export AWS_SECRET_ACCESS_KEY="sailing"
export AWS_ENDPOINT="http://localhost:9000"
```
