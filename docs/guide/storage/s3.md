---
title: AWS S3
rank: 2
---

# AWS S3

Sail supports reading and writing data to AWS S3 and S3-compatible object storage services using the `s3://`, `s3a://`, or `https://` URI schemes.
The URI can also refer to data stored in S3-compatible object storage services, such as [MinIO](https://min.io/) or services provided by other vendors.

For example, `s3://bucket/path/to/data` refers to the `path/to/data` path in the `bucket` bucket.
Sail determines whether the path refers to a single object or a key prefix.
If the path turns out to be a key prefix, we assume the key prefix is followed by `/` and represents a directory.

Sail supports the following access patterns for S3 and S3-compatible object storage:

#### S3 protocol:

- `s3://`

#### S3a protocol:

- `s3a://`

#### Virtual-hosted–style requests:

- `https://bucket-name.s3.region-code.amazonaws.com/key-name`
- `https://bucket-name.s3.amazonaws.com/key-name`

#### Transfer Acceleration:

- `https://bucket-name.s3-accelerate.amazonaws.com/key-name`
- `https://bucket-name.s3-accelerate.dualstack.amazonaws.com/key-name`

#### S3 Express:

- `https://s3express-control.region-code.amazonaws.com/key-name`
- `https://s3express-zone-id.region-code.amazonaws.com/key-name`

#### Path-style requests:

- `https://s3.region-code.amazonaws.com/bucket-name/key-name`
- `https://s3.amazonaws.com/bucket-name/key-name`

## S3 Protocol

`s3://bucket-name/path/to/object`

### DataFrame

```python
spark.sql("SELECT 1").write.parquet("s3://my-bucket/path")
df = spark.read.parquet("s3://my-bucket/path").show()
```

### SQL

```python
sql = """
CREATE TABLE s3_table
USING parquet
LOCATION 's3://my-bucket/path';
"""
spark.sql(sql)
spark.sql("SELECT * FROM s3_table;").show()

spark.sql("INSERT INTO s3_table VALUES (3, 'Charlie'), (4, 'Jessica');")
spark.sql("SELECT * FROM s3_table;").show()
```

## S3a Protocol

`s3a://bucket-name/path/to/object`

### DataFrame

```python
spark.sql("SELECT 1").write.parquet("s3a://my-bucket/path")
df = spark.read.parquet("s3a://my-bucket/path").show()
```

### SQL

```python
sql = """
CREATE TABLE s3a_table
USING parquet
LOCATION 's3a://my-bucket/path';
"""
spark.sql(sql)
spark.sql("SELECT * FROM s3a_table;").show()

spark.sql("INSERT INTO s3a_table VALUES (3, 'Charlie'), (4, 'Jessica');")
spark.sql("SELECT * FROM s3a_table;").show()
```

## Virtual-Hosted Style Requests

#### Region

`https://bucket-name.s3.region-code.amazonaws.com/key-name`

#### No Region

`https://bucket-name.s3.amazonaws.com/key-name`

### DataFrame

```python
spark.sql("SELECT 1").write.parquet("https://bucket-name.s3.region-code.amazonaws.com/key-name")
df = spark.read.parquet("https://bucket-name.s3.region-code.amazonaws.com/key-name").show()
```

### SQL

```python
sql = """
CREATE TABLE s3_vh_table
USING parquet
LOCATION 'https://bucket-name.s3.region-code.amazonaws.com/key-name';
"""
spark.sql(sql)
spark.sql("SELECT * FROM s3_vh_table;").show()

spark.sql("INSERT INTO s3_vh_table VALUES (3, 'Charlie'), (4, 'Jessica');")
spark.sql("SELECT * FROM s3_vh_table;").show()
```

## Transfer Acceleration

#### Standard

`https://bucket-name.s3-accelerate.amazonaws.com/key-name`

#### Dualstack

`https://bucket-name.s3-accelerate.dualstack.amazonaws.com/key-name`

### DataFrame

```python
spark.sql("SELECT 1").write.parquet("https://bucket-name.s3-accelerate.amazonaws.com/key-name")
df = spark.read.parquet("https://bucket-name.s3-accelerate.amazonaws.com/key-name").show()
```

### SQL

```python
sql = """
CREATE TABLE s3_accel_table
USING parquet
LOCATION 'https://bucket-name.s3-accelerate.amazonaws.com/key-name';
"""
spark.sql(sql)
spark.sql("SELECT * FROM s3_accel_table;").show()

spark.sql("INSERT INTO s3_accel_table VALUES (3, 'Charlie'), (4, 'Jessica');")
spark.sql("SELECT * FROM s3_accel_table;").show()
```

## S3 Express

## Path-Style Requests

#### Region

`https://s3.region-code.amazonaws.com/bucket-name/key-name`

#### No Region

`https://s3.amazonaws.com/bucket-name/key-name`

::: info
AWS has discontinued support for path-style requests for new buckets created after September 30, 2020.
For more information, see [Amazon S3 Path Deprecation Plan – The Rest of the Story](https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/) in the AWS News Blog.
:::

### DataFrame

```python
spark.sql("SELECT 1").write.parquet("https://s3.region-code.amazonaws.com/bucket-name/key-name")
df = spark.read.parquet("https://s3.region-code.amazonaws.com/bucket-name/key-name").show()
```

### SQL

```python
sql = """
CREATE TABLE s3_path_table
USING parquet
LOCATION 'https://s3.region-code.amazonaws.com/bucket-name/key-name';
"""
spark.sql(sql)
spark.sql("SELECT * FROM s3_path_table;").show()

spark.sql("INSERT INTO s3_path_table VALUES (3, 'Charlie'), (4, 'Jessica');")
spark.sql("SELECT * FROM s3_path_table;").show()
```

## Credentials

All AWS credential providers work out-of-box in Sail.
You can authenticate with AWS S3 using any of the supported methods, including AWS `config` and `credentials` files,
EC2 instance profiles, environment variables, and container credentials.

Credential rotation happens automatically if you use temporary credentials.

::: info
You can refer to the [AWS documentation](https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html)
for more details about the credential providers.
:::

## Region Configuration

### Single Region

If the AWS region is configured, all S3 buckets must be in the same region. Otherwise, an error will be returned when accessing the data.

```bash
export AWS_REGION="us-east-1"
```

### Multi-Region Access

To allow inferring regions for S3 buckets and accessing S3 data in all regions, you can set the `AWS_REGION` environment variable to an empty string.

```bash
export AWS_REGION=""
```

## Public Datasets

Some datasets on S3 allow public access without an AWS account.
You can skip retrieving AWS credentials by setting the environment variable `AWS_SKIP_SIGNATURE=true`.

```bash
export AWS_SKIP_SIGNATURE=true
```

```python
df = spark.read.parquet("s3://public-bucket-name/path/")
```

::: info
`AWS_SKIP_SIGNATURE` is a Sail-specific environment variable, not part of standard AWS SDKs.
:::

## S3-Compatible Storage

### Cloudflare R2

```bash
export AWS_ACCESS_KEY_ID="smooth"
export AWS_SECRET_ACCESS_KEY="sailing"
export AWS_ENDPOINT="https://[account-id].r2.cloudflarestorage.com"
```

### MinIO and Other S3-Compatible Services

```bash
export AWS_ACCESS_KEY_ID="smooth"
export AWS_SECRET_ACCESS_KEY="sailing"
export AWS_ENDPOINT="http://localhost:9000"
```
