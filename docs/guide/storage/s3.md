---
title: AWS S3
rank: 2
---

# AWS S3

Sail supports reading and writing data to AWS S3 and S3-compatible object storage services using the `s3://` or `s3a://` URI schemes.

## URI Format

```
s3://bucket-name/path/to/object
s3a://bucket-name/path/to/object
```

Components:

- `s3://` or `s3a://`: Protocol scheme (both are equivalent in Sail)
- `bucket-name`: S3 bucket name
- `/path/to/object`: Object key or prefix within the bucket

Examples:

```python
# Single file
df = spark.read.parquet("s3://my-bucket/data/file.parquet")

# Directory/prefix
df = spark.read.parquet("s3://my-bucket/data/2024/")

# With wildcards
df = spark.read.parquet("s3://my-bucket/data/*/part-*.parquet")
```

::: info
Sail automatically determines whether a path refers to a single object or a key prefix. For prefixes, it assumes they end with `/` and represent directories.
:::

## Authentication

### Standard AWS Credentials

Sail supports all AWS credential providers:

```bash
# 1. Environment variables (highest priority)
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_SESSION_TOKEN="your_session_token"  # For temporary credentials

# 2. AWS credentials file (~/.aws/credentials)
[default]
aws_access_key_id = your_access_key
aws_secret_access_key = your_secret_key

# 3. EC2 Instance Profile (automatic on EC2)
# 4. ECS Task Role (automatic on ECS/Fargate)
# 5. AWS SSO (after 'aws sso login')
```

### Credential Rotation

Temporary credentials are automatically refreshed:

```python
# Using STS AssumeRole credentials
# Sail handles rotation automatically before expiration
df = spark.read.parquet("s3://secure-bucket/data/")
```

### Cross-Account Access

```bash
# Assume role in another account
export AWS_ROLE_ARN="arn:aws:iam::123456789012:role/CrossAccountRole"
export AWS_ROLE_SESSION_NAME="sail-session"
```

## Region Configuration

### Single Region

```bash
# All buckets must be in us-east-1
export AWS_REGION="us-east-1"
```

### Multi-Region Access

```bash
# Allow access to buckets in any region
export AWS_REGION=""

# Or explicitly set default region with cross-region enabled
export AWS_DEFAULT_REGION="us-east-1"
export AWS_S3_REGION_INFERENCE="true"
```

## Reading Data

### Basic Reading

```python
# Read various formats
df_parquet = spark.read.parquet("s3://bucket/data.parquet")
df_csv = spark.read.option("header", "true").csv("s3://bucket/data.csv")
df_json = spark.read.json("s3://bucket/data.json")
df_orc = spark.read.orc("s3://bucket/data.orc")

# Read with options
df = spark.read \
    .option("mergeSchema", "true") \
    .option("recursiveFileLookup", "true") \
    .parquet("s3://bucket/data/")
```

### Partitioned Data

```python
# Read partitioned dataset
# S3 structure: s3://bucket/data/year=2024/month=01/day=15/
df = spark.read.parquet("s3://bucket/data/")

# Filter partitions while reading
df = spark.read.parquet("s3://bucket/data/year=2024/month=*/")

# Read specific partitions
df = spark.read.parquet([
    "s3://bucket/data/year=2024/month=01/",
    "s3://bucket/data/year=2024/month=02/"
])
```

### Reading Multiple Files

```python
# Read multiple files with glob patterns
df = spark.read.parquet("s3://bucket/logs/2024-*.parquet")

# Read from multiple buckets
df = spark.read.parquet([
    "s3://bucket1/data/",
    "s3://bucket2/data/"
])

# Union files with different schemas
df = spark.read \
    .option("mergeSchema", "true") \
    .parquet("s3://bucket/evolving-data/")
```

## Writing Data

### Basic Writing

```python
# Write with different modes
df.write.mode("overwrite").parquet("s3://bucket/output/data.parquet")
df.write.mode("append").parquet("s3://bucket/output/data.parquet")
df.write.mode("ignore").parquet("s3://bucket/output/data.parquet")
df.write.mode("error").parquet("s3://bucket/output/data.parquet")  # Default

# Write with compression
df.write \
    .option("compression", "snappy") \
    .parquet("s3://bucket/output/compressed.parquet")
```

### Partitioned Writing

```python
# Partition by columns
df.write \
    .partitionBy("year", "month", "day") \
    .parquet("s3://bucket/partitioned-data/")

# Dynamic partition overwrite
df.write \
    .mode("overwrite") \
    .option("partitionOverwriteMode", "dynamic") \
    .partitionBy("date") \
    .parquet("s3://bucket/data/")

# Control number of files
df.repartition(100).write.parquet("s3://bucket/data/")
df.coalesce(1).write.parquet("s3://bucket/single-file/")
```

### Output Formats

```python
# Various output formats
df.write.parquet("s3://bucket/data.parquet")
df.write.json("s3://bucket/data.json")
df.write.csv("s3://bucket/data.csv")
df.write.orc("s3://bucket/data.orc")

# With format-specific options
df.write \
    .option("header", "true") \
    .option("delimiter", "|") \
    .csv("s3://bucket/data.csv")
```

## Working with SQL

```sql
-- Create external table
CREATE TABLE IF NOT EXISTS s3_table
USING parquet
OPTIONS (path 's3://bucket/data/')
LOCATION 's3://bucket/data/';

-- Query with partition pruning
SELECT * FROM s3_table
WHERE year = 2024 AND month = 1;

-- Create table with specific options
CREATE TABLE s3_compressed
USING parquet
OPTIONS (
  path 's3://bucket/compressed/',
  compression 'gzip'
);

-- CTAS (Create Table As Select)
CREATE TABLE s3_aggregated
USING parquet
LOCATION 's3://bucket/aggregated/'
AS SELECT
  year, month,
  SUM(sales) as total_sales
FROM s3_table
GROUP BY year, month;
```

## Performance Optimization

### Request Optimization

```python
# Increase parallelism for large datasets
spark.conf.set("spark.sql.files.maxPartitionBytes", "134217728")  # 128MB
spark.conf.set("spark.sql.files.openCostInBytes", "4194304")     # 4MB

# Optimize S3 requests
spark.conf.set("spark.hadoop.fs.s3a.connection.maximum", "100")
spark.conf.set("spark.hadoop.fs.s3a.threads.max", "64")
```

### Committer Settings

```python
# Use S3A committer for better write performance
spark.conf.set("spark.sql.sources.commitProtocolClass",
               "org.apache.spark.internal.io.cloud.PathOutputCommitProtocol")
spark.conf.set("spark.sql.parquet.output.committer.class",
               "org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter")
```

### Caching and Prefetching

```python
# Enable S3 prefetching
spark.conf.set("spark.hadoop.fs.s3a.readahead.range", "1048576")  # 1MB
spark.conf.set("spark.hadoop.fs.s3a.input.fadvise", "random")

# Cache frequently accessed data
df = spark.read.parquet("s3://bucket/reference-data/")
df.cache()
df.count()  # Trigger caching
```

## Common Use Cases

### ETL Pipeline

```python
# Read raw data
raw_df = spark.read.json("s3://raw-bucket/events/2024/*/")

# Transform
transformed_df = raw_df \
    .filter(col("event_type").isin(["purchase", "view"])) \
    .withColumn("date", to_date(col("timestamp"))) \
    .groupBy("date", "product_id", "event_type") \
    .agg(count("*").alias("event_count"))

# Write results
transformed_df.write \
    .mode("overwrite") \
    .partitionBy("date") \
    .parquet("s3://processed-bucket/daily-aggregates/")
```

### Data Lake Pattern

```python
# Bronze layer (raw data)
spark.read.json("s3://data-source/api/") \
    .write.mode("append") \
    .partitionBy("ingestion_date") \
    .parquet("s3://data-lake/bronze/api-data/")

# Silver layer (cleaned data)
bronze_df = spark.read.parquet("s3://data-lake/bronze/api-data/")
silver_df = bronze_df \
    .dropDuplicates(["id"]) \
    .filter(col("status") == "valid") \
    .withColumn("processed_date", current_date())

silver_df.write \
    .mode("overwrite") \
    .partitionBy("processed_date") \
    .parquet("s3://data-lake/silver/api-data/")

# Gold layer (business aggregates)
gold_df = silver_df \
    .groupBy("category", "processed_date") \
    .agg(sum("amount").alias("total_amount"))

gold_df.write \
    .mode("overwrite") \
    .parquet("s3://data-lake/gold/category-summary/")
```

### Archival and Lifecycle

```python
# Archive old data with different storage class
df = spark.read.parquet("s3://hot-bucket/data/year=2020/")

# Write to archive bucket (configure bucket for Glacier)
df.write \
    .mode("overwrite") \
    .option("spark.hadoop.fs.s3a.storage.class", "GLACIER") \
    .parquet("s3://archive-bucket/data/year=2020/")
```

## Public Datasets

Access public S3 datasets without credentials:

```bash
# Skip authentication for public buckets
export AWS_SKIP_SIGNATURE=true
```

```python
# Read public datasets
df = spark.read.parquet("s3://aws-public-blockchain/v1.0/btc/")
df = spark.read.csv("s3://nyc-tlc/trip data/yellow_tripdata_2024-*.csv")
```

::: info
`AWS_SKIP_SIGNATURE` is a Sail-specific environment variable, not part of standard AWS SDKs.
:::

## S3-Compatible Storage

### MinIO

```bash
export AWS_ACCESS_KEY_ID="minioadmin"
export AWS_SECRET_ACCESS_KEY="minioadmin"
export AWS_ENDPOINT="http://localhost:9000"
export AWS_VIRTUAL_HOSTED_STYLE_REQUEST="false"
export AWS_ALLOW_HTTP="true"
```

### Cloudflare R2

```bash
export AWS_ACCESS_KEY_ID="your_r2_access_key_id"
export AWS_SECRET_ACCESS_KEY="your_r2_secret_access_key"
export AWS_ENDPOINT="https://[account-id].r2.cloudflarestorage.com"
export AWS_REGION="auto"
```

### Other S3-Compatible Services

```bash
# Generic S3-compatible configuration
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_ENDPOINT="https://s3-compatible-service.com"
export AWS_VIRTUAL_HOSTED_STYLE_REQUEST="true"  # or "false"
```

## Best Practices

### 1. Optimize File Sizes

- Target 128-256 MB per file for best performance
- Use `coalesce()` or `repartition()` to control output files
- Avoid too many small files (slow listing) or too few large files (poor parallelism)

### 2. Use Appropriate Formats

- **Parquet**: Best for analytics, columnar storage, compression
- **ORC**: Alternative columnar format, good compression
- **JSON**: For semi-structured data, larger storage footprint
- **CSV**: For compatibility, but inefficient for large datasets

### 3. Partition Strategically

```python
# Good: Low cardinality columns
df.write.partitionBy("year", "month").parquet("s3://bucket/data/")

# Bad: High cardinality columns (creates too many partitions)
# df.write.partitionBy("user_id").parquet("s3://bucket/data/")
```

### 4. Cost Optimization

- Use lifecycle policies to transition old data to cheaper storage classes
- Enable S3 Intelligent-Tiering for unpredictable access patterns
- Compress data to reduce storage and transfer costs
- Use S3 Select for simple filtering (when supported)

## Troubleshooting

### Authentication Errors

```python
# Debug credentials
import os
print(f"AWS_ACCESS_KEY_ID: {os.environ.get('AWS_ACCESS_KEY_ID', 'Not set')}")
print(f"AWS_REGION: {os.environ.get('AWS_REGION', 'Not set')}")

# Test S3 access
spark.read.text("s3://your-bucket/test.txt").show()
```

### Access Denied

```bash
# Check bucket policy and IAM permissions
aws s3api get-bucket-policy --bucket your-bucket
aws sts get-caller-identity

# Verify you can access the bucket
aws s3 ls s3://your-bucket/
```

### Slow Performance

```python
# Check S3 metrics
spark.conf.set("spark.hadoop.fs.s3a.metrics.enabled", "true")

# Increase connection pool
spark.conf.set("spark.hadoop.fs.s3a.connection.maximum", "200")

# Enable request pipelining
spark.conf.set("spark.hadoop.fs.s3a.experimental.fadvise", "random")
```

### Region Issues

```bash
# Error: "The bucket is in this region: us-west-2"
export AWS_REGION="us-west-2"

# Or enable cross-region access
export AWS_REGION=""
```

### Connection Timeouts

```python
# Increase timeouts for slow connections
spark.conf.set("spark.hadoop.fs.s3a.connection.timeout", "200000")
spark.conf.set("spark.hadoop.fs.s3a.connection.establish.timeout", "5000")
```
