diff --git a/pom.xml b/pom.xml
index a0e25ce4d8d..261a2b03380 100644
--- a/pom.xml
+++ b/pom.xml
@@ -3357,6 +3357,10 @@
             <relocation>
               <pattern>com.google.common</pattern>
               <shadedPattern>${spark.shade.packageName}.guava</shadedPattern>
+              <!-- https://issues.apache.org/jira/browse/SPARK-45201 -->
+              <excludes>
+                <exclude>com.google.common.util.concurrent.internal.**</exclude>
+              </excludes>
             </relocation>
             <relocation>
               <pattern>org.dmg.pmml</pattern>
diff --git a/python/pyspark/ml/tests/connect/__init__.py b/python/pyspark/ml/tests/connect/__init__.py
new file mode 100644
index 00000000000..cce3acad34a
--- /dev/null
+++ b/python/pyspark/ml/tests/connect/__init__.py
@@ -0,0 +1,16 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
diff --git a/python/pyspark/sql/connect/client/core.py b/python/pyspark/sql/connect/client/core.py
index 7b1aafbefeb..ed0883891c3 100644
--- a/python/pyspark/sql/connect/client/core.py
+++ b/python/pyspark/sql/connect/client/core.py
@@ -176,6 +176,9 @@ class ChannelBuilder:
 
             # 'spark.local.connect' is set when we use the local mode in Spark Connect.
             if session is not None and session.conf.get("spark.local.connect", "0") == "1":
+                # override the remote port via the environment variable
+                if v := os.environ.get("SPARK_TESTING_REMOTE_PORT"):
+                    return int(v)
 
                 jvm = PySparkSession._instantiatedSession._jvm  # type: ignore[union-attr]
                 return getattr(
@@ -1005,6 +1008,8 @@ class SparkConnectClient(object):
         """
         Close the channel.
         """
+        if self._closed:
+            return
         ExecutePlanResponseReattachableIterator.shutdown()
         self._channel.close()
         self._closed = True
diff --git a/python/pyspark/sql/tests/connect/test_connect_basic.py b/python/pyspark/sql/tests/connect/test_connect_basic.py
index cc95bfface0..7625a7cce85 100644
--- a/python/pyspark/sql/tests/connect/test_connect_basic.py
+++ b/python/pyspark/sql/tests/connect/test_connect_basic.py
@@ -111,6 +111,11 @@ class SparkConnectSQLTestCase(ReusedConnectTestCase, SQLTestUtils, PandasOnSpark
         cls.spark_connect_clean_up_test_data()
         # Load test data
         cls.spark_connect_load_test_data()
+        # Load test data for connect
+        _spark = cls.spark
+        cls.spark = cls.connect
+        cls.spark_connect_load_test_data()
+        cls.spark = _spark
 
     @classmethod
     def tearDownClass(cls):
@@ -145,8 +150,7 @@ class SparkConnectSQLTestCase(ReusedConnectTestCase, SQLTestUtils, PandasOnSpark
                 StructField("lastname", StringType(), True),
             ]
         )
-        emptyRDD = cls.spark.sparkContext.emptyRDD()
-        empty_df = cls.spark.createDataFrame(emptyRDD, empty_table_schema)
+        empty_df = cls.spark.createDataFrame([], empty_table_schema)
         empty_df.write.saveAsTable(cls.tbl_name_empty)
 
     @classmethod
diff --git a/python/pyspark/sql/tests/connect/test_connect_plan.py b/python/pyspark/sql/tests/connect/test_connect_plan.py
index 88ef37511a6..448a027e9e9 100644
--- a/python/pyspark/sql/tests/connect/test_connect_plan.py
+++ b/python/pyspark/sql/tests/connect/test_connect_plan.py
@@ -740,7 +740,7 @@ class SparkConnectPlanTests(PlanOnlyTestFixture):
         self.assertIsInstance(col, Column)
         self.assertEqual("Column<'UnresolvedRegex(col_name)'>", str(col))
 
-        col_plan = col.to_plan(self.session.client)
+        col_plan = col.to_plan(None)
         self.assertIsNotNone(col_plan)
         self.assertEqual(col_plan.unresolved_regex.col_name, "col_name")
 
@@ -921,7 +921,7 @@ class SparkConnectPlanTests(PlanOnlyTestFixture):
         self.assertEqual("Column<'a AS martin'>", str(col0))
 
         col0 = col("a").alias("martin", metadata={"pii": True})
-        plan = col0.to_plan(self.session.client)
+        plan = col0.to_plan(None)
         self.assertIsNotNone(plan)
         self.assertEqual(plan.alias.metadata, '{"pii": true}')
 
diff --git a/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py b/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py
index e1ec97928f7..d960e391100 100644
--- a/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py
+++ b/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py
@@ -56,7 +56,7 @@ if have_pyarrow:
 class GroupedApplyInPandasWithStateTestsMixin:
     @classmethod
     def conf(cls):
-        cfg = SparkConf()
+        cfg = super().conf()
         cfg.set("spark.sql.shuffle.partitions", "5")
         return cfg
 
diff --git a/python/pyspark/testing/connectutils.py b/python/pyspark/testing/connectutils.py
index a063f27c9ea..784161bbade 100644
--- a/python/pyspark/testing/connectutils.py
+++ b/python/pyspark/testing/connectutils.py
@@ -139,7 +139,6 @@ class PlanOnlyTestFixture(unittest.TestCase, PySparkErrorTestUtils):
     @classmethod
     def setUpClass(cls):
         cls.connect = MockRemoteSession()
-        cls.session = SparkSession.builder.remote().getOrCreate()
         cls.tbl_name = "test_connect_plan_only_table_1"
 
         cls.connect.set_hook("readTable", cls._read_table)
diff --git a/python/pyspark/testing/objects.py b/python/pyspark/testing/objects.py
index 5b97664afbd..b9deeef91c8 100644
--- a/python/pyspark/testing/objects.py
+++ b/python/pyspark/testing/objects.py
@@ -45,7 +45,7 @@ class ExamplePointUDT(UserDefinedType):
 
     @classmethod
     def module(cls):
-        return "pyspark.sql.tests"
+        return "pyspark.testing.objects"
 
     @classmethod
     def scalaUDT(cls):
diff --git a/python/setup.py b/python/setup.py
index ddd961d0412..f58d965e737 100755
--- a/python/setup.py
+++ b/python/setup.py
@@ -183,6 +183,9 @@ try:
     copyfile("pyspark/shell.py", "pyspark/python/pyspark/shell.py")
 
     if in_spark:
+        # Copy the test support files to the package.
+        copytree("test_support", "pyspark/python/test_support", dirs_exist_ok=True)
+
         # Construct the symlink farm - this is necessary since we can't refer to the path above the
         # package root and we need to copy the jars and scripts which are up above the python root.
         if _supports_symlinks():
@@ -267,6 +270,7 @@ try:
             "pyspark.pandas.spark",
             "pyspark.pandas.typedef",
             "pyspark.pandas.usage_logging",
+            "pyspark.python",
             "pyspark.python.pyspark",
             "pyspark.python.lib",
             "pyspark.testing",
@@ -276,6 +280,42 @@ try:
             "pyspark.errors",
             "pyspark.errors.exceptions",
             "pyspark.examples.src.main.python",
+            "pyspark.errors.tests",
+            "pyspark.ml.deepspeed.tests",
+            "pyspark.ml.tests",
+            "pyspark.ml.tests.connect",
+            "pyspark.ml.tests.tuning",
+            "pyspark.ml.torch.tests",
+            "pyspark.mllib.tests",
+            "pyspark.pandas.tests",
+            "pyspark.pandas.tests.computation",
+            "pyspark.pandas.tests.connect",
+            "pyspark.pandas.tests.connect.computation",
+            "pyspark.pandas.tests.connect.data_type_ops",
+            "pyspark.pandas.tests.connect.diff_frames_ops",
+            "pyspark.pandas.tests.connect.frame",
+            "pyspark.pandas.tests.connect.groupby",
+            "pyspark.pandas.tests.connect.indexes",
+            "pyspark.pandas.tests.connect.io",
+            "pyspark.pandas.tests.connect.plot",
+            "pyspark.pandas.tests.connect.series",
+            "pyspark.pandas.tests.data_type_ops",
+            "pyspark.pandas.tests.diff_frames_ops",
+            "pyspark.pandas.tests.frame",
+            "pyspark.pandas.tests.groupby",
+            "pyspark.pandas.tests.indexes",
+            "pyspark.pandas.tests.io",
+            "pyspark.pandas.tests.plot",
+            "pyspark.pandas.tests.series",
+            "pyspark.resource.tests",
+            "pyspark.sql.tests",
+            "pyspark.sql.tests.connect",
+            "pyspark.sql.tests.connect.client",
+            "pyspark.sql.tests.connect.streaming",
+            "pyspark.sql.tests.pandas",
+            "pyspark.sql.tests.streaming",
+            "pyspark.streaming.tests",
+            "pyspark.tests",
         ],
         include_package_data=True,
         package_dir={
@@ -296,10 +336,34 @@ try:
                 "start-history-server.sh",
                 "stop-history-server.sh",
             ],
+            "pyspark.python": [
+                "test_support/*.py",
+                "test_support/*.zip",
+                "test_support/hello/*.txt",
+                "test_support/hello/sub_hello/*.txt",
+                "test_support/sql/*.csv",
+                "test_support/sql/*.json",
+                "test_support/sql/*.txt",
+                "test_support/sql/orc_partitioned/_SUCCESS",
+                "test_support/sql/orc_partitioned/b=0/c=0/*.orc",
+                "test_support/sql/orc_partitioned/b=0/c=0/.*.orc.crc",
+                "test_support/sql/orc_partitioned/b=1/c=1/*.orc",
+                "test_support/sql/orc_partitioned/b=1/c=1/.*.orc.crc",
+                "test_support/streaming/*.txt",
+            ],
             "pyspark.python.lib": ["*.zip"],
-            "pyspark.data": ["*.txt", "*.data"],
+            "pyspark.data": ["*.txt", "*.data", "artifact-tests/*.jar"],
             "pyspark.licenses": ["*.txt"],
             "pyspark.examples.src.main.python": ["*.py", "*/*.py"],
+            "pyspark.ml.tests": ["typing/*.yml"],
+            "pyspark.sql.tests": ["typing/*.yml"],
+            "pyspark.tests": ["typing/*.yml"],
+        },
+        # The excluded files are still in the package
+        # but are excluded from the installation.
+        exclude_package_data={
+            # https://issues.apache.org/jira/browse/SPARK-45201
+            "pyspark.jars": ["spark-connect-common_*.jar"],
         },
         scripts=scripts,
         license="http://www.apache.org/licenses/LICENSE-2.0",
